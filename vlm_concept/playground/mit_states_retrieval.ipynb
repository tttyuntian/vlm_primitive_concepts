{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "e669bfb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import json\n",
    "from collections import defaultdict\n",
    "\n",
    "import fasttext\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import clip\n",
    "import pickle\n",
    "from PIL import Image\n",
    "from tqdm import tqdm\n",
    "from sklearn import svm\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c3fd6826",
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax(inputs):\n",
    "    res = torch.tensor(inputs).float()\n",
    "    res = res.softmax(dim=-1)\n",
    "    return res.numpy()\n",
    "\n",
    "def normalize(inputs):\n",
    "    res = torch.tensor(inputs).float()\n",
    "    res /= res.norm(dim=-1, keepdim=True)\n",
    "    return res.numpy()\n",
    "\n",
    "def get_precomputed_features(feature, args, is_softmax=False):\n",
    "    \"\"\" Get precomputed CLIP image/pair/attr/obj features \"\"\"\n",
    "    data_root = args.precomputed_data_root\n",
    "    feature_name = \"image_features\" if feature==\"image\" else f\"{feature}_activations\"\n",
    "    feature_train = np.load(os.path.join(data_root, f\"{feature_name}_train.npy\"))\n",
    "    feature_valid = np.load(os.path.join(data_root, f\"{feature_name}_valid.npy\"))\n",
    "    feature_test = np.load(os.path.join(data_root, f\"{feature_name}_test.npy\"))\n",
    "    if is_softmax:\n",
    "        feature_train = softmax(feature_train)\n",
    "        feature_valid = softmax(feature_valid)\n",
    "        feature_test = softmax(feature_test)\n",
    "    print(f\"{feature} \\t| train {feature_train.shape} \\t| valid {feature_valid.shape} \\t| test {feature_test.shape}\")\n",
    "    return feature_train, feature_valid, feature_test\n",
    "\n",
    "def get_seen_unseen_indices(split, data):\n",
    "    if split == \"train\":\n",
    "        split_data = data.train_data\n",
    "    elif split == \"valid\":\n",
    "        split_data = data.valid_data\n",
    "    elif split == \"test\":\n",
    "        split_data = data.test_data\n",
    "    else:\n",
    "        raise ValueError(f\"No split found: {split}\")\n",
    "\n",
    "    pairs = [(sample[\"attr\"], sample[\"obj\"]) for sample in split_data]\n",
    "    seen_indices = [\n",
    "        i for i in range(len(pairs))\n",
    "        if pairs[i] in data.train_pairs\n",
    "    ]\n",
    "    unseen_indices = [\n",
    "        i for i in range(len(pairs))\n",
    "        if pairs[i] not in data.train_pairs\n",
    "    ]\n",
    "    print(f\"seen_indices: {len(seen_indices)} | unseen_indices: {len(unseen_indices)}\")\n",
    "    return seen_indices, unseen_indices\n",
    "\n",
    "def evaluate(results):\n",
    "    \"\"\" Evaluate predictions and Return metrics. \"\"\"\n",
    "    all_preds, seen_preds, unseen_preds = results[\"all_preds\"], results[\"seen_preds\"], results[\"unseen_preds\"]\n",
    "    all_acc, seen_acc, unseen_acc = np.mean(all_preds), np.mean(seen_preds), np.mean(unseen_preds)    \n",
    "    return {\n",
    "        \"all_acc\": all_acc,\n",
    "        \"seen_acc\": seen_acc,\n",
    "        \"unseen_acc\": unseen_acc,\n",
    "        \"harmonic_mean\": (seen_acc * unseen_acc)**0.5,\n",
    "        \"macro_average_acc\": (seen_acc + unseen_acc)*0.5,\n",
    "    }\n",
    "\n",
    "def generate_predictions(scores, labels, seen_ids, unseen_ids, data, topk, bias=0.0):\n",
    "    \"\"\" Apply bias and Generate predictions for. \"\"\"\n",
    "    def get_predictions(_scores):\n",
    "        # Get predictions\n",
    "        _, pair_preds = _scores.topk(topk, dim=1)\n",
    "        pair_preds = pair_preds[:, :topk].contiguous().view(-1)\n",
    "        attr_preds = all_pairs[pair_preds][:,0].view(-1, topk)\n",
    "        obj_preds = all_pairs[pair_preds][:,1].view(-1, topk)\n",
    "        pair_preds = pair_preds.view(-1, topk)\n",
    "        return pair_preds, attr_preds, obj_preds\n",
    "    \n",
    "    # Get predictions with biases applied\n",
    "    all_pairs = torch.LongTensor([\n",
    "        (data.attr2idx[attr], data.obj2idx[obj]) \n",
    "        for attr, obj in data.pairs\n",
    "    ])\n",
    "    scores = scores.clone()\n",
    "    mask = data.seen_mask.repeat(scores.shape[0], 1)\n",
    "    scores[~mask] += bias\n",
    "    pair_preds, attr_preds, obj_preds = get_predictions(scores)\n",
    "    \n",
    "    # Get predictions for seen/unseen pairs\n",
    "    all_preds = np.array([label in pair_preds[row_id,:topk] for row_id, label in enumerate(labels)])\n",
    "    seen_preds = all_preds[seen_ids]\n",
    "    unseen_preds = all_preds[unseen_ids]\n",
    "    return {\n",
    "        \"pair_preds\": pair_preds,\n",
    "        \"attr_preds\": attr_preds,\n",
    "        \"obj_preds\": obj_preds,\n",
    "        \"all_preds\": all_preds,\n",
    "        \"seen_preds\": seen_preds,\n",
    "        \"unseen_preds\": unseen_preds,\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "73d6aeaf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['RN50', 'RN101', 'RN50x4', 'RN50x16', 'ViT-B/32', 'ViT-B/16']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clip.available_models()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "157b7965",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the model\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model, preprocess = clip.load(\"ViT-B/32\", device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a1bd2187",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset\n",
    "\n",
    "class MITStatesDataset(Dataset):\n",
    "    def __init__(self, root, split):\n",
    "        self.root = root\n",
    "        self.split = split\n",
    "        \n",
    "        # Load metadata\n",
    "        self.metadata = torch.load(os.path.join(root, \"metadata_compositional-split-natural.t7\"))\n",
    "        \n",
    "        # Load attribute-noun pairs for each split\n",
    "        all_info, split_info = self.parse_split()\n",
    "        self.attrs, self.objs, self.pairs = all_info\n",
    "        self.train_pairs, self.valid_pairs, self.test_pairs = split_info\n",
    "        \n",
    "        # Get obj/attr/pair to indices mappings\n",
    "        self.obj2idx = {obj: idx for idx, obj in enumerate(self.objs)}\n",
    "        self.attr2idx = {attr: idx for idx, attr in enumerate(self.attrs)}\n",
    "        self.pair2idx = {pair: idx for idx, pair in enumerate(self.pairs)}\n",
    "        self.idx2obj = {idx: obj for obj, idx in self.obj2idx.items()}\n",
    "        self.idx2attr = {idx: attr for attr, idx in self.attr2idx.items()}\n",
    "        self.idx2pair = {idx: pair for pair, idx in self.pair2idx.items()}\n",
    "        \n",
    "        # Get all data\n",
    "        self.train_data, self.valid_data, self.test_data = self.get_split_info()\n",
    "        if self.split == \"train\":\n",
    "            self.data = self.train_data\n",
    "        elif self.split == \"valid\":\n",
    "            self.data = self.valid_data\n",
    "        else:\n",
    "            self.data = self.test_data\n",
    "        \n",
    "        self.sample_indices = list(range(len(self.data)))\n",
    "        self.sample_pairs = self.train_pairs\n",
    "        print(f\"train pairs: {len(self.train_pairs)} | valid pairs: {len(self.valid_pairs)} | test pairs: {len(self.test_pairs)}\")\n",
    "        print(f\"train images: {len(self.train_data)} | valid images: {len(self.valid_data)} | test images: {len(self.test_data)}\")\n",
    "    \n",
    "    def parse_split(self):\n",
    "        def parse_pairs(pair_path):\n",
    "            with open(pair_path, \"r\") as f:\n",
    "                pairs = f.read().strip().split(\"\\n\")\n",
    "                pairs = [t.split() for t in pairs]\n",
    "                pairs = list(map(tuple, pairs))\n",
    "            attrs, objs = zip(*pairs)\n",
    "            return attrs, objs, pairs\n",
    "        \n",
    "        tr_attrs, tr_objs, tr_pairs = parse_pairs(os.path.join(self.root, \"compositional-split-natural\", \"train_pairs.txt\"))\n",
    "        vl_attrs, vl_objs, vl_pairs = parse_pairs(os.path.join(self.root, \"compositional-split-natural\", \"val_pairs.txt\"))\n",
    "        ts_attrs, ts_objs, ts_pairs = parse_pairs(os.path.join(self.root, \"compositional-split-natural\", \"test_pairs.txt\"))\n",
    "\n",
    "        all_attrs = sorted(list(set(tr_attrs + vl_attrs + ts_attrs)))\n",
    "        all_objs = sorted(list(set(tr_objs + vl_objs + ts_objs)))\n",
    "        all_pairs = sorted(list(set(tr_pairs + vl_pairs + ts_pairs)))\n",
    "        \n",
    "        return (all_attrs, all_objs, all_pairs), (tr_pairs, vl_pairs, ts_pairs)\n",
    "    \n",
    "    def get_split_info(self):\n",
    "        train_data, val_data, test_data = [], [], []\n",
    "        for instance in self.metadata:\n",
    "            image, attr, obj, settype = instance[\"image\"], instance[\"attr\"], instance[\"obj\"], instance[\"set\"]\n",
    "            image = image.split(\"/\")[1]  # Get the image name without (attr, obj) folder\n",
    "            image = os.path.join(\" \".join([attr, obj]), image)\n",
    "            \n",
    "            if (\n",
    "                (attr == \"NA\") or \n",
    "                ((attr, obj) not in self.pairs) or \n",
    "                (settype == \"NA\")\n",
    "            ):\n",
    "                # ignore instances with unlabeled attributes\n",
    "                # ignore instances that are not in current split\n",
    "                continue\n",
    "\n",
    "            data_i = {\n",
    "                \"image_path\": image, \n",
    "                \"attr\": attr, \n",
    "                \"obj\": obj,\n",
    "                \"pair\": (attr, obj),\n",
    "                \"attr_id\": self.attr2idx[attr],\n",
    "                \"obj_id\": self.obj2idx[obj],\n",
    "                \"pair_id\": self.pair2idx[(attr, obj)],\n",
    "            }\n",
    "            if settype == \"train\":\n",
    "                train_data.append(data_i)\n",
    "            elif settype == \"val\":\n",
    "                val_data.append(data_i)\n",
    "            else:\n",
    "                test_data.append(data_i)\n",
    "                \n",
    "        return train_data, val_data, test_data\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        index = self.sample_indices[index]\n",
    "        return self.data[index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "67495806",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train pairs: 1262 | valid pairs: 600 | test pairs: 800\n",
      "train images: 30338 | valid images: 10420 | test images: 12995\n",
      "split size: 30338\n"
     ]
    }
   ],
   "source": [
    "root = \"../../data/mit_states\"\n",
    "split = \"train\"\n",
    "data = MITStatesDataset(root=root, split=split)\n",
    "print(f\"split size: {len(data)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51440db7",
   "metadata": {},
   "source": [
    "# Retrieval Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1f2a49b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "# OLD!!\n",
    "class RetrievalModel(nn.Module):\n",
    "    def __init__(self, data, input_dim, args):\n",
    "        super(RetrievalModel, self).__init__()\n",
    "        self.input_dim = input_dim\n",
    "        self.args = args\n",
    "        \n",
    "        self.obj2idx, self.attr2idx, self.pair2idx = data.obj2idx, data.attr2idx, data.pair2idx\n",
    "        self.train_pairs, self.valid_pairs, self.test_pairs = data.train_pairs, data.valid_pairs, data.test_pairs\n",
    "        self.limit_pairs = set(self.train_pairs)\n",
    "        \n",
    "        self.text_input_dim = args.text_input_dim\n",
    "        self.logit_scale = nn.Parameter(torch.ones([]) * np.log(1 / args.logit_scale))\n",
    "        self.attr_encoder = nn.Embedding(len(data.attrs), self.text_input_dim)\n",
    "        self.obj_encoder = nn.Embedding(len(data.objs), self.text_input_dim)\n",
    "        self.text_projection = nn.Linear(self.text_input_dim*2, input_dim)\n",
    "        self.image_projection = nn.Linear(input_dim, input_dim)\n",
    "    \n",
    "    def get_limit_pair_inputs(self):\n",
    "        attr_inputs, obj_inputs = [],[]\n",
    "        for attr, obj in self.limit_pairs:\n",
    "            attr_id = self.attr2idx[attr]\n",
    "            obj_id = self.obj2idx[obj]\n",
    "            attr_inputs.append(attr_id)\n",
    "            obj_inputs.append(obj_id)\n",
    "        attr_inputs = torch.LongTensor(attr_inputs).to(device)\n",
    "        obj_inputs = torch.LongTensor(obj_inputs).to(device)\n",
    "        return attr_inputs, obj_inputs\n",
    "\n",
    "    def get_all_pair_inputs(self):\n",
    "        attr_inputs, obj_inputs = [],[]\n",
    "        for attr, obj in self.pair2idx.keys():\n",
    "            attr_id = self.attr2idx[attr]\n",
    "            obj_id = self.obj2idx[obj]\n",
    "            attr_inputs.append(attr_id)\n",
    "            obj_inputs.append(obj_id)\n",
    "        attr_inputs = torch.LongTensor(attr_inputs).to(device)\n",
    "        obj_inputs = torch.LongTensor(obj_inputs).to(device)\n",
    "        return attr_inputs, obj_inputs\n",
    "    \n",
    "    def forward(self, image_embs, pair_labels=None, is_train=True):\n",
    "        # image_embs refers to image embeddings or concept representations\n",
    "        attrs, objs = self.get_all_pair_inputs()\n",
    "        #attrs, objs = self.get_limit_pair_inputs() if is_train else self.get_all_pair_inputs()\n",
    "        attr_embs = self.attr_encoder(attrs)\n",
    "        obj_embs = self.obj_encoder(objs)\n",
    "        pair_embs = torch.cat([attr_embs, obj_embs], dim=1)\n",
    "        pair_embs = self.text_projection(pair_embs)\n",
    "        pair_embs = F.normalize(pair_embs, dim=1)\n",
    "        \n",
    "        image_embs = self.image_projection(image_embs)\n",
    "        image_embs = F.normalize(image_embs, dim=1)\n",
    "        \n",
    "        logit_scale = self.logit_scale.exp()\n",
    "        logit_scale = logit_scale if logit_scale<=100.0 else 100.0\n",
    "        logits = logit_scale * image_embs @ pair_embs.t()\n",
    "        \n",
    "        loss = None\n",
    "        if is_train:\n",
    "            loss_fn = nn.CrossEntropyLoss()\n",
    "            loss = loss_fn(logits, pair_labels)\n",
    "        return logits, loss\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "532309b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# NEW!!\n",
    "class RetrievalModel(nn.Module):\n",
    "    def __init__(self, data, image_dim, limit_pairs, args):\n",
    "        super(RetrievalModel, self).__init__()\n",
    "        self.image_dim = image_dim\n",
    "        self.input_dim = args.input_dim\n",
    "        self.args = args\n",
    "        \n",
    "        self.obj2idx, self.attr2idx, self.pair2idx = data.obj2idx, data.attr2idx, data.pair2idx\n",
    "        self.train_pairs, self.valid_pairs, self.test_pairs = data.train_pairs, data.valid_pairs, data.test_pairs\n",
    "        self.limit_pairs = limit_pairs\n",
    "        \n",
    "        self.logit_scale = nn.Parameter(torch.ones([]) * np.log(1 / args.logit_scale))\n",
    "        self.attr_encoder = nn.Embedding(len(data.attrs), self.input_dim)\n",
    "        self.obj_encoder = nn.Embedding(len(data.objs), self.input_dim)\n",
    "        self.text_projection = nn.Linear(self.input_dim*2, self.image_dim, bias=args.is_bias)\n",
    "        \n",
    "        if self.args.is_image_projection:\n",
    "            self.image_projection = nn.Linear(self.image_dim, self.image_dim, bias=args.is_bias)\n",
    "    \n",
    "    def get_limit_pair_inputs(self):\n",
    "        attr_inputs, obj_inputs = [],[]\n",
    "        for attr, obj in self.limit_pairs:\n",
    "            attr_id = self.attr2idx[attr]\n",
    "            obj_id = self.obj2idx[obj]\n",
    "            attr_inputs.append(attr_id)\n",
    "            obj_inputs.append(obj_id)\n",
    "        attr_inputs = torch.LongTensor(attr_inputs).to(device)\n",
    "        obj_inputs = torch.LongTensor(obj_inputs).to(device)\n",
    "        return attr_inputs, obj_inputs\n",
    "\n",
    "    def get_all_pair_inputs(self):\n",
    "        attr_inputs, obj_inputs = [],[]\n",
    "        for attr, obj in self.pair2idx.keys():\n",
    "            attr_id = self.attr2idx[attr]\n",
    "            obj_id = self.obj2idx[obj]\n",
    "            attr_inputs.append(attr_id)\n",
    "            obj_inputs.append(obj_id)\n",
    "        attr_inputs = torch.LongTensor(attr_inputs).to(device)\n",
    "        obj_inputs = torch.LongTensor(obj_inputs).to(device)\n",
    "        return attr_inputs, obj_inputs\n",
    "    \n",
    "    def forward(self, image_embs, pair_labels=None, is_train=True):\n",
    "        # image_embs refers to image embeddings or concept representations\n",
    "        #attrs, objs = self.get_all_pair_inputs()\n",
    "        attrs, objs = self.get_limit_pair_inputs() if is_train else self.get_all_pair_inputs()\n",
    "        attr_embs = self.attr_encoder(attrs)\n",
    "        obj_embs = self.obj_encoder(objs)\n",
    "        pair_embs = torch.cat([attr_embs, obj_embs], dim=1)\n",
    "        pair_embs = self.text_projection(pair_embs)\n",
    "        pair_embs = F.normalize(pair_embs, dim=1)\n",
    "        \n",
    "        if self.args.is_image_projection:\n",
    "            image_embs = self.image_projection(image_embs.float())\n",
    "        image_embs = F.normalize(image_embs, dim=1).float()\n",
    "        \n",
    "        logit_scale = self.logit_scale.exp()\n",
    "        logit_scale = logit_scale if logit_scale<=100.0 else 100.0\n",
    "        logits = logit_scale * image_embs @ pair_embs.t()\n",
    "        \n",
    "        loss = None\n",
    "        if is_train:\n",
    "            loss_fn = nn.CrossEntropyLoss()\n",
    "            loss = loss_fn(logits, pair_labels)\n",
    "        return logits, loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "24506ef2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_split_labels(split, data, limit_pair2idx, is_limit=True):\n",
    "    if split == \"train\":\n",
    "        split_data = data.train_data\n",
    "    elif split == \"valid\":\n",
    "        split_data = data.valid_data\n",
    "    elif split == \"test\":\n",
    "        split_data = data.test_data\n",
    "    lables_attr = [sample[\"attr_id\"] for sample in split_data]\n",
    "    lables_obj = [sample[\"obj_id\"] for sample in split_data]\n",
    "    \n",
    "    pair2idx = limit_pair2idx if is_limit else data.pair2idx\n",
    "    labels = []\n",
    "    for attr_id, obj_id in zip(lables_attr, lables_obj):\n",
    "        attr = data.idx2attr[attr_id]\n",
    "        obj = data.idx2obj[obj_id]\n",
    "        labels.append(pair2idx[(attr, obj)])\n",
    "    labels = torch.LongTensor(labels_train)\n",
    "    return labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "13fa720d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run this!\n",
    "labels_text = [sample[\"pair\"] for sample in data.train_data]\n",
    "limit_pair2idx = {}\n",
    "for label in labels_text:\n",
    "    if label not in limit_pair2idx:\n",
    "        limit_pair2idx[label] = len(limit_pair2idx)\n",
    "\n",
    "labels = [limit_pair2idx[(attr, obj)] for attr, obj in labels_text]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfb9de50",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get labels on train split \n",
    "labels_train_text = [sample[\"pair\"] for sample in data.train_data]\n",
    "limit_pair2idx = {}\n",
    "for label in labels_train_text:\n",
    "    if label not in limit_pair2idx:\n",
    "        limit_pair2idx[label] = len(limit_pair2idx)\n",
    "\n",
    "labels = [limit_pair2idx[(attr, obj)] for attr, obj in labels_text]\n",
    "labels_train = torch.LongTensor(labels_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64d75c7d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "torch.HalfTensor([10]) @ torch.FloatTensor([9])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "id": "67b29d75",
   "metadata": {},
   "outputs": [],
   "source": [
    "labels_train = get_split_labels(\"train\", data, limit_pair2idx, is_limit=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b1a82ee",
   "metadata": {},
   "source": [
    "## 1. Pair Actv. (Limit to Seen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b07f8696",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "seen_indices: 1844 | unseen_indices: 8576\n",
      "pair \t| train (30338, 1962) \t| valid (10420, 1962) \t| test (12995, 1962)\n",
      "features: (10420, 1262)\n"
     ]
    }
   ],
   "source": [
    "exp_name = \"retrieval_limit_pair_actv\"\n",
    "split = \"valid\"\n",
    "seed = 0\n",
    "\n",
    "seen_mask = torch.BoolTensor([1 if pair in data.train_pairs else 0 for pair in data.pairs])\n",
    "seen_ids, unseen_ids = get_seen_unseen_indices(split, data)\n",
    "pair_actvs = get_precomputed_features(\"pair\", is_softmax=False)\n",
    "\n",
    "# Limit to seen pair activations\n",
    "pair_actvs_limit = tuple(t[:, seen_mask] for t in pair_actvs)\n",
    "pair_actv_train, pair_actv_valid, pair_actv_test = tuple(normalize(t) for t in pair_actvs_limit)\n",
    "\n",
    "if split == \"train\":\n",
    "    features = pair_actv_train\n",
    "    split_data, split_pairs = data.train_data, data.train_pairs\n",
    "elif split == \"valid\":\n",
    "    features = pair_actv_valid\n",
    "    split_data, split_pairs = data.valid_data, data.valid_pairs\n",
    "elif split == \"test\":\n",
    "    features = pair_actv_test\n",
    "    split_data, split_pairs = data.test_data, data.test_pairs\n",
    "\n",
    "labels_attr_train = [sample[\"attr_id\"] for sample in data.train_data]\n",
    "labels_obj_train = [sample[\"obj_id\"] for sample in data.train_data]\n",
    "labels_train = torch.LongTensor([sample[\"pair_id\"] for sample in data.train_data])\n",
    "\n",
    "labels_attr = [sample[\"attr_id\"] for sample in split_data]\n",
    "labels_obj = [sample[\"obj_id\"] for sample in split_data]\n",
    "labels = [sample[\"pair_id\"] for sample in split_data]\n",
    "\n",
    "print(f\"features: {features.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ac585b4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.optim import Adam\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "88bc4fbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "class parseArguments:\n",
    "    def __init__(self):\n",
    "        self.feature = \"primitive\"\n",
    "        self.model = \"vilt\"\n",
    "        self.train_warmup = 0\n",
    "        self.is_image_projection = False\n",
    "        self.is_bias = False\n",
    "        \n",
    "        self.data_root = \"../../data/mit_states\"\n",
    "        self.emb_root = \"../../data\"\n",
    "        self.precomputed_data_root = f\"../../outputs/mit_states/{self.model}_precompute_features\"\n",
    "        \n",
    "        self.emb_init = True\n",
    "        self.input_dim = 600\n",
    "        self.num_epochs = 400\n",
    "        self.batch_size = 16\n",
    "        self.learning_rate = 5e-5\n",
    "        self.weight_decay = 5e-5\n",
    "        self.logit_scale = 0.07\n",
    "\n",
    "args = parseArguments()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8a9f731f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_overall_metrics(features, labels, seen_ids, unseen_ids, data, topk_list=[1]):\n",
    "    overall_metrics = {}\n",
    "    for topk in topk_list:\n",
    "        # Get model's performance (accuracy) on seen/unseen pairs\n",
    "        bias = 1e3\n",
    "        results = generate_predictions(features, labels, seen_ids, unseen_ids, data, topk=topk, bias=bias)\n",
    "        full_unseen_metrics = evaluate(results)\n",
    "        all_preds, seen_preds, unseen_preds = results[\"all_preds\"], results[\"seen_preds\"], results[\"unseen_preds\"]\n",
    "\n",
    "        # Get predicted probability distribution of unseen pairs,\n",
    "        # and the top K scores of seen pairs in the predicted prob. distribution of unseen pairs\n",
    "        correct_scores = features[np.arange(len(features)), labels][unseen_ids]\n",
    "        max_seen_scores = features[unseen_ids][:, data.seen_mask].topk(topk, dim=1)[0][:,topk-1]\n",
    "\n",
    "        # Compute biases\n",
    "        unseen_score_diff = max_seen_scores - correct_scores\n",
    "        correct_unseen_score_diff = unseen_score_diff[unseen_preds] - 1e-4\n",
    "        correct_unseen_score_diff = torch.sort(correct_unseen_score_diff)[0]\n",
    "        magic_binsize = 20\n",
    "        bias_skip = max(len(correct_unseen_score_diff) // magic_binsize, 1)\n",
    "        bias_list = correct_unseen_score_diff[::bias_skip]\n",
    "\n",
    "        # Get biased predictions and metrics with different biases\n",
    "        all_metrics = []\n",
    "        for bias in bias_list:\n",
    "            results = generate_predictions(features, labels, seen_ids, unseen_ids, data, topk=topk, bias=bias)\n",
    "            metrics = evaluate(results)\n",
    "            all_metrics.append(metrics)\n",
    "        all_metrics.append(full_unseen_metrics)\n",
    "\n",
    "        # Compute overall metrics\n",
    "        seen_accs = np.array([metric_dict[\"seen_acc\"] for metric_dict in all_metrics])\n",
    "        unseen_accs = np.array([metric_dict[\"unseen_acc\"] for metric_dict in all_metrics])\n",
    "        best_seen_acc = max([metric_dict[\"seen_acc\"] for metric_dict in all_metrics])\n",
    "        best_unseen_acc = max([metric_dict[\"unseen_acc\"] for metric_dict in all_metrics])\n",
    "        best_harmonic_mean = max([metric_dict[\"harmonic_mean\"] for metric_dict in all_metrics])\n",
    "        auc = np.trapz(seen_accs, unseen_accs)\n",
    "        #print(f\"best_seen_acc: {best_seen_acc:6.4f}\")\n",
    "        #print(f\"best_unseen_acc: {best_unseen_acc:6.4f}\")\n",
    "        #print(f\"best_harmonic_mean: {best_harmonic_mean:6.4f}\")\n",
    "        #print(f\"auc: {auc:6.4f}\")\n",
    "\n",
    "        overall_metrics[topk] = {\n",
    "            \"seen_accs\": seen_accs.tolist(),\n",
    "            \"unseen_accs\": unseen_accs.tolist(),\n",
    "            \"best_seen_acc\": best_seen_acc,\n",
    "            \"best_unseen_acc\": best_unseen_acc,\n",
    "            \"best_harmonic_mean\": best_harmonic_mean,\n",
    "            \"auc\": auc,\n",
    "        }\n",
    "    return overall_metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "80079e0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# NEW!!\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "class Precomputed_MITStatesDataset(Dataset):\n",
    "    def __init__(self, split, feature, data, args, is_limit=True):\n",
    "        self.seen_mask = torch.BoolTensor([1 if pair in data.train_pairs else 0 for pair in data.pairs])\n",
    "        if feature == \"primitive\":\n",
    "            attr_actvs_tuple = get_precomputed_features(\"attr\", args, is_softmax=False)\n",
    "            obj_actvs_tuple = get_precomputed_features(\"obj\", args, is_softmax=False)\n",
    "            attr_actvs_train, attr_actvs_valid, attr_actvs_test = attr_actvs_tuple\n",
    "            obj_actvs_train, obj_actvs_valid, obj_actvs_test = obj_actvs_tuple\n",
    "            image_embs_train = np.concatenate([attr_actvs_train, obj_actvs_train], axis=-1)\n",
    "            image_embs_valid = np.concatenate([attr_actvs_valid, obj_actvs_valid], axis=-1)\n",
    "            image_embs_test = np.concatenate([attr_actvs_test, obj_actvs_test], axis=-1)\n",
    "        elif feature == \"pair\":\n",
    "            image_embs_tuple = get_precomputed_features(\"pair\", args, is_softmax=False)\n",
    "            if (is_limit) and (args.model == \"tmn\"):\n",
    "                image_embs_tuple = tuple(t[:, self.seen_mask] for t in image_embs_tuple)\n",
    "            #image_embs_train, image_embs_valid, image_embs_test = tuple(normalize(t) for t in image_embs_tuple)\n",
    "            image_embs_train, image_embs_valid, image_embs_test = image_embs_tuple\n",
    "        elif feature == \"all\":\n",
    "            attr_actvs_tuple = get_precomputed_features(\"attr\", args, is_softmax=False)\n",
    "            obj_actvs_tuple = get_precomputed_features(\"obj\", args, is_softmax=False)\n",
    "            pair_actvs_tuple = get_precomputed_features(\"pair\", args, is_softmax=False)\n",
    "            if (is_limit) and (args.model == \"tmn\"):\n",
    "                pair_actvs_tuple = tuple(t[:, self.seen_mask] for t in pair_actvs_tuple)\n",
    "            attr_actvs_train, attr_actvs_valid, attr_actvs_test = attr_actvs_tuple\n",
    "            obj_actvs_train, obj_actvs_valid, obj_actvs_test = obj_actvs_tuple\n",
    "            pair_actvs_train, pair_actvs_valid, pair_actvs_test = pair_actvs_tuple\n",
    "            image_embs_train = np.concatenate([attr_actvs_train, obj_actvs_train, pair_actvs_train], axis=-1)\n",
    "            image_embs_valid = np.concatenate([attr_actvs_valid, obj_actvs_valid, pair_actvs_valid], axis=-1)\n",
    "            image_embs_test = np.concatenate([attr_actvs_test, obj_actvs_test, pair_actvs_test], axis=-1)\n",
    "\n",
    "        labels = None\n",
    "        if split == \"train\":\n",
    "            image_embs = image_embs_train\n",
    "            if is_limit:\n",
    "                labels_text = [sample[\"pair\"] for sample in data.train_data]\n",
    "                limit_pair2idx = {}\n",
    "                for label in labels_text:\n",
    "                    if label not in limit_pair2idx:\n",
    "                        limit_pair2idx[label] = len(limit_pair2idx)\n",
    "                labels = [limit_pair2idx[(attr, obj)] for attr, obj in labels_text]\n",
    "                self.limit_pair2idx = limit_pair2idx\n",
    "            else:\n",
    "                labels = [sample[\"pair_id\"] for sample in data.train_data]\n",
    "        elif split == \"valid\":\n",
    "            image_embs = image_embs_valid\n",
    "            labels = [sample[\"pair_id\"] for sample in data.valid_data]\n",
    "        elif split == \"test\":\n",
    "            image_embs = image_embs_test\n",
    "            labels = [sample[\"pair_id\"] for sample in data.test_data]\n",
    "        self.image_embs = image_embs\n",
    "        self.labels = np.array(labels)\n",
    "        self.image_dim = self.image_embs.shape[-1]\n",
    "        print(f\"image_dim: {self.image_dim:6d}\")\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_embs)\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        image_embs = self.image_embs[index]\n",
    "        labels = self.labels[index]\n",
    "        return image_embs, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "d143edde",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train pairs: 1262 | valid pairs: 600 | test pairs: 800\n",
      "train images: 30338 | valid images: 10420 | test images: 12995\n",
      "seen_indices: 1844 | unseen_indices: 8576\n",
      "seen_indices: 2380 | unseen_indices: 10615\n",
      "pair \t| train (30338, 1262) \t| valid (10420, 1262) \t| test (12995, 1262)\n",
      "image_dim:   1262\n",
      "pair \t| train (30338, 1262) \t| valid (10420, 1262) \t| test (12995, 1262)\n",
      "image_dim:   1262\n",
      "pair \t| train (30338, 1262) \t| valid (10420, 1262) \t| test (12995, 1262)\n",
      "image_dim:   1262\n"
     ]
    }
   ],
   "source": [
    "# Load dataset\n",
    "data = MITStatesDataset(root=args.data_root, split=\"train\")  # split can be ignored here\n",
    "seen_ids_valid, unseen_ids_valid = get_seen_unseen_indices(\"valid\", data)\n",
    "seen_ids_test, unseen_ids_test = get_seen_unseen_indices(\"test\", data)\n",
    "\n",
    "train_dataset = Precomputed_MITStatesDataset(split=\"train\", feature=\"pair\", data=data, args=args, is_limit=True)\n",
    "valid_dataset = Precomputed_MITStatesDataset(split=\"valid\", feature=\"pair\", data=data, args=args, is_limit=True)\n",
    "test_dataset = Precomputed_MITStatesDataset(split=\"test\", feature=\"pair\", data=data, args=args, is_limit=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "2e08074c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_fasttext_embeddings(vocab, args):\n",
    "    custom_map = {\n",
    "        'Faux.Fur': 'fake fur',\n",
    "        'Faux.Leather': 'fake leather',\n",
    "        'Full.grain.leather': 'thick leather',\n",
    "        'Hair.Calf': 'hairy leather',\n",
    "        'Patent.Leather': 'shiny leather',\n",
    "        'Boots.Ankle': 'ankle boots',\n",
    "        'Boots.Knee.High': 'kneehigh boots',\n",
    "        'Boots.Mid-Calf': 'midcalf boots',\n",
    "        'Shoes.Boat.Shoes': 'boatshoes',\n",
    "        'Shoes.Clogs.and.Mules': 'clogs shoes',\n",
    "        'Shoes.Flats': 'flats shoes',\n",
    "        'Shoes.Heels': 'heels',\n",
    "        'Shoes.Loafers': 'loafers',\n",
    "        'Shoes.Oxfords': 'oxford shoes',\n",
    "        'Shoes.Sneakers.and.Athletic.Shoes': 'sneakers',\n",
    "        'traffic_light': 'traficlight',\n",
    "        'trash_can': 'trashcan',\n",
    "        'dry-erase_board' : 'dry_erase_board',\n",
    "        'black_and_white' : 'black_white',\n",
    "        'eiffel_tower' : 'tower'\n",
    "    }\n",
    "    vocab_lower = [v.lower() for v in vocab]\n",
    "    vocab = []\n",
    "    for current in vocab_lower:\n",
    "        if current in custom_map:\n",
    "            vocab.append(custom_map[current])\n",
    "        else:\n",
    "            vocab.append(current)\n",
    "\n",
    "    import fasttext.util\n",
    "    ft = fasttext.load_model(args.emb_root+'/fasttext/cc.en.300.bin')\n",
    "    embeds = []\n",
    "    for k in vocab:\n",
    "        if '_' in k:\n",
    "            ks = k.split('_')\n",
    "            emb = np.stack([ft.get_word_vector(it) for it in ks]).mean(axis=0)\n",
    "        else:\n",
    "            emb = ft.get_word_vector(k)\n",
    "        embeds.append(emb)\n",
    "\n",
    "    embeds = torch.Tensor(np.stack(embeds))\n",
    "    print('Fasttext Embeddings loaded, total embeddings: {}'.format(embeds.size()))\n",
    "    return embeds\n",
    "\n",
    "def load_word2vec_embeddings(vocab, args):\n",
    "    # vocab = [v.lower() for v in vocab]\n",
    "\n",
    "    from gensim import models\n",
    "    model = models.KeyedVectors.load_word2vec_format(\n",
    "        args.emb_root+'/word2vec/GoogleNews-vectors-negative300.bin', binary=True\n",
    "    )\n",
    "\n",
    "    custom_map = {\n",
    "        'Faux.Fur': 'fake_fur',\n",
    "        'Faux.Leather': 'fake_leather',\n",
    "        'Full.grain.leather': 'thick_leather',\n",
    "        'Hair.Calf': 'hair_leather',\n",
    "        'Patent.Leather': 'shiny_leather',\n",
    "        'Boots.Ankle': 'ankle_boots',\n",
    "        'Boots.Knee.High': 'knee_high_boots',\n",
    "        'Boots.Mid-Calf': 'midcalf_boots',\n",
    "        'Shoes.Boat.Shoes': 'boat_shoes',\n",
    "        'Shoes.Clogs.and.Mules': 'clogs_shoes',\n",
    "        'Shoes.Flats': 'flats_shoes',\n",
    "        'Shoes.Heels': 'heels',\n",
    "        'Shoes.Loafers': 'loafers',\n",
    "        'Shoes.Oxfords': 'oxford_shoes',\n",
    "        'Shoes.Sneakers.and.Athletic.Shoes': 'sneakers',\n",
    "        'traffic_light': 'traffic_light',\n",
    "        'trash_can': 'trashcan',\n",
    "        'dry-erase_board' : 'dry_erase_board',\n",
    "        'black_and_white' : 'black_white',\n",
    "        \"eiffel_tower\" : \"tower\"\n",
    "    }\n",
    "\n",
    "    embeds = []\n",
    "    for k in vocab:\n",
    "        if k in custom_map:\n",
    "            k = custom_map[k]\n",
    "        if \"_\" in k and k not in model:\n",
    "            ks = k.split(\"_\")\n",
    "            emb = np.stack([model[it] for it in ks]).mean(axis=0)\n",
    "        else:\n",
    "            emb = model[k]\n",
    "        embeds.append(emb)\n",
    "    embeds = torch.Tensor(np.stack(embeds))\n",
    "    print(\"Word2Vec Embeddings loaded, total embeddings: {}\".format(embeds.size()))\n",
    "    return embeds\n",
    "\n",
    "def get_pretrained_weights(vocab, args):\n",
    "    embeds1 = load_fasttext_embeddings(vocab, args)\n",
    "    embeds2 = load_word2vec_embeddings(vocab, args)\n",
    "    embeds = torch.cat([embeds1, embeds2], dim = 1)\n",
    "    print(\"Combined embeddings are \",embeds.shape)\n",
    "    return embeds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74694ac9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get retrieval model and optimizer\n",
    "limit_pairs = list(train_dataset.limit_pair2idx.keys())\n",
    "model = RetrievalModel(data, train_dataset.image_dim, limit_pairs, args)\n",
    "#model = RetrievalModel(data, train_dataset.input_dim, args)\n",
    "model.to(device)\n",
    "optimizer = Adam(model.parameters(), lr=args.learning_rate, weight_decay=args.weight_decay)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "51f0890a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RetrievalModel(\n",
       "  (attr_encoder): Embedding(115, 600)\n",
       "  (obj_encoder): Embedding(245, 600)\n",
       "  (text_projection): Linear(in_features=1200, out_features=1262, bias=True)\n",
       "  (image_projection): Linear(in_features=1262, out_features=1262, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "a46e3fab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fasttext Embeddings loaded, total embeddings: torch.Size([245, 300])\n",
      "Word2Vec Embeddings loaded, total embeddings: torch.Size([245, 300])\n"
     ]
    }
   ],
   "source": [
    "pretrained_weight = get_pretrained_weights(data.attrs, args)\n",
    "model.attr_encoder.weight.data.copy_(pretrained_weight)\n",
    "pretrained_weight = get_pretrained_weights(data.objs, args)\n",
    "model.obj_encoder.weight.data.copy_(pretrained_weight)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "id": "e30d417b",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_auc = float(\"-inf\")\n",
    "best_epoch_id = 0\n",
    "best_overall_metrics = {}\n",
    "#for epoch_id in range(args.num_epochs):\n",
    "for epoch_id in range(1):\n",
    "    # Train model\n",
    "    model.train()\n",
    "    loader = DataLoader(train_dataset, shuffle=True, pin_memory=True, batch_size=args.batch_size, drop_last=True)\n",
    "    for iter_id, batch in enumerate(loader):\n",
    "        image_embs, labels = tuple(t.to(device) for t in batch)\n",
    "        logits, loss = model(image_embs, labels, is_train=True)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        model.zero_grad()\n",
    "        break\n",
    "        #if args.verbose and (iter_id % args.report_step == 0):\n",
    "        #    logger.info(f\"\")\n",
    "\n",
    "    # Early stop and Evaluation\n",
    "    model.eval()\n",
    "    loader = DataLoader(valid_dataset, shuffle=False, pin_memory=True, batch_size=args.batch_size, drop_last=False)\n",
    "    features = np.zeros([len(valid_dataset), len(data.pairs)])\n",
    "    for iter_id, batch in enumerate(loader):\n",
    "        image_embs, labels = tuple(t.to(device) for t in batch)\n",
    "        logits, loss = model(image_embs, labels, is_train=False)\n",
    "        prob = logits.softmax(dim=-1).log()\n",
    "        features[iter_id*args.batch_size:(iter_id+1)*args.batch_size] = prob.detach().cpu().numpy().copy()\n",
    "    features = torch.tensor(features)\n",
    "\n",
    "    labels = valid_dataset.labels\n",
    "    overall_metrics = get_overall_metrics(\n",
    "        features, labels, seen_ids_valid, unseen_ids_valid, data, topk_list=[1],\n",
    "    )\n",
    "    auc = overall_metrics[1][\"auc\"]\n",
    "    if auc > best_auc:\n",
    "        best_auc = auc\n",
    "        best_epoch_id = epoch_id\n",
    "        best_overall_metrics\n",
    "        #torch.save(model.state_dict(), args.ckpt_path)\n",
    "\n",
    "# Load best checkpoint and test\n",
    "#model = RetrievalModel(data, train_dataset.input_dim, args)\n",
    "#model.load_state_dict(torch.load(args.ckpt_path))\n",
    "#model.to(device)\n",
    "\n",
    "model.eval()\n",
    "loader = DataLoader(test_dataset, shuffle=False, pin_memory=True, batch_size=args.batch_size, drop_last=False)\n",
    "features = np.zeros([len(test_dataset), len(data.pairs)])\n",
    "for iter_id, batch in enumerate(loader):\n",
    "    image_embs, labels = tuple(t.to(device) for t in batch)\n",
    "    logits, loss = model(image_embs, labels, is_train=False)\n",
    "    prob = logits.softmax(dim=-1).log()\n",
    "    features[iter_id*args.batch_size:(iter_id+1)*args.batch_size] = prob.detach().cpu().numpy().copy()\n",
    "features = torch.tensor(features)\n",
    "\n",
    "labels = test_dataset.labels\n",
    "overall_metrics = get_overall_metrics(\n",
    "    features, labels, seen_ids_test, unseen_ids_test, data, \n",
    "    topk_list=[1,2,3],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "id": "1b5e99f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = valid_dataset.labels\n",
    "overall_metrics_valid = get_overall_metrics(\n",
    "    features, labels, seen_ids_valid, unseen_ids_valid, data, topk_list=[1],\n",
    ")\n",
    "auc_valid = overall_metrics_valid[1][\"auc\"]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d35757c",
   "metadata": {},
   "source": [
    "# 2. Intervention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5409e2d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_gt_primitives(split, data, ):\n",
    "    \"\"\" Get groundtruth primtiive concepts. \"\"\"\n",
    "    data_dict = {\n",
    "        \"train\": data.train_data,\n",
    "        \"valid\": data.valid_data,\n",
    "        \"test\": data.test_data,\n",
    "    }\n",
    "    split_data = data_dict[split]\n",
    "    labels_attr = [sample[\"attr_id\"] for sample in split_data]\n",
    "    labels_obj = [sample[\"obj_id\"] for sample in split_data]\n",
    "    gt_features_attr = np.zeros((len(split_data), len(data.attrs)))\n",
    "    gt_features_obj = np.zeros((len(split_data), len(data.objs)))\n",
    "    gt_features_attr[np.arange(len(labels_attr)), labels_attr] = 1\n",
    "    gt_features_obj[np.arange(len(labels_obj)), labels_obj] = 1\n",
    "    gt_features_concat = np.concatenate([gt_features_attr, gt_features_obj], axis=-1)\n",
    "    return gt_features_concat\n",
    "\n",
    "def evaluate(results):\n",
    "    \"\"\" Evaluate predictions and Return metrics. \"\"\"\n",
    "    all_preds, seen_preds, unseen_preds = results[\"all_preds\"], results[\"seen_preds\"], results[\"unseen_preds\"]\n",
    "    all_acc, seen_acc, unseen_acc = np.mean(all_preds), np.mean(seen_preds), np.mean(unseen_preds)    \n",
    "    return {\n",
    "        \"all_acc\": all_acc,\n",
    "        \"seen_acc\": seen_acc,\n",
    "        \"unseen_acc\": unseen_acc,\n",
    "        \"harmonic_mean\": (seen_acc * unseen_acc)**0.5,\n",
    "        \"macro_average_acc\": (seen_acc + unseen_acc)*0.5,\n",
    "    }\n",
    "\n",
    "def generate_predictions(scores, labels, seen_ids, unseen_ids, seen_mask, data, topk, bias=0.0):\n",
    "    \"\"\" Apply bias and Generate predictions for. \"\"\"\n",
    "    def get_predictions(_scores):\n",
    "        # Get predictions\n",
    "        _, pair_preds = _scores.topk(topk, dim=1)\n",
    "        pair_preds = pair_preds[:, :topk].contiguous().view(-1)\n",
    "        attr_preds = all_pairs[pair_preds][:,0].view(-1, topk)\n",
    "        obj_preds = all_pairs[pair_preds][:,1].view(-1, topk)\n",
    "        pair_preds = pair_preds.view(-1, topk)\n",
    "        return pair_preds, attr_preds, obj_preds\n",
    "    \n",
    "    # Get predictions with biases applied\n",
    "    all_pairs = torch.LongTensor([\n",
    "        (data.attr2idx[attr], data.obj2idx[obj]) \n",
    "        for attr, obj in data.pairs\n",
    "    ])\n",
    "    scores = scores.clone()\n",
    "    mask = seen_mask.repeat(scores.shape[0], 1)\n",
    "    scores[~mask] += bias\n",
    "    pair_preds, attr_preds, obj_preds = get_predictions(scores)\n",
    "    \n",
    "    # Get predictions for seen/unseen pairs\n",
    "    all_preds = np.array([label in pair_preds[row_id,:topk] for row_id, label in enumerate(labels)])\n",
    "    seen_preds = all_preds[seen_ids]\n",
    "    unseen_preds = all_preds[unseen_ids]\n",
    "    return {\n",
    "        \"pair_preds\": pair_preds,\n",
    "        \"attr_preds\": attr_preds,\n",
    "        \"obj_preds\": obj_preds,\n",
    "        \"all_preds\": all_preds,\n",
    "        \"seen_preds\": seen_preds,\n",
    "        \"unseen_preds\": unseen_preds,\n",
    "    }\n",
    "\n",
    "def get_overall_metrics(features, labels, seen_ids, unseen_ids, seen_mask, data, topk_list=[1], is_open_world=False):\n",
    "    overall_metrics = {}\n",
    "    for topk in topk_list:\n",
    "        # Get model\"s performance (accuracy) on seen/unseen pairs\n",
    "        bias = 1e3\n",
    "        results = generate_predictions(features, labels, seen_ids, unseen_ids, seen_mask, data, topk=topk, bias=bias)\n",
    "        full_unseen_metrics = evaluate(results)\n",
    "        all_preds, seen_preds, unseen_preds = results[\"all_preds\"], results[\"seen_preds\"], results[\"unseen_preds\"]\n",
    "\n",
    "        # Get predicted probability distribution of unseen pairs,\n",
    "        # and the top K scores of seen pairs in the predicted prob. distribution of unseen pairs\n",
    "        correct_scores = features[np.arange(len(features)), labels][unseen_ids]\n",
    "        max_seen_scores = features[unseen_ids][:, seen_mask].topk(topk, dim=1)[0][:,topk-1]\n",
    "\n",
    "        # Compute biases\n",
    "        unseen_score_diff = max_seen_scores - correct_scores\n",
    "        correct_unseen_score_diff = unseen_score_diff[unseen_preds] - 1e-4\n",
    "        correct_unseen_score_diff = torch.sort(correct_unseen_score_diff)[0]\n",
    "        magic_binsize = 20\n",
    "        bias_skip = max(len(correct_unseen_score_diff) // magic_binsize, 1)\n",
    "        bias_list = correct_unseen_score_diff[::bias_skip]\n",
    "\n",
    "        # Get biased predictions and metrics with different biases\n",
    "        all_metrics = []\n",
    "        for bias in bias_list:\n",
    "            results = generate_predictions(features, labels, seen_ids, unseen_ids, seen_mask, data, topk=topk, bias=bias)\n",
    "            metrics = evaluate(results)\n",
    "            all_metrics.append(metrics)\n",
    "        all_metrics.append(full_unseen_metrics)\n",
    "\n",
    "        # Compute overall metrics\n",
    "        seen_accs = np.array([metric_dict[\"seen_acc\"] for metric_dict in all_metrics])\n",
    "        unseen_accs = np.array([metric_dict[\"unseen_acc\"] for metric_dict in all_metrics])\n",
    "        best_seen_acc = max([metric_dict[\"seen_acc\"] for metric_dict in all_metrics])\n",
    "        best_unseen_acc = max([metric_dict[\"unseen_acc\"] for metric_dict in all_metrics])\n",
    "        best_harmonic_mean = max([metric_dict[\"harmonic_mean\"] for metric_dict in all_metrics])\n",
    "        auc = np.trapz(seen_accs, unseen_accs)\n",
    "        print(f\"topk: {topk}\")\n",
    "        print(f\"best_seen_acc: {best_seen_acc:6.4f}\")\n",
    "        print(f\"best_unseen_acc: {best_unseen_acc:6.4f}\")\n",
    "        print(f\"best_harmonic_mean: {best_harmonic_mean:6.4f}\")\n",
    "        print(f\"auc: {auc:6.4f}\")\n",
    "\n",
    "        overall_metrics[topk] = {\n",
    "            \"seen_accs\": seen_accs.tolist(),\n",
    "            \"unseen_accs\": unseen_accs.tolist(),\n",
    "            \"best_seen_acc\": best_seen_acc,\n",
    "            \"best_unseen_acc\": best_unseen_acc,\n",
    "            \"best_harmonic_mean\": best_harmonic_mean,\n",
    "            \"auc\": auc,\n",
    "        }\n",
    "    return overall_metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c03c8b09",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Precomputed_MITStatesDataset(Dataset):\n",
    "    def __init__(self, split, feature, data, args, is_limit=True):\n",
    "        # Load precomputed features with temporary seen_mask\n",
    "        self.seen_mask = torch.BoolTensor([1 if pair in data.train_pairs else 0 for pair in data.pairs])\n",
    "        if feature == \"primitive\":\n",
    "            attr_actvs_tuple = get_precomputed_features(\"attr\", args, is_softmax=False)\n",
    "            obj_actvs_tuple = get_precomputed_features(\"obj\", args, is_softmax=False)\n",
    "            attr_actvs_train, attr_actvs_valid, attr_actvs_test = attr_actvs_tuple\n",
    "            obj_actvs_train, obj_actvs_valid, obj_actvs_test = obj_actvs_tuple\n",
    "            image_embs_train = np.concatenate([attr_actvs_train, obj_actvs_train], axis=-1)\n",
    "            image_embs_valid = np.concatenate([attr_actvs_valid, obj_actvs_valid], axis=-1)\n",
    "            image_embs_test = np.concatenate([attr_actvs_test, obj_actvs_test], axis=-1)\n",
    "        elif feature == \"pair\":\n",
    "            image_embs_tuple = get_precomputed_features(\"pair\", args, is_softmax=False)\n",
    "            if is_limit:\n",
    "                image_embs_tuple = tuple(t[:, self.seen_mask] for t in image_embs_tuple)\n",
    "            #image_embs_train, image_embs_valid, image_embs_test = tuple(normalize(t) for t in image_embs_tuple)\n",
    "            image_embs_train, image_embs_valid, image_embs_test = image_embs_tuple\n",
    "        elif feature == \"all\":\n",
    "            attr_actvs_tuple = get_precomputed_features(\"attr\", args, is_softmax=False)\n",
    "            obj_actvs_tuple = get_precomputed_features(\"obj\", args, is_softmax=False)\n",
    "            pair_actvs_tuple = get_precomputed_features(\"pair\", args, is_softmax=False)\n",
    "            if is_limit:\n",
    "                pair_actvs_tuple = tuple(t[:, self.seen_mask] for t in pair_actvs_tuple)\n",
    "            attr_actvs_train, attr_actvs_valid, attr_actvs_test = attr_actvs_tuple\n",
    "            obj_actvs_train, obj_actvs_valid, obj_actvs_test = obj_actvs_tuple\n",
    "            pair_actvs_train, pair_actvs_valid, pair_actvs_test = pair_actvs_tuple\n",
    "            image_embs_train = np.concatenate([attr_actvs_train, obj_actvs_train, pair_actvs_train], axis=-1)\n",
    "            image_embs_valid = np.concatenate([attr_actvs_valid, obj_actvs_valid, pair_actvs_valid], axis=-1)\n",
    "            image_embs_test = np.concatenate([attr_actvs_test, obj_actvs_test, pair_actvs_test], axis=-1)\n",
    "        elif feature == \"gt_primitive\":\n",
    "            image_embs_train = get_gt_primitives(\"train\", data)\n",
    "            image_embs_valid = get_gt_primitives(\"valid\", data)\n",
    "            image_embs_test = get_gt_primitives(\"test\", data)\n",
    "        \n",
    "        # Prepare labels\n",
    "        self.limit_pair2idx = self.get_limit_pair2idx(data)\n",
    "        self.open_world_pair2idx = self.get_open_world_pair2idx(split, data)\n",
    "        \n",
    "        labels = None\n",
    "        if split == \"train\":\n",
    "            image_embs = image_embs_train\n",
    "            if args.is_open_world:\n",
    "                labels_text = [sample[\"pair\"] for sample in data.valid_data]\n",
    "                labels = [self.open_world_pair2idx[(attr, obj)] for attr, obj in labels_text]\n",
    "            else:\n",
    "                if is_limit:\n",
    "                    labels_text = [sample[\"pair\"] for sample in data.train_data]\n",
    "                    labels = [self.limit_pair2idx[(attr, obj)] for attr, obj in labels_text]\n",
    "                else:\n",
    "                    labels = [sample[\"pair_id\"] for sample in data.train_data]\n",
    "        elif split == \"valid\":\n",
    "            image_embs = image_embs_valid\n",
    "            if args.is_open_world:\n",
    "                labels_text = [sample[\"pair\"] for sample in data.valid_data]\n",
    "                labels = [self.open_world_pair2idx[(attr, obj)] for attr, obj in labels_text]\n",
    "            else:\n",
    "                labels = [sample[\"pair_id\"] for sample in data.valid_data]\n",
    "        elif split == \"test\":\n",
    "            image_embs = image_embs_test\n",
    "            if args.is_open_world:\n",
    "                labels_text = [sample[\"pair\"] for sample in data.test_data]\n",
    "                labels = [self.open_world_pair2idx[(attr, obj)] for attr, obj in labels_text]\n",
    "            else:\n",
    "                labels = [sample[\"pair_id\"] for sample in data.test_data]\n",
    "        \n",
    "        # Compute seen mask, 1 for training pair, 0 for the others\n",
    "        if args.is_open_world:\n",
    "            self.seen_mask = torch.BoolTensor([1 if pair in data.train_pairs else 0 for pair in self.open_world_pair2idx.keys()])\n",
    "        else:\n",
    "            self.seen_mask = torch.BoolTensor([1 if pair in data.train_pairs else 0 for pair in data.pairs])\n",
    "        \n",
    "        self.image_embs = image_embs\n",
    "        self.labels = np.array(labels)\n",
    "        self.image_dim = self.image_embs.shape[-1]\n",
    "        print(f\"image_dim: {self.image_dim:6d}\")\n",
    "    \n",
    "    def get_limit_pair2idx(self, data):\n",
    "        labels_text = [sample[\"pair\"] for sample in data.train_data]\n",
    "        limit_pair2idx = {}\n",
    "        for label in labels_text:\n",
    "            if label not in limit_pair2idx:\n",
    "                limit_pair2idx[label] = len(limit_pair2idx)\n",
    "        return limit_pair2idx\n",
    "    \n",
    "    def get_open_world_pair2idx(self, split, data):\n",
    "        open_world_pair2idx = {}\n",
    "        for attr in data.attrs:\n",
    "            for obj in data.objs:\n",
    "                if (attr, obj) not in open_world_pair2idx:\n",
    "                    open_world_pair2idx[(attr, obj)] = len(open_world_pair2idx)\n",
    "        return open_world_pair2idx\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.image_embs)\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        image_embs = self.image_embs[index]\n",
    "        labels = self.labels[index]\n",
    "        return image_embs, labels\n",
    "\n",
    "class RetrievalModel(nn.Module):\n",
    "    def __init__(self, data, image_dim, limit_pairs, open_world_pairs, args):\n",
    "        super(RetrievalModel, self).__init__()\n",
    "        self.image_dim = image_dim\n",
    "        self.input_dim = args.input_dim\n",
    "        self.args = args\n",
    "        \n",
    "        self.obj2idx, self.attr2idx, self.pair2idx = data.obj2idx, data.attr2idx, data.pair2idx\n",
    "        self.train_pairs, self.valid_pairs, self.test_pairs = data.train_pairs, data.valid_pairs, data.test_pairs\n",
    "        self.limit_pairs = limit_pairs\n",
    "        self.open_world_pairs = open_world_pairs\n",
    "        \n",
    "        self.logit_scale = nn.Parameter(torch.ones([]) * np.log(1 / args.logit_scale))\n",
    "        self.attr_encoder = nn.Embedding(len(data.attrs), self.input_dim)\n",
    "        self.obj_encoder = nn.Embedding(len(data.objs), self.input_dim)\n",
    "        self.text_projection = nn.Linear(self.input_dim*2, self.image_dim, bias=args.is_bias)\n",
    "        \n",
    "        if self.args.is_image_projection:\n",
    "            self.image_projection = nn.Linear(self.image_dim, self.image_dim, bias=args.is_bias)\n",
    "    \n",
    "    def get_limit_pair_inputs(self):\n",
    "        attr_inputs, obj_inputs = [],[]\n",
    "        for attr, obj in self.limit_pairs:\n",
    "            attr_id = self.attr2idx[attr]\n",
    "            obj_id = self.obj2idx[obj]\n",
    "            attr_inputs.append(attr_id)\n",
    "            obj_inputs.append(obj_id)\n",
    "        attr_inputs = torch.LongTensor(attr_inputs).to(device)\n",
    "        obj_inputs = torch.LongTensor(obj_inputs).to(device)\n",
    "        return attr_inputs, obj_inputs\n",
    "\n",
    "    def get_all_pair_inputs(self):\n",
    "        attr_inputs, obj_inputs = [],[]\n",
    "        for attr, obj in self.pair2idx.keys():\n",
    "            attr_id = self.attr2idx[attr]\n",
    "            obj_id = self.obj2idx[obj]\n",
    "            attr_inputs.append(attr_id)\n",
    "            obj_inputs.append(obj_id)\n",
    "        attr_inputs = torch.LongTensor(attr_inputs).to(device)\n",
    "        obj_inputs = torch.LongTensor(obj_inputs).to(device)\n",
    "        return attr_inputs, obj_inputs\n",
    "    \n",
    "    def get_open_world_pair_inputs(self):\n",
    "        attr_inputs, obj_inputs = [],[]\n",
    "        for attr, obj in self.open_world_pairs:\n",
    "            attr_id = self.attr2idx[attr]\n",
    "            obj_id = self.obj2idx[obj]\n",
    "            attr_inputs.append(attr_id)\n",
    "            obj_inputs.append(obj_id)\n",
    "        attr_inputs = torch.LongTensor(attr_inputs).to(device)\n",
    "        obj_inputs = torch.LongTensor(obj_inputs).to(device)\n",
    "        return attr_inputs, obj_inputs\n",
    "    \n",
    "    def forward(self, image_embs, pair_labels=None, is_train=True, is_intervene=False, is_open_world=False):\n",
    "        # image_embs refers to image embeddings or concept representations\n",
    "        #attrs, objs = self.get_all_pair_inputs()\n",
    "        if is_open_world:\n",
    "            # Open world setting, get all the possible pairs\n",
    "            attrs, objs = self.get_open_world_pair_inputs()\n",
    "        else:\n",
    "            # Close world setting, get all the pairs in the data\n",
    "            attrs, objs = self.get_limit_pair_inputs() if is_train else self.get_all_pair_inputs()\n",
    "        attr_embs = self.attr_encoder(attrs)\n",
    "        obj_embs = self.obj_encoder(objs)\n",
    "        pair_embs = torch.cat([attr_embs, obj_embs], dim=1)\n",
    "        pair_embs = self.text_projection(pair_embs)\n",
    "        pair_embs = F.normalize(pair_embs, dim=1)\n",
    "        \n",
    "        if self.args.is_image_projection:\n",
    "            image_embs = self.image_projection(image_embs.float())\n",
    "        if not is_intervene:\n",
    "            image_embs = F.normalize(image_embs, dim=1).float()\n",
    "        image_embs = image_embs.float()\n",
    "        \n",
    "        logit_scale = self.logit_scale.exp()\n",
    "        logit_scale = logit_scale if logit_scale<=100.0 else 100.0\n",
    "        logits = logit_scale * image_embs @ pair_embs.t()\n",
    "        \n",
    "        loss = None\n",
    "        if is_train:\n",
    "            loss_fn = nn.CrossEntropyLoss()\n",
    "            loss = loss_fn(logits, pair_labels)\n",
    "        return logits, loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e9574ff",
   "metadata": {},
   "source": [
    "## 2.0. GT -> GT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "id": "ab0d8491",
   "metadata": {},
   "outputs": [],
   "source": [
    "class parseArguments:\n",
    "    def __init__(self):\n",
    "        self.split = \"valid\"\n",
    "        self.is_open_world = False\n",
    "        \n",
    "        self.feature = \"gt_primitive\"\n",
    "        self.model = \"gt\"\n",
    "        self.train_warmup = 0\n",
    "        self.is_image_projection = True\n",
    "        self.is_bias = False\n",
    "        self.is_limit = True\n",
    "        \n",
    "        self.data_root = \"../../data/mit_states\"\n",
    "        self.emb_root = \"../../data\"\n",
    "        self.precomputed_data_root = f\"../../outputs/mit_states/{self.model}_precompute_features\"\n",
    "        \n",
    "        self.emb_init = True\n",
    "        self.input_dim = 600\n",
    "        self.num_epochs = 100\n",
    "        self.batch_size = 128\n",
    "        self.learning_rate = 5e-5\n",
    "        self.weight_decay = 5e-5\n",
    "        self.logit_scale = 0.07\n",
    "\n",
    "args = parseArguments()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "id": "400fd88b",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train pairs: 1262 | valid pairs: 600 | test pairs: 800\n",
      "train images: 30338 | valid images: 10420 | test images: 12995\n",
      "seen_indices: 1844 | unseen_indices: 8576\n",
      "seen_indices: 2380 | unseen_indices: 10615\n",
      "image_dim:    360\n",
      "image_dim:    360\n",
      "image_dim:    360\n"
     ]
    }
   ],
   "source": [
    "# Load dataset\n",
    "data = MITStatesDataset(root=args.data_root, split=\"train\")  # split can be ignored here\n",
    "seen_ids_valid, unseen_ids_valid = get_seen_unseen_indices(\"valid\", data)\n",
    "seen_ids_test, unseen_ids_test = get_seen_unseen_indices(\"test\", data)\n",
    "\n",
    "train_dataset = Precomputed_MITStatesDataset(split=\"train\", feature=\"gt_primitive\", data=data, args=args, is_limit=True)\n",
    "valid_dataset = Precomputed_MITStatesDataset(split=\"valid\", feature=\"gt_primitive\", data=data, args=args, is_limit=True)\n",
    "test_dataset = Precomputed_MITStatesDataset(split=\"test\", feature=\"gt_primitive\", data=data, args=args, is_limit=True)\n",
    "seen_mask = train_dataset.seen_mask\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "id": "843771be",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "emb_init:  1\n",
      "imgproj:  True\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "RetrievalModel(\n",
       "  (attr_encoder): Embedding(115, 600)\n",
       "  (obj_encoder): Embedding(245, 600)\n",
       "  (text_projection): Linear(in_features=1200, out_features=360, bias=False)\n",
       "  (image_projection): Linear(in_features=360, out_features=360, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 161,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load trained model\n",
    "emb_init = 1 if args.emb_init else 0\n",
    "imgproj = args.is_image_projection\n",
    "print(\"emb_init: \", emb_init)\n",
    "print(\"imgproj: \", imgproj)\n",
    "\n",
    "ckpt_dir = f\"../../outputs/mit_states/{args.model}_retrieval_model\"\n",
    "ckpt_name = \"{}_retrieval_model_init{}_imgproj{}_bias{}_{}_Lim{}_N{}_TW{}_B{}_LR{}_WD{}_L{}\".format(\n",
    "    args.model,\n",
    "    1 if args.emb_init else 0,\n",
    "    1 if args.is_image_projection else 0,\n",
    "    1 if args.is_bias else 0,\n",
    "    args.feature,\n",
    "    1 if args.is_limit else 0, \n",
    "    args.num_epochs,\n",
    "    args.train_warmup,\n",
    "    args.batch_size,\n",
    "    args.learning_rate,\n",
    "    args.weight_decay,\n",
    "    args.logit_scale,\n",
    ")\n",
    "ckpt_path = os.path.join(ckpt_dir, ckpt_name, \"retrieval_model.ckpt\")\n",
    "\n",
    "limit_pairs = list(train_dataset.limit_pair2idx.keys())\n",
    "open_world_pairs = list(valid_dataset.open_world_pair2idx.keys())\n",
    "model = RetrievalModel(data, train_dataset.image_dim, limit_pairs, open_world_pairs, args)\n",
    "model.load_state_dict(torch.load(ckpt_path))\n",
    "model.eval()\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "id": "1bdbaf56",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "seen_indices: 1844 | unseen_indices: 8576\n",
      "Intervention features: torch.Size([10420, 360])\n"
     ]
    }
   ],
   "source": [
    "# Prepare groundtruth labels\n",
    "seen_ids, unseen_ids = get_seen_unseen_indices(args.split, data)\n",
    "\n",
    "if args.split == \"valid\":\n",
    "    labels_attr = [sample[\"attr_id\"] for sample in data.valid_data]\n",
    "    labels_obj = [sample[\"obj_id\"] for sample in data.valid_data]\n",
    "    labels = [sample[\"pair_id\"] for sample in data.valid_data]\n",
    "    gt_features_attr = np.zeros((valid_dataset.image_embs.shape[0], 115))\n",
    "    gt_features_obj = np.zeros((valid_dataset.image_embs.shape[0], 245))\n",
    "elif args.split == \"test\":\n",
    "    labels_attr = [sample[\"attr_id\"] for sample in data.test_data]\n",
    "    labels_obj = [sample[\"obj_id\"] for sample in data.test_data]\n",
    "    labels = [sample[\"pair_id\"] for sample in data.test_data]\n",
    "    gt_features_attr = np.zeros((test_dataset.image_embs.shape[0], 115))\n",
    "    gt_features_obj = np.zeros((test_dataset.image_embs.shape[0], 245))\n",
    "    \n",
    "gt_features_attr[np.arange(len(labels_attr)), labels_attr] = 1\n",
    "gt_features_obj[np.arange(len(labels_obj)), labels_obj] = 1\n",
    "gt_features_concat = np.concatenate([gt_features_attr, gt_features_obj], axis=-1)\n",
    "gt_features_concat = torch.tensor(gt_features_concat).to(device).double()\n",
    "print(f\"Intervention features: {gt_features_concat.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "id": "6a049446",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "features:  torch.Size([10420, 1962])\n"
     ]
    }
   ],
   "source": [
    "# Intervene!!!\n",
    "with torch.no_grad():\n",
    "    logits, loss = model(gt_features_concat, pair_labels=None, is_train=False, is_intervene=True)\n",
    "features = logits.softmax(dim=-1).log()\n",
    "print(\"features: \", features.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c13af3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "overall_metrics = get_overall_metrics(features, labels, seen_ids, unseen_ids, seen_mask, data, topk_list=[1,2,3])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c42a5a1c",
   "metadata": {},
   "source": [
    "## 2.1. Primitive -> GT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 302,
   "id": "43f4c53a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class parseArguments:\n",
    "    def __init__(self):\n",
    "        self.split = \"valid\"\n",
    "        self.is_open_world = False\n",
    "        \n",
    "        self.feature = \"primitive\"\n",
    "        self.model = \"vilt\"\n",
    "        self.train_warmup = 0\n",
    "        self.is_image_projection = True\n",
    "        self.is_bias = False\n",
    "        self.is_limit = True\n",
    "        \n",
    "        self.data_root = \"../../data/mit_states\"\n",
    "        self.emb_root = \"../../data\"\n",
    "        self.precomputed_data_root = f\"../../outputs/mit_states/{self.model}_precompute_features\"\n",
    "        \n",
    "        self.emb_init = True\n",
    "        self.input_dim = 600\n",
    "        self.num_epochs = 400\n",
    "        self.batch_size = 128\n",
    "        self.learning_rate = 5e-5\n",
    "        self.weight_decay = 5e-5\n",
    "        self.logit_scale = 0.07\n",
    "\n",
    "args = parseArguments()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 303,
   "id": "f43d8427",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train pairs: 1262 | valid pairs: 600 | test pairs: 800\n",
      "train images: 30338 | valid images: 10420 | test images: 12995\n",
      "seen_indices: 1844 | unseen_indices: 8576\n",
      "seen_indices: 2380 | unseen_indices: 10615\n",
      "attr \t| train (30338, 115) \t| valid (10420, 115) \t| test (12995, 115)\n",
      "obj \t| train (30338, 245) \t| valid (10420, 245) \t| test (12995, 245)\n",
      "image_dim:    360\n",
      "attr \t| train (30338, 115) \t| valid (10420, 115) \t| test (12995, 115)\n",
      "obj \t| train (30338, 245) \t| valid (10420, 245) \t| test (12995, 245)\n",
      "image_dim:    360\n",
      "attr \t| train (30338, 115) \t| valid (10420, 115) \t| test (12995, 115)\n",
      "obj \t| train (30338, 245) \t| valid (10420, 245) \t| test (12995, 245)\n",
      "image_dim:    360\n"
     ]
    }
   ],
   "source": [
    "# Load dataset\n",
    "data = MITStatesDataset(root=args.data_root, split=\"train\")  # split can be ignored here\n",
    "seen_ids_valid, unseen_ids_valid = get_seen_unseen_indices(\"valid\", data)\n",
    "seen_ids_test, unseen_ids_test = get_seen_unseen_indices(\"test\", data)\n",
    "\n",
    "train_dataset = Precomputed_MITStatesDataset(split=\"train\", feature=\"primitive\", data=data, args=args, is_limit=True)\n",
    "valid_dataset = Precomputed_MITStatesDataset(split=\"valid\", feature=\"primitive\", data=data, args=args, is_limit=True)\n",
    "test_dataset = Precomputed_MITStatesDataset(split=\"test\", feature=\"primitive\", data=data, args=args, is_limit=True)\n",
    "seen_mask = train_dataset.seen_mask\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 304,
   "id": "881380bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "emb_init:  1\n",
      "imgproj:  True\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "RetrievalModel(\n",
       "  (attr_encoder): Embedding(115, 600)\n",
       "  (obj_encoder): Embedding(245, 600)\n",
       "  (text_projection): Linear(in_features=1200, out_features=360, bias=False)\n",
       "  (image_projection): Linear(in_features=360, out_features=360, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 304,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load trained model\n",
    "emb_init = 1 if args.emb_init else 0\n",
    "imgproj = args.is_image_projection\n",
    "print(\"emb_init: \", emb_init)\n",
    "print(\"imgproj: \", imgproj)\n",
    "\n",
    "ckpt_dir = f\"../../outputs/mit_states/{args.model}_retrieval_model\"\n",
    "ckpt_name = \"{}_retrieval_model_init{}_imgproj{}_bias{}_{}_Lim{}_N{}_TW{}_B{}_LR{}_WD{}_L{}\".format(\n",
    "    args.model,\n",
    "    1 if args.emb_init else 0,\n",
    "    1 if args.is_image_projection else 0,\n",
    "    1 if args.is_bias else 0,\n",
    "    args.feature,\n",
    "    1 if args.is_limit else 0, \n",
    "    args.num_epochs,\n",
    "    args.train_warmup,\n",
    "    args.batch_size,\n",
    "    args.learning_rate,\n",
    "    args.weight_decay,\n",
    "    args.logit_scale,\n",
    ")\n",
    "ckpt_path = os.path.join(ckpt_dir, ckpt_name, \"retrieval_model.ckpt\")\n",
    "\n",
    "limit_pairs = list(train_dataset.limit_pair2idx.keys())\n",
    "open_world_pairs = list(valid_dataset.open_world_pair2idx.keys())\n",
    "model = RetrievalModel(data, train_dataset.image_dim, limit_pairs, open_world_pairs, args)\n",
    "model.load_state_dict(torch.load(ckpt_path))\n",
    "model.eval()\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 305,
   "id": "8beeef83",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "seen_indices: 1844 | unseen_indices: 8576\n",
      "Intervention features: torch.Size([10420, 360])\n"
     ]
    }
   ],
   "source": [
    "# Prepare groundtruth labels\n",
    "seen_ids, unseen_ids = get_seen_unseen_indices(args.split, data)\n",
    "\n",
    "if args.split == \"valid\":\n",
    "    labels_attr = [sample[\"attr_id\"] for sample in data.valid_data]\n",
    "    labels_obj = [sample[\"obj_id\"] for sample in data.valid_data]\n",
    "    labels = [sample[\"pair_id\"] for sample in data.valid_data]\n",
    "    gt_features_attr = np.zeros((valid_dataset.image_embs.shape[0], 115))\n",
    "    gt_features_obj = np.zeros((valid_dataset.image_embs.shape[0], 245))\n",
    "elif args.split == \"test\":\n",
    "    labels_attr = [sample[\"attr_id\"] for sample in data.test_data]\n",
    "    labels_obj = [sample[\"obj_id\"] for sample in data.test_data]\n",
    "    labels = [sample[\"pair_id\"] for sample in data.test_data]\n",
    "    gt_features_attr = np.zeros((test_dataset.image_embs.shape[0], 115))\n",
    "    gt_features_obj = np.zeros((test_dataset.image_embs.shape[0], 245))\n",
    "    \n",
    "gt_features_attr[np.arange(len(labels_attr)), labels_attr] = 1\n",
    "gt_features_obj[np.arange(len(labels_obj)), labels_obj] = 1\n",
    "gt_features_concat = np.concatenate([gt_features_attr, gt_features_obj], axis=-1)\n",
    "gt_features_concat = torch.tensor(gt_features_concat).to(device).double()\n",
    "print(f\"Intervention features: {gt_features_concat.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 306,
   "id": "8b170308",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "features:  torch.Size([10420, 1962])\n"
     ]
    }
   ],
   "source": [
    "# Intervene!!!\n",
    "with torch.no_grad():\n",
    "    logits, loss = model(gt_features_concat, pair_labels=None, is_train=False, is_intervene=True)\n",
    "features = logits.softmax(dim=-1).log()\n",
    "print(\"features: \", features.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 307,
   "id": "6a19f40b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "topk: 1\n",
      "best_seen_acc: 0.2912\n",
      "best_unseen_acc: 0.3853\n",
      "best_harmonic_mean: 0.2283\n",
      "auc: 0.0856\n",
      "topk: 2\n",
      "best_seen_acc: 0.4084\n",
      "best_unseen_acc: 0.5146\n",
      "best_harmonic_mean: 0.3298\n",
      "auc: 0.1711\n",
      "topk: 3\n",
      "best_seen_acc: 0.4572\n",
      "best_unseen_acc: 0.6027\n",
      "best_harmonic_mean: 0.4060\n",
      "auc: 0.2363\n"
     ]
    }
   ],
   "source": [
    "overall_metrics = get_overall_metrics(features, labels, seen_ids, unseen_ids, seen_mask, data, topk_list=[1,2,3])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21317efa",
   "metadata": {},
   "source": [
    "## 2.2. GT -> Primitive (argmax)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 260,
   "id": "e950433c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class parseArguments:\n",
    "    def __init__(self):\n",
    "        self.split = \"valid\"\n",
    "        self.is_open_world = False\n",
    "        \n",
    "        self.feature = \"gt_primitive\"\n",
    "        self.model = \"albef\"  # determines where the precomputed activations come from\n",
    "        self.train_warmup = 0\n",
    "        self.is_image_projection = True\n",
    "        self.is_bias = False\n",
    "        self.is_limit = True\n",
    "        \n",
    "        self.data_root = \"../../data/mit_states\"\n",
    "        self.emb_root = \"../../data\"\n",
    "        self.precomputed_data_root = f\"../../outputs/mit_states/{self.model}_precompute_features\"\n",
    "        \n",
    "        self.emb_init = True\n",
    "        self.input_dim = 600\n",
    "        self.num_epochs = 100\n",
    "        self.batch_size = 128\n",
    "        self.learning_rate = 5e-5\n",
    "        self.weight_decay = 5e-5\n",
    "        self.logit_scale = 0.07\n",
    "\n",
    "args = parseArguments()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 261,
   "id": "99dd5f69",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train pairs: 1262 | valid pairs: 600 | test pairs: 800\n",
      "train images: 30338 | valid images: 10420 | test images: 12995\n",
      "seen_indices: 1844 | unseen_indices: 8576\n",
      "seen_indices: 2380 | unseen_indices: 10615\n",
      "attr \t| train (30338, 115) \t| valid (10420, 115) \t| test (12995, 115)\n",
      "obj \t| train (30338, 245) \t| valid (10420, 245) \t| test (12995, 245)\n",
      "image_dim:    360\n",
      "attr \t| train (30338, 115) \t| valid (10420, 115) \t| test (12995, 115)\n",
      "obj \t| train (30338, 245) \t| valid (10420, 245) \t| test (12995, 245)\n",
      "image_dim:    360\n",
      "attr \t| train (30338, 115) \t| valid (10420, 115) \t| test (12995, 115)\n",
      "obj \t| train (30338, 245) \t| valid (10420, 245) \t| test (12995, 245)\n",
      "image_dim:    360\n"
     ]
    }
   ],
   "source": [
    "# Load dataset\n",
    "data = MITStatesDataset(root=args.data_root, split=\"train\")  # split can be ignored here\n",
    "seen_ids_valid, unseen_ids_valid = get_seen_unseen_indices(\"valid\", data)\n",
    "seen_ids_test, unseen_ids_test = get_seen_unseen_indices(\"test\", data)\n",
    "\n",
    "train_dataset = Precomputed_MITStatesDataset(split=\"train\", feature=\"primitive\", data=data, args=args, is_limit=True)\n",
    "valid_dataset = Precomputed_MITStatesDataset(split=\"valid\", feature=\"primitive\", data=data, args=args, is_limit=True)\n",
    "test_dataset = Precomputed_MITStatesDataset(split=\"test\", feature=\"primitive\", data=data, args=args, is_limit=True)\n",
    "seen_mask = train_dataset.seen_mask\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 262,
   "id": "86749462",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "emb_init:  1\n",
      "imgproj:  True\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "RetrievalModel(\n",
       "  (attr_encoder): Embedding(115, 600)\n",
       "  (obj_encoder): Embedding(245, 600)\n",
       "  (text_projection): Linear(in_features=1200, out_features=360, bias=False)\n",
       "  (image_projection): Linear(in_features=360, out_features=360, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 262,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load trained model\n",
    "emb_init = 1 if args.emb_init else 0\n",
    "imgproj = args.is_image_projection\n",
    "print(\"emb_init: \", emb_init)\n",
    "print(\"imgproj: \", imgproj)\n",
    "\n",
    "# Fixed to gt_retrieval_model\n",
    "ckpt_dir = f\"../../outputs/mit_states/gt_retrieval_model\"\n",
    "ckpt_name = \"gt_retrieval_model_init{}_imgproj{}_bias{}_{}_Lim{}_N{}_TW{}_B{}_LR{}_WD{}_L{}\".format(\n",
    "    1 if args.emb_init else 0,\n",
    "    1 if args.is_image_projection else 0,\n",
    "    1 if args.is_bias else 0,\n",
    "    args.feature,\n",
    "    1 if args.is_limit else 0, \n",
    "    args.num_epochs,\n",
    "    args.train_warmup,\n",
    "    args.batch_size,\n",
    "    args.learning_rate,\n",
    "    args.weight_decay,\n",
    "    args.logit_scale,\n",
    ")\n",
    "ckpt_path = os.path.join(ckpt_dir, ckpt_name, \"retrieval_model.ckpt\")\n",
    "\n",
    "limit_pairs = list(train_dataset.limit_pair2idx.keys())\n",
    "open_world_pairs = list(valid_dataset.open_world_pair2idx.keys())\n",
    "model = RetrievalModel(data, train_dataset.image_dim, limit_pairs, open_world_pairs, args)\n",
    "model.load_state_dict(torch.load(ckpt_path))\n",
    "model.eval()\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 263,
   "id": "aeda882d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "seen_indices: 1844 | unseen_indices: 8576\n",
      "Intervention features: torch.Size([10420, 360])\n"
     ]
    }
   ],
   "source": [
    "# Prepare primitive concepts\n",
    "seen_ids, unseen_ids = get_seen_unseen_indices(args.split, data)\n",
    "\n",
    "if args.split == \"valid\":\n",
    "    labels = [sample[\"pair_id\"] for sample in data.valid_data]\n",
    "    features_interv = valid_dataset.image_embs\n",
    "elif args.split == \"test\":\n",
    "    labels = [sample[\"pair_id\"] for sample in data.test_data]\n",
    "    features_interv = test_dataset.image_embs\n",
    "\n",
    "features_interv = torch.tensor(features_interv).to(device).double()\n",
    "features_interv = F.normalize(features_interv, dim=1).float()\n",
    "\n",
    "# Compute argmax\n",
    "pred_attr_ids = features_interv[:,:len(data.attrs)].argmax(dim=1)\n",
    "pred_obj_ids = features_interv[:,len(data.attrs):].argmax(dim=1) + len(data.attrs)\n",
    "\n",
    "features_interv = torch.zeros(features_interv.shape).to(device)\n",
    "features_interv[range(features_interv.shape[0]), pred_attr_ids] = 1\n",
    "features_interv[range(features_interv.shape[0]), pred_obj_ids] = 1\n",
    "print(f\"Intervention features: {features_interv.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 264,
   "id": "5753e0b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "features:  torch.Size([10420, 1962])\n"
     ]
    }
   ],
   "source": [
    "# Intervene!!!\n",
    "with torch.no_grad():\n",
    "    logits, loss = model(features_interv, pair_labels=None, is_train=False, is_intervene=True)\n",
    "features = logits.softmax(dim=-1).log()\n",
    "print(\"features: \", features.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 265,
   "id": "9c6073ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "topk: 1\n",
      "best_seen_acc: 0.0461\n",
      "best_unseen_acc: 0.0394\n",
      "best_harmonic_mean: 0.0282\n",
      "auc: 0.0012\n",
      "topk: 2\n",
      "best_seen_acc: 0.0895\n",
      "best_unseen_acc: 0.0849\n",
      "best_harmonic_mean: 0.0508\n",
      "auc: 0.0044\n",
      "topk: 3\n",
      "best_seen_acc: 0.1171\n",
      "best_unseen_acc: 0.1193\n",
      "best_harmonic_mean: 0.0674\n",
      "auc: 0.0083\n"
     ]
    }
   ],
   "source": [
    "overall_metrics = get_overall_metrics(features, labels, seen_ids, unseen_ids, seen_mask, data, topk_list=[1,2,3])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c5d3b85",
   "metadata": {},
   "source": [
    "## 2.3. Primitive -> ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 322,
   "id": "1be3984f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class parseArguments:\n",
    "    def __init__(self):\n",
    "        self.split = \"valid\"\n",
    "        self.is_open_world = False\n",
    "        \n",
    "        self.feature = \"primitive\"\n",
    "        self.model = \"albef\"\n",
    "        self.train_warmup = 0\n",
    "        self.is_image_projection = True\n",
    "        self.is_bias = False\n",
    "        self.is_limit = True\n",
    "        \n",
    "        self.data_root = \"../../data/mit_states\"\n",
    "        self.emb_root = \"../../data\"\n",
    "        self.precomputed_data_root = f\"../../outputs/mit_states/{self.model}_precompute_features\"\n",
    "        \n",
    "        self.emb_init = True\n",
    "        self.input_dim = 600\n",
    "        self.num_epochs = 400\n",
    "        self.batch_size = 128\n",
    "        self.learning_rate = 5e-5\n",
    "        self.weight_decay = 5e-5\n",
    "        self.logit_scale = 0.07\n",
    "\n",
    "args = parseArguments()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 323,
   "id": "a56e4404",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train pairs: 1262 | valid pairs: 600 | test pairs: 800\n",
      "train images: 30338 | valid images: 10420 | test images: 12995\n",
      "seen_indices: 1844 | unseen_indices: 8576\n",
      "seen_indices: 2380 | unseen_indices: 10615\n",
      "attr \t| train (30338, 115) \t| valid (10420, 115) \t| test (12995, 115)\n",
      "obj \t| train (30338, 245) \t| valid (10420, 245) \t| test (12995, 245)\n",
      "image_dim:    360\n",
      "attr \t| train (30338, 115) \t| valid (10420, 115) \t| test (12995, 115)\n",
      "obj \t| train (30338, 245) \t| valid (10420, 245) \t| test (12995, 245)\n",
      "image_dim:    360\n",
      "attr \t| train (30338, 115) \t| valid (10420, 115) \t| test (12995, 115)\n",
      "obj \t| train (30338, 245) \t| valid (10420, 245) \t| test (12995, 245)\n",
      "image_dim:    360\n"
     ]
    }
   ],
   "source": [
    "# Load dataset\n",
    "data = MITStatesDataset(root=args.data_root, split=\"train\")  # split can be ignored here\n",
    "seen_ids_valid, unseen_ids_valid = get_seen_unseen_indices(\"valid\", data)\n",
    "seen_ids_test, unseen_ids_test = get_seen_unseen_indices(\"test\", data)\n",
    "\n",
    "train_dataset = Precomputed_MITStatesDataset(split=\"train\", feature=\"primitive\", data=data, args=args, is_limit=True)\n",
    "valid_dataset = Precomputed_MITStatesDataset(split=\"valid\", feature=\"primitive\", data=data, args=args, is_limit=True)\n",
    "test_dataset = Precomputed_MITStatesDataset(split=\"test\", feature=\"primitive\", data=data, args=args, is_limit=True)\n",
    "seen_mask = train_dataset.seen_mask\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 324,
   "id": "1e4cc68c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "emb_init:  1\n",
      "imgproj:  True\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "RetrievalModel(\n",
       "  (attr_encoder): Embedding(115, 600)\n",
       "  (obj_encoder): Embedding(245, 600)\n",
       "  (text_projection): Linear(in_features=1200, out_features=360, bias=False)\n",
       "  (image_projection): Linear(in_features=360, out_features=360, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 324,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load trained model\n",
    "emb_init = 1 if args.emb_init else 0\n",
    "imgproj = args.is_image_projection\n",
    "print(\"emb_init: \", emb_init)\n",
    "print(\"imgproj: \", imgproj)\n",
    "\n",
    "# Open to arbitrary models\n",
    "ckpt_dir = f\"../../outputs/mit_states/{args.model}_retrieval_model\"\n",
    "ckpt_name = \"{}_retrieval_model_init{}_imgproj{}_bias{}_{}_Lim{}_N{}_TW{}_B{}_LR{}_WD{}_L{}\".format(\n",
    "    args.model,\n",
    "    1 if args.emb_init else 0,\n",
    "    1 if args.is_image_projection else 0,\n",
    "    1 if args.is_bias else 0,\n",
    "    args.feature,\n",
    "    1 if args.is_limit else 0, \n",
    "    args.num_epochs,\n",
    "    args.train_warmup,\n",
    "    args.batch_size,\n",
    "    args.learning_rate,\n",
    "    args.weight_decay,\n",
    "    args.logit_scale,\n",
    ")\n",
    "ckpt_path = os.path.join(ckpt_dir, ckpt_name, \"retrieval_model.ckpt\")\n",
    "\n",
    "limit_pairs = list(train_dataset.limit_pair2idx.keys())\n",
    "open_world_pairs = list(valid_dataset.open_world_pair2idx.keys())\n",
    "model = RetrievalModel(data, train_dataset.image_dim, limit_pairs, open_world_pairs, args)\n",
    "model.load_state_dict(torch.load(ckpt_path))\n",
    "model.eval()\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 325,
   "id": "b2bc7114",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "seen_indices: 1844 | unseen_indices: 8576\n",
      "GT features: torch.Size([10420, 360])\n"
     ]
    }
   ],
   "source": [
    "# Prepare groundtruth labels\n",
    "seen_ids, unseen_ids = get_seen_unseen_indices(args.split, data)\n",
    "\n",
    "if args.split == \"valid\":\n",
    "    labels_attr = [sample[\"attr_id\"] for sample in data.valid_data]\n",
    "    labels_obj = [sample[\"obj_id\"] for sample in data.valid_data]\n",
    "    labels = [sample[\"pair_id\"] for sample in data.valid_data]\n",
    "    gt_features_attr = np.zeros((valid_dataset.image_embs.shape[0], 115))\n",
    "    gt_features_obj = np.zeros((valid_dataset.image_embs.shape[0], 245))\n",
    "elif args.split == \"test\":\n",
    "    labels_attr = [sample[\"attr_id\"] for sample in data.test_data]\n",
    "    labels_obj = [sample[\"obj_id\"] for sample in data.test_data]\n",
    "    labels = [sample[\"pair_id\"] for sample in data.test_data]\n",
    "    gt_features_attr = np.zeros((test_dataset.image_embs.shape[0], 115))\n",
    "    gt_features_obj = np.zeros((test_dataset.image_embs.shape[0], 245))\n",
    "    \n",
    "gt_features_attr[np.arange(len(labels_attr)), labels_attr] = 1\n",
    "gt_features_obj[np.arange(len(labels_obj)), labels_obj] = 1\n",
    "gt_features_concat = np.concatenate([gt_features_attr, gt_features_obj], axis=-1)\n",
    "gt_features_concat = torch.tensor(gt_features_concat).to(device).double()\n",
    "print(f\"GT features: {gt_features_concat.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 326,
   "id": "ff56f14b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "seen_indices: 1844 | unseen_indices: 8576\n",
      "Intervention features: torch.Size([10420, 360])\n"
     ]
    }
   ],
   "source": [
    "# Prepare \"binary\" primitive concept activations\n",
    "# Set GT concepts to 1\n",
    "seen_ids, unseen_ids = get_seen_unseen_indices(args.split, data)\n",
    "\n",
    "if args.split == \"valid\":\n",
    "    labels_attr = [sample[\"attr_id\"] for sample in data.valid_data]\n",
    "    labels_obj = [sample[\"obj_id\"] for sample in data.valid_data]\n",
    "    labels = [sample[\"pair_id\"] for sample in data.valid_data]\n",
    "    features_interv = valid_dataset.image_embs\n",
    "    features_interv = torch.tensor(features_interv).to(device).double()\n",
    "    features_interv = F.normalize(features_interv, dim=1)\n",
    "    \n",
    "elif args.split == \"test\":\n",
    "    labels_attr = [sample[\"attr_id\"] for sample in data.test_data]\n",
    "    labels_obj = [sample[\"obj_id\"] for sample in data.test_data]\n",
    "    labels = [sample[\"pair_id\"] for sample in data.test_data]\n",
    "    features_interv = test_dataset.image_embs\n",
    "    features_interv = torch.tensor(features_interv).to(device).double()\n",
    "    features_interv = F.normalize(features_interv, dim=1)\n",
    "\n",
    "features_interv = torch.where(gt_features_concat==1, 1.0, features_interv)\n",
    "print(f\"Intervention features: {features_interv.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 327,
   "id": "a0f6182d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "features:  torch.Size([10420, 1962])\n"
     ]
    }
   ],
   "source": [
    "# Intervene!!!\n",
    "with torch.no_grad():\n",
    "    logits, loss = model(features_interv, pair_labels=None, is_train=False, is_intervene=True)\n",
    "features = logits.softmax(dim=-1).log()\n",
    "print(\"features: \", features.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 328,
   "id": "28797a25",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "topk: 1\n",
      "best_seen_acc: 0.6014\n",
      "best_unseen_acc: 0.6931\n",
      "best_harmonic_mean: 0.5389\n",
      "auc: 0.3867\n",
      "topk: 2\n",
      "best_seen_acc: 0.7402\n",
      "best_unseen_acc: 0.8089\n",
      "best_harmonic_mean: 0.6768\n",
      "auc: 0.5650\n",
      "topk: 3\n",
      "best_seen_acc: 0.8259\n",
      "best_unseen_acc: 0.8791\n",
      "best_harmonic_mean: 0.7447\n",
      "auc: 0.6831\n"
     ]
    }
   ],
   "source": [
    "overall_metrics = get_overall_metrics(features, labels, seen_ids, unseen_ids, seen_mask, data, topk_list=[1,2,3])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1966bd30",
   "metadata": {},
   "source": [
    "# 3.  Open World MIT States"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "656609a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_gt_primitives(split, data):\n",
    "    \"\"\" Get groundtruth primtiive concepts. \"\"\"\n",
    "    data_dict = {\n",
    "        \"train\": data.train_data,\n",
    "        \"valid\": data.valid_data,\n",
    "        \"test\": data.test_data,\n",
    "    }\n",
    "    split_data = data_dict[split]\n",
    "    labels_attr = [sample[\"attr_id\"] for sample in split_data]\n",
    "    labels_obj = [sample[\"obj_id\"] for sample in split_data]\n",
    "    gt_features_attr = np.zeros((len(split_data), len(data.attrs)))\n",
    "    gt_features_obj = np.zeros((len(split_data), len(data.objs)))\n",
    "    gt_features_attr[np.arange(len(labels_attr)), labels_attr] = 1\n",
    "    gt_features_obj[np.arange(len(labels_obj)), labels_obj] = 1\n",
    "    gt_features_concat = np.concatenate([gt_features_attr, gt_features_obj], axis=-1)\n",
    "    return gt_features_concat\n",
    "\n",
    "def evaluate(results):\n",
    "    \"\"\" Evaluate predictions and Return metrics. \"\"\"\n",
    "    all_preds, seen_preds, unseen_preds = results[\"all_preds\"], results[\"seen_preds\"], results[\"unseen_preds\"]\n",
    "    all_acc, seen_acc, unseen_acc = np.mean(all_preds), np.mean(seen_preds), np.mean(unseen_preds)    \n",
    "    return {\n",
    "        \"all_acc\": all_acc,\n",
    "        \"seen_acc\": seen_acc,\n",
    "        \"unseen_acc\": unseen_acc,\n",
    "        \"harmonic_mean\": (seen_acc * unseen_acc)**0.5,\n",
    "        \"macro_average_acc\": (seen_acc + unseen_acc)*0.5,\n",
    "    }\n",
    "\n",
    "def generate_predictions(scores, labels, seen_ids, unseen_ids, seen_mask, data, topk, is_open_world, bias=0.0):\n",
    "    \"\"\" Apply bias and Generate predictions for. \"\"\"\n",
    "    def get_predictions(_scores):\n",
    "        # Get predictions\n",
    "        _, pair_preds = _scores.topk(topk, dim=1)\n",
    "        pair_preds = pair_preds[:, :topk].contiguous().view(-1)\n",
    "        attr_preds = all_pairs[pair_preds][:,0].view(-1, topk)\n",
    "        obj_preds = all_pairs[pair_preds][:,1].view(-1, topk)\n",
    "        pair_preds = pair_preds.view(-1, topk)\n",
    "        return pair_preds, attr_preds, obj_preds\n",
    "    \n",
    "    # Get predictions with biases applied\n",
    "    if is_open_world:\n",
    "        all_pairs = torch.LongTensor([\n",
    "            (data.attr2idx[attr], data.obj2idx[obj])\n",
    "            for attr in data.attrs\n",
    "            for obj in data.objs\n",
    "        ])\n",
    "    else:\n",
    "        all_pairs = torch.LongTensor([\n",
    "            (data.attr2idx[attr], data.obj2idx[obj]) \n",
    "            for attr, obj in data.pairs\n",
    "        ])\n",
    "    scores = scores.clone()\n",
    "    mask = seen_mask.repeat(scores.shape[0], 1)\n",
    "    scores[~mask] += bias\n",
    "    pair_preds, attr_preds, obj_preds = get_predictions(scores)\n",
    "    \n",
    "    # Get predictions for seen/unseen pairs\n",
    "    all_preds = np.array([label in pair_preds[row_id,:topk] for row_id, label in enumerate(labels)])\n",
    "    seen_preds = all_preds[seen_ids]\n",
    "    unseen_preds = all_preds[unseen_ids]\n",
    "    return {\n",
    "        \"pair_preds\": pair_preds,\n",
    "        \"attr_preds\": attr_preds,\n",
    "        \"obj_preds\": obj_preds,\n",
    "        \"all_preds\": all_preds,\n",
    "        \"seen_preds\": seen_preds,\n",
    "        \"unseen_preds\": unseen_preds,\n",
    "    }\n",
    "\n",
    "def get_overall_metrics(features, labels, seen_ids, unseen_ids, seen_mask, data, topk_list=[1], is_open_world=False):\n",
    "    overall_metrics = {}\n",
    "    for topk in topk_list:\n",
    "        # Get model\"s performance (accuracy) on seen/unseen pairs\n",
    "        bias = 1e3\n",
    "        results = generate_predictions(\n",
    "            features, labels, seen_ids, unseen_ids, seen_mask, data, topk, is_open_world, bias=bias,\n",
    "        )\n",
    "        full_unseen_metrics = evaluate(results)\n",
    "        all_preds, seen_preds, unseen_preds = results[\"all_preds\"], results[\"seen_preds\"], results[\"unseen_preds\"]\n",
    "\n",
    "        # Get predicted probability distribution of unseen pairs,\n",
    "        # and the top K scores of seen pairs in the predicted prob. distribution of unseen pairs\n",
    "        correct_scores = features[np.arange(len(features)), labels][unseen_ids]\n",
    "        max_seen_scores = features[unseen_ids][:, seen_mask].topk(topk, dim=1)[0][:,topk-1]\n",
    "\n",
    "        # Compute biases\n",
    "        unseen_score_diff = max_seen_scores - correct_scores\n",
    "        correct_unseen_score_diff = unseen_score_diff[unseen_preds] - 1e-4\n",
    "        correct_unseen_score_diff = torch.sort(correct_unseen_score_diff)[0]\n",
    "        magic_binsize = 20\n",
    "        bias_skip = max(len(correct_unseen_score_diff) // magic_binsize, 1)\n",
    "        bias_list = correct_unseen_score_diff[::bias_skip]\n",
    "\n",
    "        # Get biased predictions and metrics with different biases\n",
    "        all_metrics = []\n",
    "        for bias in bias_list:\n",
    "            results = generate_predictions(\n",
    "                features, labels, seen_ids, unseen_ids, seen_mask, data, topk, is_open_world, bias=bias,\n",
    "            )\n",
    "            metrics = evaluate(results)\n",
    "            all_metrics.append(metrics)\n",
    "        all_metrics.append(full_unseen_metrics)\n",
    "\n",
    "        # Compute overall metrics\n",
    "        seen_accs = np.array([metric_dict[\"seen_acc\"] for metric_dict in all_metrics])\n",
    "        unseen_accs = np.array([metric_dict[\"unseen_acc\"] for metric_dict in all_metrics])\n",
    "        best_seen_acc = max([metric_dict[\"seen_acc\"] for metric_dict in all_metrics])\n",
    "        best_unseen_acc = max([metric_dict[\"unseen_acc\"] for metric_dict in all_metrics])\n",
    "        best_harmonic_mean = max([metric_dict[\"harmonic_mean\"] for metric_dict in all_metrics])\n",
    "        auc = np.trapz(seen_accs, unseen_accs)\n",
    "        print(f\"topk: {topk}\")\n",
    "        print(f\"best_seen_acc: {best_seen_acc:6.4f}\")\n",
    "        print(f\"best_unseen_acc: {best_unseen_acc:6.4f}\")\n",
    "        print(f\"best_harmonic_mean: {best_harmonic_mean:6.4f}\")\n",
    "        print(f\"auc: {auc:6.4f}\")\n",
    "\n",
    "        overall_metrics[topk] = {\n",
    "            \"seen_accs\": seen_accs.tolist(),\n",
    "            \"unseen_accs\": unseen_accs.tolist(),\n",
    "            \"best_seen_acc\": best_seen_acc,\n",
    "            \"best_unseen_acc\": best_unseen_acc,\n",
    "            \"best_harmonic_mean\": best_harmonic_mean,\n",
    "            \"auc\": auc,\n",
    "        }\n",
    "    return overall_metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "89d19f8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class parseArguments:\n",
    "    def __init__(self):\n",
    "        self.split = \"valid\"\n",
    "        self.is_open_world = True\n",
    "        \n",
    "        self.feature = \"primitive\"\n",
    "        self.model = \"vilt\"\n",
    "        self.train_warmup = 0\n",
    "        self.is_image_projection = True\n",
    "        self.is_bias = False\n",
    "        self.is_limit = True\n",
    "        \n",
    "        self.data_root = \"../../data/mit_states\"\n",
    "        self.emb_root = \"../../data\"\n",
    "        self.precomputed_data_root = f\"../../outputs/mit_states/{self.model}_precompute_features\"\n",
    "        \n",
    "        self.emb_init = True\n",
    "        self.input_dim = 600\n",
    "        self.num_epochs = 400\n",
    "        self.batch_size = 128\n",
    "        self.learning_rate = 5e-5\n",
    "        self.weight_decay = 5e-5\n",
    "        self.logit_scale = 0.07\n",
    "\n",
    "args = parseArguments()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "7ce8866b",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train pairs: 1262 | valid pairs: 600 | test pairs: 800\n",
      "train images: 30338 | valid images: 10420 | test images: 12995\n",
      "seen_indices: 1844 | unseen_indices: 8576\n",
      "seen_indices: 2380 | unseen_indices: 10615\n",
      "attr \t| train (30338, 115) \t| valid (10420, 115) \t| test (12995, 115)\n",
      "obj \t| train (30338, 245) \t| valid (10420, 245) \t| test (12995, 245)\n",
      "image_dim:    360\n",
      "attr \t| train (30338, 115) \t| valid (10420, 115) \t| test (12995, 115)\n",
      "obj \t| train (30338, 245) \t| valid (10420, 245) \t| test (12995, 245)\n",
      "image_dim:    360\n",
      "attr \t| train (30338, 115) \t| valid (10420, 115) \t| test (12995, 115)\n",
      "obj \t| train (30338, 245) \t| valid (10420, 245) \t| test (12995, 245)\n",
      "image_dim:    360\n"
     ]
    }
   ],
   "source": [
    "# Load dataset\n",
    "data = MITStatesDataset(root=args.data_root, split=\"train\")  # split can be ignored here\n",
    "seen_ids_valid, unseen_ids_valid = get_seen_unseen_indices(\"valid\", data)\n",
    "seen_ids_test, unseen_ids_test = get_seen_unseen_indices(\"test\", data)\n",
    "\n",
    "train_dataset = Precomputed_MITStatesDataset(split=\"train\", feature=args.feature, data=data, args=args, is_limit=True)\n",
    "valid_dataset = Precomputed_MITStatesDataset(split=\"valid\", feature=args.feature, data=data, args=args, is_limit=True)\n",
    "test_dataset = Precomputed_MITStatesDataset(split=\"test\", feature=args.feature, data=data, args=args, is_limit=True)\n",
    "seen_mask = train_dataset.seen_mask\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "1bdeb352",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "emb_init:  1\n",
      "imgproj:  True\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "RetrievalModel(\n",
       "  (attr_encoder): Embedding(115, 600)\n",
       "  (obj_encoder): Embedding(245, 600)\n",
       "  (text_projection): Linear(in_features=1200, out_features=360, bias=False)\n",
       "  (image_projection): Linear(in_features=360, out_features=360, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load trained model\n",
    "emb_init = 1 if args.emb_init else 0\n",
    "imgproj = args.is_image_projection\n",
    "print(\"emb_init: \", emb_init)\n",
    "print(\"imgproj: \", imgproj)\n",
    "\n",
    "# Open to arbitrary models\n",
    "ckpt_dir = f\"../../outputs/mit_states/{args.model}_retrieval_model\"\n",
    "ckpt_name = \"{}_retrieval_model_init{}_imgproj{}_bias{}_{}_Lim{}_N{}_TW{}_B{}_LR{}_WD{}_L{}\".format(\n",
    "    args.model,\n",
    "    1 if args.emb_init else 0,\n",
    "    1 if args.is_image_projection else 0,\n",
    "    1 if args.is_bias else 0,\n",
    "    args.feature,\n",
    "    1 if args.is_limit else 0, \n",
    "    args.num_epochs,\n",
    "    args.train_warmup,\n",
    "    args.batch_size,\n",
    "    args.learning_rate,\n",
    "    args.weight_decay,\n",
    "    args.logit_scale,\n",
    ")\n",
    "ckpt_path = os.path.join(ckpt_dir, ckpt_name, \"retrieval_model.ckpt\")\n",
    "\n",
    "limit_pairs = list(train_dataset.limit_pair2idx.keys())\n",
    "open_world_pairs = list(valid_dataset.open_world_pair2idx.keys())\n",
    "model = RetrievalModel(data, train_dataset.image_dim, limit_pairs, open_world_pairs, args)\n",
    "model.load_state_dict(torch.load(ckpt_path))\n",
    "model.eval()\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "deeb0a91",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "seen_indices: 1844 | unseen_indices: 8576\n",
      "Intervention features: torch.Size([10420, 360])\n"
     ]
    }
   ],
   "source": [
    "# Prepare primitive concept activations\n",
    "seen_ids, unseen_ids = get_seen_unseen_indices(args.split, data)\n",
    "\n",
    "if args.split == \"valid\":\n",
    "    labels = valid_dataset.labels\n",
    "    features_interv = valid_dataset.image_embs\n",
    "    \n",
    "elif args.split == \"test\":\n",
    "    labels = test_dataset.labels\n",
    "    features_interv = test_dataset.image_embs\n",
    "    \n",
    "features_interv = torch.tensor(features_interv).to(device).double()\n",
    "print(f\"Intervention features: {features_interv.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "89475e6f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "features:  torch.Size([10420, 28175])\n"
     ]
    }
   ],
   "source": [
    "# Intervene!!!\n",
    "with torch.no_grad():\n",
    "    logits, loss = model(features_interv, pair_labels=None, is_train=False, is_intervene=False, is_open_world=True)\n",
    "features = logits.softmax(dim=-1).log()\n",
    "print(\"features: \", features.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "8cb2a718",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "7b82dcdf",
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "CUDA out of memory. Tried to allocate 4.18 GiB (GPU 0; 10.92 GiB total capacity; 4.68 GiB already allocated; 3.17 GiB free; 6.97 GiB reserved in total by PyTorch)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-39-cd8a5c8f2c47>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m overall_metrics = get_overall_metrics(\n\u001b[0;32m----> 2\u001b[0;31m     \u001b[0mfeatures\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mseen_ids\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0munseen_ids\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mseen_mask\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtopk_list\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mis_open_world\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m )\n",
      "\u001b[0;32m<ipython-input-36-0459bf1b33dc>\u001b[0m in \u001b[0;36mget_overall_metrics\u001b[0;34m(features, labels, seen_ids, unseen_ids, seen_mask, data, topk_list, is_open_world)\u001b[0m\n\u001b[1;32m     75\u001b[0m         \u001b[0mbias\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1e3\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     76\u001b[0m         results = generate_predictions(\n\u001b[0;32m---> 77\u001b[0;31m             \u001b[0mfeatures\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mseen_ids\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0munseen_ids\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mseen_mask\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtopk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mis_open_world\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbias\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     78\u001b[0m         )\n\u001b[1;32m     79\u001b[0m         \u001b[0mfull_unseen_metrics\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mevaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresults\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-36-0459bf1b33dc>\u001b[0m in \u001b[0;36mgenerate_predictions\u001b[0;34m(scores, labels, seen_ids, unseen_ids, seen_mask, data, topk, is_open_world, bias)\u001b[0m\n\u001b[1;32m     53\u001b[0m     \u001b[0mscores\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mscores\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclone\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     54\u001b[0m     \u001b[0mmask\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mseen_mask\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrepeat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mscores\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 55\u001b[0;31m     \u001b[0mscores\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m~\u001b[0m\u001b[0mmask\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mbias\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     56\u001b[0m     \u001b[0mpair_preds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattr_preds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mobj_preds\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_predictions\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mscores\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     57\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: CUDA out of memory. Tried to allocate 4.18 GiB (GPU 0; 10.92 GiB total capacity; 4.68 GiB already allocated; 3.17 GiB free; 6.97 GiB reserved in total by PyTorch)"
     ]
    }
   ],
   "source": [
    "overall_metrics = get_overall_metrics(\n",
    "    features, labels, seen_ids, unseen_ids, seen_mask, data, topk_list=[1,2,3], is_open_world=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42f74135",
   "metadata": {},
   "source": [
    "# 4. Aggregate Results for Multiple Prompts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "a4c23f0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load trained model\n",
    "def load_model(template_src, template_id):\n",
    "    ckpt_dir = f\"../../outputs/mit_states/clip_prompt_retrieval_model\"\n",
    "    ckpt_name = \"clip_prompt_retrieval_model_TP{}_TID{}_open{}_init{}_imgproj{}_bias{}_{}_Lim{}_N{}_TW{}_B{}_LR{}_WD{}_L{}\".format(\n",
    "        template_src,\n",
    "        template_id,\n",
    "        1 if args.is_open_world else 0,\n",
    "        1 if args.emb_init else 0,\n",
    "        1 if args.is_image_projection else 0,\n",
    "        1 if args.is_bias else 0,\n",
    "        args.feature,\n",
    "        1 if args.is_limit else 0, \n",
    "        args.num_epochs,\n",
    "        args.train_warmup,\n",
    "        args.batch_size,\n",
    "        args.learning_rate,\n",
    "        args.weight_decay,\n",
    "        args.logit_scale,\n",
    "    )\n",
    "    ckpt_path = os.path.join(ckpt_dir, ckpt_name, \"retrieval_model.ckpt\")\n",
    "\n",
    "    limit_pairs = list(train_dataset.limit_pair2idx.keys())\n",
    "    open_world_pairs = list(valid_dataset.open_world_pair2idx.keys())\n",
    "    model = RetrievalModel(data, train_dataset.image_dim, limit_pairs, open_world_pairs, args)\n",
    "    model.load_state_dict(torch.load(ckpt_path))\n",
    "    model.eval()\n",
    "    model.to(device)\n",
    "    return model\n",
    "\n",
    "def get_output_path(template_src, template_id, args):\n",
    "    ckpt_dir = f\"../../outputs/mit_states/clip_prompt_retrieval_model\"\n",
    "    ckpt_name = \"clip_prompt_retrieval_model_TP{}_TID{}_open{}_init{}_imgproj{}_bias{}_{}_Lim{}_N{}_TW{}_B{}_LR{}_WD{}_L{}\".format(\n",
    "        template_src,\n",
    "        template_id,\n",
    "        1 if args.is_open_world else 0,\n",
    "        1 if args.emb_init else 0,\n",
    "        1 if args.is_image_projection else 0,\n",
    "        1 if args.is_bias else 0,\n",
    "        args.feature,\n",
    "        1 if args.is_limit else 0, \n",
    "        args.num_epochs,\n",
    "        args.train_warmup,\n",
    "        args.batch_size,\n",
    "        args.learning_rate,\n",
    "        args.weight_decay,\n",
    "        args.logit_scale,\n",
    "    )\n",
    "    return os.path.join(ckpt_dir, ckpt_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "77cfe607",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "retrieval_model.ckpt  test_metrics.json  valid_metrics.json\r\n"
     ]
    }
   ],
   "source": [
    "!ls ../../outputs/mit_states/clip_prompt_retrieval_model/clip_prompt_retrieval_model_TPclip_TID0_open0_init1_imgproj1_bias0_pair_Lim1_N400_TW0_B128_LR5e-05_WD5e-05_L0.07\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "921453ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "class parseArguments:\n",
    "    def __init__(self):\n",
    "        self.data_root = \"../../outputs/mit_states/clip_prompt_retrieval_model\"\n",
    "        self.feature = \"pair\"\n",
    "        #self.template_src = \"clip\"\n",
    "        #self.template_id = 0\n",
    "\n",
    "args = parseArguments()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a655fd87",
   "metadata": {},
   "source": [
    "## 4.1. Usefulness"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "f2b407a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "template_src: clip\n",
      "split: valid\n",
      "| topk 1 | auc | mean: 8.340957 | std: 0.342888\n",
      "| topk 2 | auc | mean: 17.300506 | std: 0.453321\n",
      "| topk 3 | auc | mean: 24.907180 | std: 0.547247\n",
      "\n",
      "split: test\n",
      "| topk 1 | auc | mean: 6.974296 | std: 0.238532\n",
      "| topk 1 | best_seen_acc | mean: 34.255702 | std: 0.625115\n",
      "| topk 1 | best_unseen_acc | mean: 27.981966 | std: 0.822270\n",
      "| topk 1 | best_harmonic_mean | mean: 20.556065 | std: 0.439918\n",
      "| topk 2 | auc | mean: 15.927997 | std: 0.390615\n",
      "| topk 3 | auc | mean: 23.217200 | std: 0.519239\n",
      "\n",
      "template_src: compdl\n",
      "split: valid\n",
      "| topk 1 | auc | mean: 7.913990 | std: 0.143389\n",
      "| topk 2 | auc | mean: 16.757348 | std: 0.466115\n",
      "| topk 3 | auc | mean: 24.071215 | std: 0.549441\n",
      "\n",
      "split: test\n",
      "| topk 1 | auc | mean: 6.633088 | std: 0.353728\n",
      "| topk 1 | best_seen_acc | mean: 33.369748 | std: 1.019165\n",
      "| topk 1 | best_unseen_acc | mean: 27.368818 | std: 0.541875\n",
      "| topk 1 | best_harmonic_mean | mean: 20.056457 | std: 0.591366\n",
      "| topk 2 | auc | mean: 15.167544 | std: 0.483960\n",
      "| topk 3 | auc | mean: 22.453010 | std: 0.551025\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Usefulness\n",
    "template_count_dict = {\"clip\": 7, \"compdl\": 10}\n",
    "\n",
    "for template_src, template_count in template_count_dict.items():\n",
    "    # For each possible template source, intialize a new statistics dict\n",
    "    all_statistics_dict = {\n",
    "        \"valid\": {\n",
    "            \"1\": {\"auc\": []},\n",
    "            \"2\": {\"auc\": []},\n",
    "            \"3\": {\"auc\": []},\n",
    "        },\n",
    "        \"test\": {\n",
    "            \"1\": {\n",
    "                \"auc\": [],\n",
    "                \"best_seen_acc\": [],\n",
    "                \"best_unseen_acc\": [],\n",
    "                \"best_harmonic_mean\": [],\n",
    "            },\n",
    "            \"2\": {\"auc\": []},\n",
    "            \"3\": {\"auc\": []},\n",
    "        },\n",
    "    }\n",
    "\n",
    "    # Sum all the required statistics\n",
    "    for template_id in range(template_count):\n",
    "        exp_name = \"clip_prompt_retrieval_model_TP{}_TID{}_open0_init1_imgproj1_bias0_{}_Lim1_N400_TW0_B128_LR5e-05_WD5e-05_L0.07\".format(\n",
    "            template_src,\n",
    "            template_id,\n",
    "            args.feature,\n",
    "        )\n",
    "        output_dir = os.path.join(args.data_root, exp_name)\n",
    "        \n",
    "        # Valid metrics\n",
    "        with open(os.path.join(output_dir, \"valid_metrics.json\"), \"rb\") as f:\n",
    "            data = json.load(f)\n",
    "        all_statistics_dict[\"valid\"][\"1\"][\"auc\"].append(data[\"1\"][\"auc\"])\n",
    "        all_statistics_dict[\"valid\"][\"2\"][\"auc\"].append(data[\"2\"][\"auc\"])\n",
    "        all_statistics_dict[\"valid\"][\"3\"][\"auc\"].append(data[\"3\"][\"auc\"])\n",
    "        \n",
    "        # Test metrics\n",
    "        with open(os.path.join(output_dir, \"test_metrics.json\"), \"rb\") as f:\n",
    "            data = json.load(f)\n",
    "        all_statistics_dict[\"test\"][\"1\"][\"auc\"].append(data[\"1\"][\"auc\"])\n",
    "        all_statistics_dict[\"test\"][\"2\"][\"auc\"].append(data[\"2\"][\"auc\"])\n",
    "        all_statistics_dict[\"test\"][\"3\"][\"auc\"].append(data[\"3\"][\"auc\"])\n",
    "        all_statistics_dict[\"test\"][\"1\"][\"best_seen_acc\"].append(data[\"1\"][\"best_seen_acc\"])\n",
    "        all_statistics_dict[\"test\"][\"1\"][\"best_unseen_acc\"].append(data[\"1\"][\"best_unseen_acc\"])\n",
    "        all_statistics_dict[\"test\"][\"1\"][\"best_harmonic_mean\"].append(data[\"1\"][\"best_harmonic_mean\"])\n",
    "        \n",
    "    # Take the mean of the required statistics and output\n",
    "    print(f\"template_src: {template_src}\")\n",
    "    print(\"split: valid\")\n",
    "    for topk in all_statistics_dict[\"valid\"].keys():\n",
    "        for k, v in all_statistics_dict[\"valid\"][topk].items():\n",
    "            mean = 100 * np.mean(all_statistics_dict[\"valid\"][topk][k])\n",
    "            std = 100 * np.std(all_statistics_dict[\"valid\"][topk][k])\n",
    "            print(f\"| topk {topk} | {k} | mean: {mean:8.6f} | std: {std:8.6f}\")\n",
    "    print()\n",
    "    \n",
    "    print(\"split: test\")\n",
    "    for topk in all_statistics_dict[\"test\"].keys():\n",
    "        for k, v in all_statistics_dict[\"test\"][topk].items():\n",
    "            mean = 100 * np.mean(all_statistics_dict[\"test\"][topk][k])\n",
    "            std = 100 * np.std(all_statistics_dict[\"test\"][topk][k])\n",
    "            print(f\"| topk {topk} | {k} | mean: {mean:8.6f} | std: {std:8.6f}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5df39eda",
   "metadata": {},
   "source": [
    "## 4.2. Interpretability for Interv (GT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "97333226",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "| valid | template clip | template_id 0 |\n",
      "train pairs: 1262 | valid pairs: 600 | test pairs: 800\n",
      "train images: 30338 | valid images: 10420 | test images: 12995\n",
      "seen_indices: 1844 | unseen_indices: 8576\n",
      "seen_indices: 2380 | unseen_indices: 10615\n",
      "attr \t| train (30338, 115) \t| valid (10420, 115) \t| test (12995, 115)\n",
      "obj \t| train (30338, 245) \t| valid (10420, 245) \t| test (12995, 245)\n",
      "image_dim:    360\n",
      "attr \t| train (30338, 115) \t| valid (10420, 115) \t| test (12995, 115)\n",
      "obj \t| train (30338, 245) \t| valid (10420, 245) \t| test (12995, 245)\n",
      "image_dim:    360\n",
      "attr \t| train (30338, 115) \t| valid (10420, 115) \t| test (12995, 115)\n",
      "obj \t| train (30338, 245) \t| valid (10420, 245) \t| test (12995, 245)\n",
      "image_dim:    360\n",
      "seen_indices: 1844 | unseen_indices: 8576\n",
      "Intervention features: torch.Size([10420, 360])\n",
      "features:  torch.Size([10420, 1962])\n",
      "topk: 1\n",
      "best_seen_acc: 0.6388\n",
      "best_unseen_acc: 0.6941\n",
      "best_harmonic_mean: 0.5666\n",
      "auc: 0.4128\n",
      "topk: 2\n",
      "best_seen_acc: 0.7961\n",
      "best_unseen_acc: 0.8328\n",
      "best_harmonic_mean: 0.7336\n",
      "auc: 0.6298\n",
      "topk: 3\n",
      "best_seen_acc: 0.8530\n",
      "best_unseen_acc: 0.8561\n",
      "best_harmonic_mean: 0.7821\n",
      "auc: 0.7056\n",
      "\n",
      "\n",
      "\n",
      "======================================================================\n",
      "| valid | template clip | template_id 1 |\n",
      "train pairs: 1262 | valid pairs: 600 | test pairs: 800\n",
      "train images: 30338 | valid images: 10420 | test images: 12995\n",
      "seen_indices: 1844 | unseen_indices: 8576\n",
      "seen_indices: 2380 | unseen_indices: 10615\n",
      "attr \t| train (30338, 115) \t| valid (10420, 115) \t| test (12995, 115)\n",
      "obj \t| train (30338, 245) \t| valid (10420, 245) \t| test (12995, 245)\n",
      "image_dim:    360\n",
      "attr \t| train (30338, 115) \t| valid (10420, 115) \t| test (12995, 115)\n",
      "obj \t| train (30338, 245) \t| valid (10420, 245) \t| test (12995, 245)\n",
      "image_dim:    360\n",
      "attr \t| train (30338, 115) \t| valid (10420, 115) \t| test (12995, 115)\n",
      "obj \t| train (30338, 245) \t| valid (10420, 245) \t| test (12995, 245)\n",
      "image_dim:    360\n",
      "seen_indices: 1844 | unseen_indices: 8576\n",
      "Intervention features: torch.Size([10420, 360])\n",
      "features:  torch.Size([10420, 1962])\n",
      "topk: 1\n",
      "best_seen_acc: 0.6177\n",
      "best_unseen_acc: 0.6876\n",
      "best_harmonic_mean: 0.5558\n",
      "auc: 0.3927\n",
      "topk: 2\n",
      "best_seen_acc: 0.7711\n",
      "best_unseen_acc: 0.8017\n",
      "best_harmonic_mean: 0.6900\n",
      "auc: 0.5781\n",
      "topk: 3\n",
      "best_seen_acc: 0.8286\n",
      "best_unseen_acc: 0.8580\n",
      "best_harmonic_mean: 0.7499\n",
      "auc: 0.6757\n",
      "\n",
      "\n",
      "\n",
      "======================================================================\n",
      "| valid | template clip | template_id 2 |\n",
      "train pairs: 1262 | valid pairs: 600 | test pairs: 800\n",
      "train images: 30338 | valid images: 10420 | test images: 12995\n",
      "seen_indices: 1844 | unseen_indices: 8576\n",
      "seen_indices: 2380 | unseen_indices: 10615\n",
      "attr \t| train (30338, 115) \t| valid (10420, 115) \t| test (12995, 115)\n",
      "obj \t| train (30338, 245) \t| valid (10420, 245) \t| test (12995, 245)\n",
      "image_dim:    360\n",
      "attr \t| train (30338, 115) \t| valid (10420, 115) \t| test (12995, 115)\n",
      "obj \t| train (30338, 245) \t| valid (10420, 245) \t| test (12995, 245)\n",
      "image_dim:    360\n",
      "attr \t| train (30338, 115) \t| valid (10420, 115) \t| test (12995, 115)\n",
      "obj \t| train (30338, 245) \t| valid (10420, 245) \t| test (12995, 245)\n",
      "image_dim:    360\n",
      "seen_indices: 1844 | unseen_indices: 8576\n",
      "Intervention features: torch.Size([10420, 360])\n",
      "features:  torch.Size([10420, 1962])\n",
      "topk: 1\n",
      "best_seen_acc: 0.5705\n",
      "best_unseen_acc: 0.6224\n",
      "best_harmonic_mean: 0.5233\n",
      "auc: 0.3323\n",
      "topk: 2\n",
      "best_seen_acc: 0.7240\n",
      "best_unseen_acc: 0.7919\n",
      "best_harmonic_mean: 0.6682\n",
      "auc: 0.5367\n",
      "topk: 3\n",
      "best_seen_acc: 0.7777\n",
      "best_unseen_acc: 0.8440\n",
      "best_harmonic_mean: 0.7210\n",
      "auc: 0.6250\n",
      "\n",
      "\n",
      "\n",
      "======================================================================\n",
      "| valid | template clip | template_id 3 |\n",
      "train pairs: 1262 | valid pairs: 600 | test pairs: 800\n",
      "train images: 30338 | valid images: 10420 | test images: 12995\n",
      "seen_indices: 1844 | unseen_indices: 8576\n",
      "seen_indices: 2380 | unseen_indices: 10615\n",
      "attr \t| train (30338, 115) \t| valid (10420, 115) \t| test (12995, 115)\n",
      "obj \t| train (30338, 245) \t| valid (10420, 245) \t| test (12995, 245)\n",
      "image_dim:    360\n",
      "attr \t| train (30338, 115) \t| valid (10420, 115) \t| test (12995, 115)\n",
      "obj \t| train (30338, 245) \t| valid (10420, 245) \t| test (12995, 245)\n",
      "image_dim:    360\n",
      "attr \t| train (30338, 115) \t| valid (10420, 115) \t| test (12995, 115)\n",
      "obj \t| train (30338, 245) \t| valid (10420, 245) \t| test (12995, 245)\n",
      "image_dim:    360\n",
      "seen_indices: 1844 | unseen_indices: 8576\n",
      "Intervention features: torch.Size([10420, 360])\n",
      "features:  torch.Size([10420, 1962])\n",
      "topk: 1\n",
      "best_seen_acc: 0.6752\n",
      "best_unseen_acc: 0.7058\n",
      "best_harmonic_mean: 0.5888\n",
      "auc: 0.4364\n",
      "topk: 2\n",
      "best_seen_acc: 0.8178\n",
      "best_unseen_acc: 0.8218\n",
      "best_harmonic_mean: 0.7122\n",
      "auc: 0.6312\n",
      "topk: 3\n",
      "best_seen_acc: 0.8671\n",
      "best_unseen_acc: 0.8906\n",
      "best_harmonic_mean: 0.7830\n",
      "auc: 0.7297\n",
      "\n",
      "\n",
      "\n",
      "======================================================================\n",
      "| valid | template clip | template_id 4 |\n",
      "train pairs: 1262 | valid pairs: 600 | test pairs: 800\n",
      "train images: 30338 | valid images: 10420 | test images: 12995\n",
      "seen_indices: 1844 | unseen_indices: 8576\n",
      "seen_indices: 2380 | unseen_indices: 10615\n",
      "attr \t| train (30338, 115) \t| valid (10420, 115) \t| test (12995, 115)\n",
      "obj \t| train (30338, 245) \t| valid (10420, 245) \t| test (12995, 245)\n",
      "image_dim:    360\n",
      "attr \t| train (30338, 115) \t| valid (10420, 115) \t| test (12995, 115)\n",
      "obj \t| train (30338, 245) \t| valid (10420, 245) \t| test (12995, 245)\n",
      "image_dim:    360\n",
      "attr \t| train (30338, 115) \t| valid (10420, 115) \t| test (12995, 115)\n",
      "obj \t| train (30338, 245) \t| valid (10420, 245) \t| test (12995, 245)\n",
      "image_dim:    360\n",
      "seen_indices: 1844 | unseen_indices: 8576\n",
      "Intervention features: torch.Size([10420, 360])\n",
      "features:  torch.Size([10420, 1962])\n",
      "topk: 1\n",
      "best_seen_acc: 0.5770\n",
      "best_unseen_acc: 0.6600\n",
      "best_harmonic_mean: 0.5150\n",
      "auc: 0.3437\n",
      "topk: 2\n",
      "best_seen_acc: 0.7424\n",
      "best_unseen_acc: 0.7957\n",
      "best_harmonic_mean: 0.6492\n",
      "auc: 0.5397\n",
      "topk: 3\n",
      "best_seen_acc: 0.8265\n",
      "best_unseen_acc: 0.8533\n",
      "best_harmonic_mean: 0.7171\n",
      "auc: 0.6546\n",
      "\n",
      "\n",
      "\n",
      "======================================================================\n",
      "| valid | template clip | template_id 5 |\n",
      "train pairs: 1262 | valid pairs: 600 | test pairs: 800\n",
      "train images: 30338 | valid images: 10420 | test images: 12995\n",
      "seen_indices: 1844 | unseen_indices: 8576\n",
      "seen_indices: 2380 | unseen_indices: 10615\n",
      "attr \t| train (30338, 115) \t| valid (10420, 115) \t| test (12995, 115)\n",
      "obj \t| train (30338, 245) \t| valid (10420, 245) \t| test (12995, 245)\n",
      "image_dim:    360\n",
      "attr \t| train (30338, 115) \t| valid (10420, 115) \t| test (12995, 115)\n",
      "obj \t| train (30338, 245) \t| valid (10420, 245) \t| test (12995, 245)\n",
      "image_dim:    360\n",
      "attr \t| train (30338, 115) \t| valid (10420, 115) \t| test (12995, 115)\n",
      "obj \t| train (30338, 245) \t| valid (10420, 245) \t| test (12995, 245)\n",
      "image_dim:    360\n",
      "seen_indices: 1844 | unseen_indices: 8576\n",
      "Intervention features: torch.Size([10420, 360])\n",
      "features:  torch.Size([10420, 1962])\n",
      "topk: 1\n",
      "best_seen_acc: 0.6616\n",
      "best_unseen_acc: 0.7165\n",
      "best_harmonic_mean: 0.5935\n",
      "auc: 0.4418\n",
      "topk: 2\n",
      "best_seen_acc: 0.7825\n",
      "best_unseen_acc: 0.7971\n",
      "best_harmonic_mean: 0.7083\n",
      "auc: 0.5969\n",
      "topk: 3\n",
      "best_seen_acc: 0.8747\n",
      "best_unseen_acc: 0.8556\n",
      "best_harmonic_mean: 0.7762\n",
      "auc: 0.7166\n",
      "\n",
      "\n",
      "\n",
      "======================================================================\n",
      "| valid | template clip | template_id 6 |\n",
      "train pairs: 1262 | valid pairs: 600 | test pairs: 800\n",
      "train images: 30338 | valid images: 10420 | test images: 12995\n",
      "seen_indices: 1844 | unseen_indices: 8576\n",
      "seen_indices: 2380 | unseen_indices: 10615\n",
      "attr \t| train (30338, 115) \t| valid (10420, 115) \t| test (12995, 115)\n",
      "obj \t| train (30338, 245) \t| valid (10420, 245) \t| test (12995, 245)\n",
      "image_dim:    360\n",
      "attr \t| train (30338, 115) \t| valid (10420, 115) \t| test (12995, 115)\n",
      "obj \t| train (30338, 245) \t| valid (10420, 245) \t| test (12995, 245)\n",
      "image_dim:    360\n",
      "attr \t| train (30338, 115) \t| valid (10420, 115) \t| test (12995, 115)\n",
      "obj \t| train (30338, 245) \t| valid (10420, 245) \t| test (12995, 245)\n",
      "image_dim:    360\n",
      "seen_indices: 1844 | unseen_indices: 8576\n",
      "Intervention features: torch.Size([10420, 360])\n",
      "features:  torch.Size([10420, 1962])\n",
      "topk: 1\n",
      "best_seen_acc: 0.6074\n",
      "best_unseen_acc: 0.6897\n",
      "best_harmonic_mean: 0.5693\n",
      "auc: 0.3890\n",
      "topk: 2\n",
      "best_seen_acc: 0.7576\n",
      "best_unseen_acc: 0.8291\n",
      "best_harmonic_mean: 0.6882\n",
      "auc: 0.5928\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "topk: 3\n",
      "best_seen_acc: 0.8297\n",
      "best_unseen_acc: 0.8720\n",
      "best_harmonic_mean: 0.7561\n",
      "auc: 0.6913\n",
      "\n",
      "\n",
      "\n",
      "======================================================================\n",
      "| valid | template compdl | template_id 0 |\n",
      "train pairs: 1262 | valid pairs: 600 | test pairs: 800\n",
      "train images: 30338 | valid images: 10420 | test images: 12995\n",
      "seen_indices: 1844 | unseen_indices: 8576\n",
      "seen_indices: 2380 | unseen_indices: 10615\n",
      "attr \t| train (30338, 115) \t| valid (10420, 115) \t| test (12995, 115)\n",
      "obj \t| train (30338, 245) \t| valid (10420, 245) \t| test (12995, 245)\n",
      "image_dim:    360\n",
      "attr \t| train (30338, 115) \t| valid (10420, 115) \t| test (12995, 115)\n",
      "obj \t| train (30338, 245) \t| valid (10420, 245) \t| test (12995, 245)\n",
      "image_dim:    360\n",
      "attr \t| train (30338, 115) \t| valid (10420, 115) \t| test (12995, 115)\n",
      "obj \t| train (30338, 245) \t| valid (10420, 245) \t| test (12995, 245)\n",
      "image_dim:    360\n",
      "seen_indices: 1844 | unseen_indices: 8576\n",
      "Intervention features: torch.Size([10420, 360])\n",
      "features:  torch.Size([10420, 1962])\n",
      "topk: 1\n",
      "best_seen_acc: 0.5640\n",
      "best_unseen_acc: 0.6247\n",
      "best_harmonic_mean: 0.4965\n",
      "auc: 0.3232\n",
      "topk: 2\n",
      "best_seen_acc: 0.6768\n",
      "best_unseen_acc: 0.7850\n",
      "best_harmonic_mean: 0.6195\n",
      "auc: 0.4899\n",
      "topk: 3\n",
      "best_seen_acc: 0.7565\n",
      "best_unseen_acc: 0.8209\n",
      "best_harmonic_mean: 0.6858\n",
      "auc: 0.5663\n",
      "\n",
      "\n",
      "\n",
      "======================================================================\n",
      "| valid | template compdl | template_id 1 |\n",
      "train pairs: 1262 | valid pairs: 600 | test pairs: 800\n",
      "train images: 30338 | valid images: 10420 | test images: 12995\n",
      "seen_indices: 1844 | unseen_indices: 8576\n",
      "seen_indices: 2380 | unseen_indices: 10615\n",
      "attr \t| train (30338, 115) \t| valid (10420, 115) \t| test (12995, 115)\n",
      "obj \t| train (30338, 245) \t| valid (10420, 245) \t| test (12995, 245)\n",
      "image_dim:    360\n",
      "attr \t| train (30338, 115) \t| valid (10420, 115) \t| test (12995, 115)\n",
      "obj \t| train (30338, 245) \t| valid (10420, 245) \t| test (12995, 245)\n",
      "image_dim:    360\n",
      "attr \t| train (30338, 115) \t| valid (10420, 115) \t| test (12995, 115)\n",
      "obj \t| train (30338, 245) \t| valid (10420, 245) \t| test (12995, 245)\n",
      "image_dim:    360\n",
      "seen_indices: 1844 | unseen_indices: 8576\n",
      "Intervention features: torch.Size([10420, 360])\n",
      "features:  torch.Size([10420, 1962])\n",
      "topk: 1\n",
      "best_seen_acc: 0.6150\n",
      "best_unseen_acc: 0.7243\n",
      "best_harmonic_mean: 0.5695\n",
      "auc: 0.4073\n",
      "topk: 2\n",
      "best_seen_acc: 0.7717\n",
      "best_unseen_acc: 0.8434\n",
      "best_harmonic_mean: 0.7118\n",
      "auc: 0.6146\n",
      "topk: 3\n",
      "best_seen_acc: 0.8568\n",
      "best_unseen_acc: 0.9031\n",
      "best_harmonic_mean: 0.7804\n",
      "auc: 0.7385\n",
      "\n",
      "\n",
      "\n",
      "======================================================================\n",
      "| valid | template compdl | template_id 2 |\n",
      "train pairs: 1262 | valid pairs: 600 | test pairs: 800\n",
      "train images: 30338 | valid images: 10420 | test images: 12995\n",
      "seen_indices: 1844 | unseen_indices: 8576\n",
      "seen_indices: 2380 | unseen_indices: 10615\n",
      "attr \t| train (30338, 115) \t| valid (10420, 115) \t| test (12995, 115)\n",
      "obj \t| train (30338, 245) \t| valid (10420, 245) \t| test (12995, 245)\n",
      "image_dim:    360\n",
      "attr \t| train (30338, 115) \t| valid (10420, 115) \t| test (12995, 115)\n",
      "obj \t| train (30338, 245) \t| valid (10420, 245) \t| test (12995, 245)\n",
      "image_dim:    360\n",
      "attr \t| train (30338, 115) \t| valid (10420, 115) \t| test (12995, 115)\n",
      "obj \t| train (30338, 245) \t| valid (10420, 245) \t| test (12995, 245)\n",
      "image_dim:    360\n",
      "seen_indices: 1844 | unseen_indices: 8576\n",
      "Intervention features: torch.Size([10420, 360])\n",
      "features:  torch.Size([10420, 1962])\n",
      "topk: 1\n",
      "best_seen_acc: 0.5879\n",
      "best_unseen_acc: 0.6398\n",
      "best_harmonic_mean: 0.5190\n",
      "auc: 0.3455\n",
      "topk: 2\n",
      "best_seen_acc: 0.7272\n",
      "best_unseen_acc: 0.7914\n",
      "best_harmonic_mean: 0.6519\n",
      "auc: 0.5286\n",
      "topk: 3\n",
      "best_seen_acc: 0.7890\n",
      "best_unseen_acc: 0.8714\n",
      "best_harmonic_mean: 0.7548\n",
      "auc: 0.6506\n",
      "\n",
      "\n",
      "\n",
      "======================================================================\n",
      "| valid | template compdl | template_id 3 |\n",
      "train pairs: 1262 | valid pairs: 600 | test pairs: 800\n",
      "train images: 30338 | valid images: 10420 | test images: 12995\n",
      "seen_indices: 1844 | unseen_indices: 8576\n",
      "seen_indices: 2380 | unseen_indices: 10615\n",
      "attr \t| train (30338, 115) \t| valid (10420, 115) \t| test (12995, 115)\n",
      "obj \t| train (30338, 245) \t| valid (10420, 245) \t| test (12995, 245)\n",
      "image_dim:    360\n",
      "attr \t| train (30338, 115) \t| valid (10420, 115) \t| test (12995, 115)\n",
      "obj \t| train (30338, 245) \t| valid (10420, 245) \t| test (12995, 245)\n",
      "image_dim:    360\n",
      "attr \t| train (30338, 115) \t| valid (10420, 115) \t| test (12995, 115)\n",
      "obj \t| train (30338, 245) \t| valid (10420, 245) \t| test (12995, 245)\n",
      "image_dim:    360\n",
      "seen_indices: 1844 | unseen_indices: 8576\n",
      "Intervention features: torch.Size([10420, 360])\n",
      "features:  torch.Size([10420, 1962])\n",
      "topk: 1\n",
      "best_seen_acc: 0.5504\n",
      "best_unseen_acc: 0.6305\n",
      "best_harmonic_mean: 0.4932\n",
      "auc: 0.3148\n",
      "topk: 2\n",
      "best_seen_acc: 0.6849\n",
      "best_unseen_acc: 0.7507\n",
      "best_harmonic_mean: 0.6253\n",
      "auc: 0.4767\n",
      "topk: 3\n",
      "best_seen_acc: 0.7809\n",
      "best_unseen_acc: 0.8168\n",
      "best_harmonic_mean: 0.7031\n",
      "auc: 0.5960\n",
      "\n",
      "\n",
      "\n",
      "======================================================================\n",
      "| valid | template compdl | template_id 4 |\n",
      "train pairs: 1262 | valid pairs: 600 | test pairs: 800\n",
      "train images: 30338 | valid images: 10420 | test images: 12995\n",
      "seen_indices: 1844 | unseen_indices: 8576\n",
      "seen_indices: 2380 | unseen_indices: 10615\n",
      "attr \t| train (30338, 115) \t| valid (10420, 115) \t| test (12995, 115)\n",
      "obj \t| train (30338, 245) \t| valid (10420, 245) \t| test (12995, 245)\n",
      "image_dim:    360\n",
      "attr \t| train (30338, 115) \t| valid (10420, 115) \t| test (12995, 115)\n",
      "obj \t| train (30338, 245) \t| valid (10420, 245) \t| test (12995, 245)\n",
      "image_dim:    360\n",
      "attr \t| train (30338, 115) \t| valid (10420, 115) \t| test (12995, 115)\n",
      "obj \t| train (30338, 245) \t| valid (10420, 245) \t| test (12995, 245)\n",
      "image_dim:    360\n",
      "seen_indices: 1844 | unseen_indices: 8576\n",
      "Intervention features: torch.Size([10420, 360])\n",
      "features:  torch.Size([10420, 1962])\n",
      "topk: 1\n",
      "best_seen_acc: 0.5531\n",
      "best_unseen_acc: 0.6479\n",
      "best_harmonic_mean: 0.5049\n",
      "auc: 0.3206\n",
      "topk: 2\n",
      "best_seen_acc: 0.7202\n",
      "best_unseen_acc: 0.8099\n",
      "best_harmonic_mean: 0.6385\n",
      "auc: 0.5329\n",
      "topk: 3\n",
      "best_seen_acc: 0.8200\n",
      "best_unseen_acc: 0.8827\n",
      "best_harmonic_mean: 0.7409\n",
      "auc: 0.6717\n",
      "\n",
      "\n",
      "\n",
      "======================================================================\n",
      "| valid | template compdl | template_id 5 |\n",
      "train pairs: 1262 | valid pairs: 600 | test pairs: 800\n",
      "train images: 30338 | valid images: 10420 | test images: 12995\n",
      "seen_indices: 1844 | unseen_indices: 8576\n",
      "seen_indices: 2380 | unseen_indices: 10615\n",
      "attr \t| train (30338, 115) \t| valid (10420, 115) \t| test (12995, 115)\n",
      "obj \t| train (30338, 245) \t| valid (10420, 245) \t| test (12995, 245)\n",
      "image_dim:    360\n",
      "attr \t| train (30338, 115) \t| valid (10420, 115) \t| test (12995, 115)\n",
      "obj \t| train (30338, 245) \t| valid (10420, 245) \t| test (12995, 245)\n",
      "image_dim:    360\n",
      "attr \t| train (30338, 115) \t| valid (10420, 115) \t| test (12995, 115)\n",
      "obj \t| train (30338, 245) \t| valid (10420, 245) \t| test (12995, 245)\n",
      "image_dim:    360\n",
      "seen_indices: 1844 | unseen_indices: 8576\n",
      "Intervention features: torch.Size([10420, 360])\n",
      "features:  torch.Size([10420, 1962])\n",
      "topk: 1\n",
      "best_seen_acc: 0.6020\n",
      "best_unseen_acc: 0.6424\n",
      "best_harmonic_mean: 0.5120\n",
      "auc: 0.3520\n",
      "topk: 2\n",
      "best_seen_acc: 0.7158\n",
      "best_unseen_acc: 0.7562\n",
      "best_harmonic_mean: 0.6532\n",
      "auc: 0.5096\n",
      "topk: 3\n",
      "best_seen_acc: 0.8026\n",
      "best_unseen_acc: 0.8403\n",
      "best_harmonic_mean: 0.7370\n",
      "auc: 0.6417\n",
      "\n",
      "\n",
      "\n",
      "======================================================================\n",
      "| valid | template compdl | template_id 6 |\n",
      "train pairs: 1262 | valid pairs: 600 | test pairs: 800\n",
      "train images: 30338 | valid images: 10420 | test images: 12995\n",
      "seen_indices: 1844 | unseen_indices: 8576\n",
      "seen_indices: 2380 | unseen_indices: 10615\n",
      "attr \t| train (30338, 115) \t| valid (10420, 115) \t| test (12995, 115)\n",
      "obj \t| train (30338, 245) \t| valid (10420, 245) \t| test (12995, 245)\n",
      "image_dim:    360\n",
      "attr \t| train (30338, 115) \t| valid (10420, 115) \t| test (12995, 115)\n",
      "obj \t| train (30338, 245) \t| valid (10420, 245) \t| test (12995, 245)\n",
      "image_dim:    360\n",
      "attr \t| train (30338, 115) \t| valid (10420, 115) \t| test (12995, 115)\n",
      "obj \t| train (30338, 245) \t| valid (10420, 245) \t| test (12995, 245)\n",
      "image_dim:    360\n",
      "seen_indices: 1844 | unseen_indices: 8576\n",
      "Intervention features: torch.Size([10420, 360])\n",
      "features:  torch.Size([10420, 1962])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "topk: 1\n",
      "best_seen_acc: 0.6198\n",
      "best_unseen_acc: 0.6974\n",
      "best_harmonic_mean: 0.5528\n",
      "auc: 0.3940\n",
      "topk: 2\n",
      "best_seen_acc: 0.7587\n",
      "best_unseen_acc: 0.8336\n",
      "best_harmonic_mean: 0.6924\n",
      "auc: 0.5938\n",
      "topk: 3\n",
      "best_seen_acc: 0.8270\n",
      "best_unseen_acc: 0.9144\n",
      "best_harmonic_mean: 0.7745\n",
      "auc: 0.7161\n",
      "\n",
      "\n",
      "\n",
      "======================================================================\n",
      "| valid | template compdl | template_id 7 |\n",
      "train pairs: 1262 | valid pairs: 600 | test pairs: 800\n",
      "train images: 30338 | valid images: 10420 | test images: 12995\n",
      "seen_indices: 1844 | unseen_indices: 8576\n",
      "seen_indices: 2380 | unseen_indices: 10615\n",
      "attr \t| train (30338, 115) \t| valid (10420, 115) \t| test (12995, 115)\n",
      "obj \t| train (30338, 245) \t| valid (10420, 245) \t| test (12995, 245)\n",
      "image_dim:    360\n",
      "attr \t| train (30338, 115) \t| valid (10420, 115) \t| test (12995, 115)\n",
      "obj \t| train (30338, 245) \t| valid (10420, 245) \t| test (12995, 245)\n",
      "image_dim:    360\n",
      "attr \t| train (30338, 115) \t| valid (10420, 115) \t| test (12995, 115)\n",
      "obj \t| train (30338, 245) \t| valid (10420, 245) \t| test (12995, 245)\n",
      "image_dim:    360\n",
      "seen_indices: 1844 | unseen_indices: 8576\n",
      "Intervention features: torch.Size([10420, 360])\n",
      "features:  torch.Size([10420, 1962])\n",
      "topk: 1\n",
      "best_seen_acc: 0.5483\n",
      "best_unseen_acc: 0.6315\n",
      "best_harmonic_mean: 0.4921\n",
      "auc: 0.3168\n",
      "topk: 2\n",
      "best_seen_acc: 0.7207\n",
      "best_unseen_acc: 0.7759\n",
      "best_harmonic_mean: 0.6618\n",
      "auc: 0.5201\n",
      "topk: 3\n",
      "best_seen_acc: 0.8004\n",
      "best_unseen_acc: 0.8580\n",
      "best_harmonic_mean: 0.7303\n",
      "auc: 0.6430\n",
      "\n",
      "\n",
      "\n",
      "======================================================================\n",
      "| valid | template compdl | template_id 8 |\n",
      "train pairs: 1262 | valid pairs: 600 | test pairs: 800\n",
      "train images: 30338 | valid images: 10420 | test images: 12995\n",
      "seen_indices: 1844 | unseen_indices: 8576\n",
      "seen_indices: 2380 | unseen_indices: 10615\n",
      "attr \t| train (30338, 115) \t| valid (10420, 115) \t| test (12995, 115)\n",
      "obj \t| train (30338, 245) \t| valid (10420, 245) \t| test (12995, 245)\n",
      "image_dim:    360\n",
      "attr \t| train (30338, 115) \t| valid (10420, 115) \t| test (12995, 115)\n",
      "obj \t| train (30338, 245) \t| valid (10420, 245) \t| test (12995, 245)\n",
      "image_dim:    360\n",
      "attr \t| train (30338, 115) \t| valid (10420, 115) \t| test (12995, 115)\n",
      "obj \t| train (30338, 245) \t| valid (10420, 245) \t| test (12995, 245)\n",
      "image_dim:    360\n",
      "seen_indices: 1844 | unseen_indices: 8576\n",
      "Intervention features: torch.Size([10420, 360])\n",
      "features:  torch.Size([10420, 1962])\n",
      "topk: 1\n",
      "best_seen_acc: 0.5081\n",
      "best_unseen_acc: 0.5791\n",
      "best_harmonic_mean: 0.4411\n",
      "auc: 0.2597\n",
      "topk: 2\n",
      "best_seen_acc: 0.6329\n",
      "best_unseen_acc: 0.7733\n",
      "best_harmonic_mean: 0.5742\n",
      "auc: 0.4341\n",
      "topk: 3\n",
      "best_seen_acc: 0.7223\n",
      "best_unseen_acc: 0.8523\n",
      "best_harmonic_mean: 0.6533\n",
      "auc: 0.5569\n",
      "\n",
      "\n",
      "\n",
      "======================================================================\n",
      "| valid | template compdl | template_id 9 |\n",
      "train pairs: 1262 | valid pairs: 600 | test pairs: 800\n",
      "train images: 30338 | valid images: 10420 | test images: 12995\n",
      "seen_indices: 1844 | unseen_indices: 8576\n",
      "seen_indices: 2380 | unseen_indices: 10615\n",
      "attr \t| train (30338, 115) \t| valid (10420, 115) \t| test (12995, 115)\n",
      "obj \t| train (30338, 245) \t| valid (10420, 245) \t| test (12995, 245)\n",
      "image_dim:    360\n",
      "attr \t| train (30338, 115) \t| valid (10420, 115) \t| test (12995, 115)\n",
      "obj \t| train (30338, 245) \t| valid (10420, 245) \t| test (12995, 245)\n",
      "image_dim:    360\n",
      "attr \t| train (30338, 115) \t| valid (10420, 115) \t| test (12995, 115)\n",
      "obj \t| train (30338, 245) \t| valid (10420, 245) \t| test (12995, 245)\n",
      "image_dim:    360\n",
      "seen_indices: 1844 | unseen_indices: 8576\n",
      "Intervention features: torch.Size([10420, 360])\n",
      "features:  torch.Size([10420, 1962])\n",
      "topk: 1\n",
      "best_seen_acc: 0.5537\n",
      "best_unseen_acc: 0.6665\n",
      "best_harmonic_mean: 0.5127\n",
      "auc: 0.3372\n",
      "topk: 2\n",
      "best_seen_acc: 0.7088\n",
      "best_unseen_acc: 0.7720\n",
      "best_harmonic_mean: 0.6474\n",
      "auc: 0.5143\n",
      "topk: 3\n",
      "best_seen_acc: 0.7804\n",
      "best_unseen_acc: 0.8540\n",
      "best_harmonic_mean: 0.7276\n",
      "auc: 0.6287\n",
      "\n",
      "\n",
      "\n",
      "======================================================================\n",
      "| test | template clip | template_id 0 |\n",
      "train pairs: 1262 | valid pairs: 600 | test pairs: 800\n",
      "train images: 30338 | valid images: 10420 | test images: 12995\n",
      "seen_indices: 1844 | unseen_indices: 8576\n",
      "seen_indices: 2380 | unseen_indices: 10615\n",
      "attr \t| train (30338, 115) \t| valid (10420, 115) \t| test (12995, 115)\n",
      "obj \t| train (30338, 245) \t| valid (10420, 245) \t| test (12995, 245)\n",
      "image_dim:    360\n",
      "attr \t| train (30338, 115) \t| valid (10420, 115) \t| test (12995, 115)\n",
      "obj \t| train (30338, 245) \t| valid (10420, 245) \t| test (12995, 245)\n",
      "image_dim:    360\n",
      "attr \t| train (30338, 115) \t| valid (10420, 115) \t| test (12995, 115)\n",
      "obj \t| train (30338, 245) \t| valid (10420, 245) \t| test (12995, 245)\n",
      "image_dim:    360\n",
      "seen_indices: 2380 | unseen_indices: 10615\n",
      "Intervention features: torch.Size([12995, 360])\n",
      "features:  torch.Size([12995, 1962])\n",
      "topk: 1\n",
      "best_seen_acc: 0.6143\n",
      "best_unseen_acc: 0.7264\n",
      "best_harmonic_mean: 0.5663\n",
      "auc: 0.4129\n",
      "topk: 2\n",
      "best_seen_acc: 0.7529\n",
      "best_unseen_acc: 0.8300\n",
      "best_harmonic_mean: 0.7066\n",
      "auc: 0.5965\n",
      "topk: 3\n",
      "best_seen_acc: 0.8290\n",
      "best_unseen_acc: 0.8795\n",
      "best_harmonic_mean: 0.7753\n",
      "auc: 0.7009\n",
      "\n",
      "\n",
      "\n",
      "======================================================================\n",
      "| test | template clip | template_id 1 |\n",
      "train pairs: 1262 | valid pairs: 600 | test pairs: 800\n",
      "train images: 30338 | valid images: 10420 | test images: 12995\n",
      "seen_indices: 1844 | unseen_indices: 8576\n",
      "seen_indices: 2380 | unseen_indices: 10615\n",
      "attr \t| train (30338, 115) \t| valid (10420, 115) \t| test (12995, 115)\n",
      "obj \t| train (30338, 245) \t| valid (10420, 245) \t| test (12995, 245)\n",
      "image_dim:    360\n",
      "attr \t| train (30338, 115) \t| valid (10420, 115) \t| test (12995, 115)\n",
      "obj \t| train (30338, 245) \t| valid (10420, 245) \t| test (12995, 245)\n",
      "image_dim:    360\n",
      "attr \t| train (30338, 115) \t| valid (10420, 115) \t| test (12995, 115)\n",
      "obj \t| train (30338, 245) \t| valid (10420, 245) \t| test (12995, 245)\n",
      "image_dim:    360\n",
      "seen_indices: 2380 | unseen_indices: 10615\n",
      "Intervention features: torch.Size([12995, 360])\n",
      "features:  torch.Size([12995, 1962])\n",
      "topk: 1\n",
      "best_seen_acc: 0.6122\n",
      "best_unseen_acc: 0.7372\n",
      "best_harmonic_mean: 0.5759\n",
      "auc: 0.4211\n",
      "topk: 2\n",
      "best_seen_acc: 0.7466\n",
      "best_unseen_acc: 0.8460\n",
      "best_harmonic_mean: 0.7155\n",
      "auc: 0.5946\n",
      "topk: 3\n",
      "best_seen_acc: 0.8088\n",
      "best_unseen_acc: 0.9063\n",
      "best_harmonic_mean: 0.7701\n",
      "auc: 0.6893\n",
      "\n",
      "\n",
      "\n",
      "======================================================================\n",
      "| test | template clip | template_id 2 |\n",
      "train pairs: 1262 | valid pairs: 600 | test pairs: 800\n",
      "train images: 30338 | valid images: 10420 | test images: 12995\n",
      "seen_indices: 1844 | unseen_indices: 8576\n",
      "seen_indices: 2380 | unseen_indices: 10615\n",
      "attr \t| train (30338, 115) \t| valid (10420, 115) \t| test (12995, 115)\n",
      "obj \t| train (30338, 245) \t| valid (10420, 245) \t| test (12995, 245)\n",
      "image_dim:    360\n",
      "attr \t| train (30338, 115) \t| valid (10420, 115) \t| test (12995, 115)\n",
      "obj \t| train (30338, 245) \t| valid (10420, 245) \t| test (12995, 245)\n",
      "image_dim:    360\n",
      "attr \t| train (30338, 115) \t| valid (10420, 115) \t| test (12995, 115)\n",
      "obj \t| train (30338, 245) \t| valid (10420, 245) \t| test (12995, 245)\n",
      "image_dim:    360\n",
      "seen_indices: 2380 | unseen_indices: 10615\n",
      "Intervention features: torch.Size([12995, 360])\n",
      "features:  torch.Size([12995, 1962])\n",
      "topk: 1\n",
      "best_seen_acc: 0.5487\n",
      "best_unseen_acc: 0.7100\n",
      "best_harmonic_mean: 0.5271\n",
      "auc: 0.3603\n",
      "topk: 2\n",
      "best_seen_acc: 0.6929\n",
      "best_unseen_acc: 0.8443\n",
      "best_harmonic_mean: 0.6574\n",
      "auc: 0.5414\n",
      "topk: 3\n",
      "best_seen_acc: 0.7765\n",
      "best_unseen_acc: 0.8975\n",
      "best_harmonic_mean: 0.7340\n",
      "auc: 0.6599\n",
      "\n",
      "\n",
      "\n",
      "======================================================================\n",
      "| test | template clip | template_id 3 |\n",
      "train pairs: 1262 | valid pairs: 600 | test pairs: 800\n",
      "train images: 30338 | valid images: 10420 | test images: 12995\n",
      "seen_indices: 1844 | unseen_indices: 8576\n",
      "seen_indices: 2380 | unseen_indices: 10615\n",
      "attr \t| train (30338, 115) \t| valid (10420, 115) \t| test (12995, 115)\n",
      "obj \t| train (30338, 245) \t| valid (10420, 245) \t| test (12995, 245)\n",
      "image_dim:    360\n",
      "attr \t| train (30338, 115) \t| valid (10420, 115) \t| test (12995, 115)\n",
      "obj \t| train (30338, 245) \t| valid (10420, 245) \t| test (12995, 245)\n",
      "image_dim:    360\n",
      "attr \t| train (30338, 115) \t| valid (10420, 115) \t| test (12995, 115)\n",
      "obj \t| train (30338, 245) \t| valid (10420, 245) \t| test (12995, 245)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "image_dim:    360\n",
      "seen_indices: 2380 | unseen_indices: 10615\n",
      "Intervention features: torch.Size([12995, 360])\n",
      "features:  torch.Size([12995, 1962])\n",
      "topk: 1\n",
      "best_seen_acc: 0.5899\n",
      "best_unseen_acc: 0.7511\n",
      "best_harmonic_mean: 0.5631\n",
      "auc: 0.4087\n",
      "topk: 2\n",
      "best_seen_acc: 0.7445\n",
      "best_unseen_acc: 0.8539\n",
      "best_harmonic_mean: 0.7120\n",
      "auc: 0.5979\n",
      "topk: 3\n",
      "best_seen_acc: 0.8218\n",
      "best_unseen_acc: 0.9048\n",
      "best_harmonic_mean: 0.7758\n",
      "auc: 0.7102\n",
      "\n",
      "\n",
      "\n",
      "======================================================================\n",
      "| test | template clip | template_id 4 |\n",
      "train pairs: 1262 | valid pairs: 600 | test pairs: 800\n",
      "train images: 30338 | valid images: 10420 | test images: 12995\n",
      "seen_indices: 1844 | unseen_indices: 8576\n",
      "seen_indices: 2380 | unseen_indices: 10615\n",
      "attr \t| train (30338, 115) \t| valid (10420, 115) \t| test (12995, 115)\n",
      "obj \t| train (30338, 245) \t| valid (10420, 245) \t| test (12995, 245)\n",
      "image_dim:    360\n",
      "attr \t| train (30338, 115) \t| valid (10420, 115) \t| test (12995, 115)\n",
      "obj \t| train (30338, 245) \t| valid (10420, 245) \t| test (12995, 245)\n",
      "image_dim:    360\n",
      "attr \t| train (30338, 115) \t| valid (10420, 115) \t| test (12995, 115)\n",
      "obj \t| train (30338, 245) \t| valid (10420, 245) \t| test (12995, 245)\n",
      "image_dim:    360\n",
      "seen_indices: 2380 | unseen_indices: 10615\n",
      "Intervention features: torch.Size([12995, 360])\n",
      "features:  torch.Size([12995, 1962])\n",
      "topk: 1\n",
      "best_seen_acc: 0.5689\n",
      "best_unseen_acc: 0.6769\n",
      "best_harmonic_mean: 0.5123\n",
      "auc: 0.3478\n",
      "topk: 2\n",
      "best_seen_acc: 0.7004\n",
      "best_unseen_acc: 0.8095\n",
      "best_harmonic_mean: 0.6401\n",
      "auc: 0.5252\n",
      "topk: 3\n",
      "best_seen_acc: 0.7718\n",
      "best_unseen_acc: 0.8617\n",
      "best_harmonic_mean: 0.7274\n",
      "auc: 0.6336\n",
      "\n",
      "\n",
      "\n",
      "======================================================================\n",
      "| test | template clip | template_id 5 |\n",
      "train pairs: 1262 | valid pairs: 600 | test pairs: 800\n",
      "train images: 30338 | valid images: 10420 | test images: 12995\n",
      "seen_indices: 1844 | unseen_indices: 8576\n",
      "seen_indices: 2380 | unseen_indices: 10615\n",
      "attr \t| train (30338, 115) \t| valid (10420, 115) \t| test (12995, 115)\n",
      "obj \t| train (30338, 245) \t| valid (10420, 245) \t| test (12995, 245)\n",
      "image_dim:    360\n",
      "attr \t| train (30338, 115) \t| valid (10420, 115) \t| test (12995, 115)\n",
      "obj \t| train (30338, 245) \t| valid (10420, 245) \t| test (12995, 245)\n",
      "image_dim:    360\n",
      "attr \t| train (30338, 115) \t| valid (10420, 115) \t| test (12995, 115)\n",
      "obj \t| train (30338, 245) \t| valid (10420, 245) \t| test (12995, 245)\n",
      "image_dim:    360\n",
      "seen_indices: 2380 | unseen_indices: 10615\n",
      "Intervention features: torch.Size([12995, 360])\n",
      "features:  torch.Size([12995, 1962])\n",
      "topk: 1\n",
      "best_seen_acc: 0.6206\n",
      "best_unseen_acc: 0.7458\n",
      "best_harmonic_mean: 0.5957\n",
      "auc: 0.4367\n",
      "topk: 2\n",
      "best_seen_acc: 0.7739\n",
      "best_unseen_acc: 0.8315\n",
      "best_harmonic_mean: 0.7177\n",
      "auc: 0.6136\n",
      "topk: 3\n",
      "best_seen_acc: 0.8261\n",
      "best_unseen_acc: 0.8800\n",
      "best_harmonic_mean: 0.7812\n",
      "auc: 0.7063\n",
      "\n",
      "\n",
      "\n",
      "======================================================================\n",
      "| test | template clip | template_id 6 |\n",
      "train pairs: 1262 | valid pairs: 600 | test pairs: 800\n",
      "train images: 30338 | valid images: 10420 | test images: 12995\n",
      "seen_indices: 1844 | unseen_indices: 8576\n",
      "seen_indices: 2380 | unseen_indices: 10615\n",
      "attr \t| train (30338, 115) \t| valid (10420, 115) \t| test (12995, 115)\n",
      "obj \t| train (30338, 245) \t| valid (10420, 245) \t| test (12995, 245)\n",
      "image_dim:    360\n",
      "attr \t| train (30338, 115) \t| valid (10420, 115) \t| test (12995, 115)\n",
      "obj \t| train (30338, 245) \t| valid (10420, 245) \t| test (12995, 245)\n",
      "image_dim:    360\n",
      "attr \t| train (30338, 115) \t| valid (10420, 115) \t| test (12995, 115)\n",
      "obj \t| train (30338, 245) \t| valid (10420, 245) \t| test (12995, 245)\n",
      "image_dim:    360\n",
      "seen_indices: 2380 | unseen_indices: 10615\n",
      "Intervention features: torch.Size([12995, 360])\n",
      "features:  torch.Size([12995, 1962])\n",
      "topk: 1\n",
      "best_seen_acc: 0.5920\n",
      "best_unseen_acc: 0.7392\n",
      "best_harmonic_mean: 0.5589\n",
      "auc: 0.4029\n",
      "topk: 2\n",
      "best_seen_acc: 0.7471\n",
      "best_unseen_acc: 0.8603\n",
      "best_harmonic_mean: 0.6872\n",
      "auc: 0.6011\n",
      "topk: 3\n",
      "best_seen_acc: 0.8084\n",
      "best_unseen_acc: 0.8880\n",
      "best_harmonic_mean: 0.7656\n",
      "auc: 0.6912\n",
      "\n",
      "\n",
      "\n",
      "======================================================================\n",
      "| test | template compdl | template_id 0 |\n",
      "train pairs: 1262 | valid pairs: 600 | test pairs: 800\n",
      "train images: 30338 | valid images: 10420 | test images: 12995\n",
      "seen_indices: 1844 | unseen_indices: 8576\n",
      "seen_indices: 2380 | unseen_indices: 10615\n",
      "attr \t| train (30338, 115) \t| valid (10420, 115) \t| test (12995, 115)\n",
      "obj \t| train (30338, 245) \t| valid (10420, 245) \t| test (12995, 245)\n",
      "image_dim:    360\n",
      "attr \t| train (30338, 115) \t| valid (10420, 115) \t| test (12995, 115)\n",
      "obj \t| train (30338, 245) \t| valid (10420, 245) \t| test (12995, 245)\n",
      "image_dim:    360\n",
      "attr \t| train (30338, 115) \t| valid (10420, 115) \t| test (12995, 115)\n",
      "obj \t| train (30338, 245) \t| valid (10420, 245) \t| test (12995, 245)\n",
      "image_dim:    360\n",
      "seen_indices: 2380 | unseen_indices: 10615\n",
      "Intervention features: torch.Size([12995, 360])\n",
      "features:  torch.Size([12995, 1962])\n",
      "topk: 1\n",
      "best_seen_acc: 0.5231\n",
      "best_unseen_acc: 0.6563\n",
      "best_harmonic_mean: 0.4762\n",
      "auc: 0.2987\n",
      "topk: 2\n",
      "best_seen_acc: 0.6655\n",
      "best_unseen_acc: 0.8100\n",
      "best_harmonic_mean: 0.6276\n",
      "auc: 0.4953\n",
      "topk: 3\n",
      "best_seen_acc: 0.7399\n",
      "best_unseen_acc: 0.8577\n",
      "best_harmonic_mean: 0.7071\n",
      "auc: 0.6005\n",
      "\n",
      "\n",
      "\n",
      "======================================================================\n",
      "| test | template compdl | template_id 1 |\n",
      "train pairs: 1262 | valid pairs: 600 | test pairs: 800\n",
      "train images: 30338 | valid images: 10420 | test images: 12995\n",
      "seen_indices: 1844 | unseen_indices: 8576\n",
      "seen_indices: 2380 | unseen_indices: 10615\n",
      "attr \t| train (30338, 115) \t| valid (10420, 115) \t| test (12995, 115)\n",
      "obj \t| train (30338, 245) \t| valid (10420, 245) \t| test (12995, 245)\n",
      "image_dim:    360\n",
      "attr \t| train (30338, 115) \t| valid (10420, 115) \t| test (12995, 115)\n",
      "obj \t| train (30338, 245) \t| valid (10420, 245) \t| test (12995, 245)\n",
      "image_dim:    360\n",
      "attr \t| train (30338, 115) \t| valid (10420, 115) \t| test (12995, 115)\n",
      "obj \t| train (30338, 245) \t| valid (10420, 245) \t| test (12995, 245)\n",
      "image_dim:    360\n",
      "seen_indices: 2380 | unseen_indices: 10615\n",
      "Intervention features: torch.Size([12995, 360])\n",
      "features:  torch.Size([12995, 1962])\n",
      "topk: 1\n",
      "best_seen_acc: 0.6059\n",
      "best_unseen_acc: 0.7095\n",
      "best_harmonic_mean: 0.5415\n",
      "auc: 0.3930\n",
      "topk: 2\n",
      "best_seen_acc: 0.7395\n",
      "best_unseen_acc: 0.8486\n",
      "best_harmonic_mean: 0.6987\n",
      "auc: 0.5929\n",
      "topk: 3\n",
      "best_seen_acc: 0.8130\n",
      "best_unseen_acc: 0.8833\n",
      "best_harmonic_mean: 0.7559\n",
      "auc: 0.6906\n",
      "\n",
      "\n",
      "\n",
      "======================================================================\n",
      "| test | template compdl | template_id 2 |\n",
      "train pairs: 1262 | valid pairs: 600 | test pairs: 800\n",
      "train images: 30338 | valid images: 10420 | test images: 12995\n",
      "seen_indices: 1844 | unseen_indices: 8576\n",
      "seen_indices: 2380 | unseen_indices: 10615\n",
      "attr \t| train (30338, 115) \t| valid (10420, 115) \t| test (12995, 115)\n",
      "obj \t| train (30338, 245) \t| valid (10420, 245) \t| test (12995, 245)\n",
      "image_dim:    360\n",
      "attr \t| train (30338, 115) \t| valid (10420, 115) \t| test (12995, 115)\n",
      "obj \t| train (30338, 245) \t| valid (10420, 245) \t| test (12995, 245)\n",
      "image_dim:    360\n",
      "attr \t| train (30338, 115) \t| valid (10420, 115) \t| test (12995, 115)\n",
      "obj \t| train (30338, 245) \t| valid (10420, 245) \t| test (12995, 245)\n",
      "image_dim:    360\n",
      "seen_indices: 2380 | unseen_indices: 10615\n",
      "Intervention features: torch.Size([12995, 360])\n",
      "features:  torch.Size([12995, 1962])\n",
      "topk: 1\n",
      "best_seen_acc: 0.5395\n",
      "best_unseen_acc: 0.6681\n",
      "best_harmonic_mean: 0.4873\n",
      "auc: 0.3224\n",
      "topk: 2\n",
      "best_seen_acc: 0.6706\n",
      "best_unseen_acc: 0.7899\n",
      "best_harmonic_mean: 0.6341\n",
      "auc: 0.4936\n",
      "topk: 3\n",
      "best_seen_acc: 0.7294\n",
      "best_unseen_acc: 0.8416\n",
      "best_harmonic_mean: 0.7065\n",
      "auc: 0.5818\n",
      "\n",
      "\n",
      "\n",
      "======================================================================\n",
      "| test | template compdl | template_id 3 |\n",
      "train pairs: 1262 | valid pairs: 600 | test pairs: 800\n",
      "train images: 30338 | valid images: 10420 | test images: 12995\n",
      "seen_indices: 1844 | unseen_indices: 8576\n",
      "seen_indices: 2380 | unseen_indices: 10615\n",
      "attr \t| train (30338, 115) \t| valid (10420, 115) \t| test (12995, 115)\n",
      "obj \t| train (30338, 245) \t| valid (10420, 245) \t| test (12995, 245)\n",
      "image_dim:    360\n",
      "attr \t| train (30338, 115) \t| valid (10420, 115) \t| test (12995, 115)\n",
      "obj \t| train (30338, 245) \t| valid (10420, 245) \t| test (12995, 245)\n",
      "image_dim:    360\n",
      "attr \t| train (30338, 115) \t| valid (10420, 115) \t| test (12995, 115)\n",
      "obj \t| train (30338, 245) \t| valid (10420, 245) \t| test (12995, 245)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "image_dim:    360\n",
      "seen_indices: 2380 | unseen_indices: 10615\n",
      "Intervention features: torch.Size([12995, 360])\n",
      "features:  torch.Size([12995, 1962])\n",
      "topk: 1\n",
      "best_seen_acc: 0.5332\n",
      "best_unseen_acc: 0.6640\n",
      "best_harmonic_mean: 0.4835\n",
      "auc: 0.3073\n",
      "topk: 2\n",
      "best_seen_acc: 0.6777\n",
      "best_unseen_acc: 0.8039\n",
      "best_harmonic_mean: 0.6296\n",
      "auc: 0.4983\n",
      "topk: 3\n",
      "best_seen_acc: 0.7298\n",
      "best_unseen_acc: 0.8500\n",
      "best_harmonic_mean: 0.7002\n",
      "auc: 0.5911\n",
      "\n",
      "\n",
      "\n",
      "======================================================================\n",
      "| test | template compdl | template_id 4 |\n",
      "train pairs: 1262 | valid pairs: 600 | test pairs: 800\n",
      "train images: 30338 | valid images: 10420 | test images: 12995\n",
      "seen_indices: 1844 | unseen_indices: 8576\n",
      "seen_indices: 2380 | unseen_indices: 10615\n",
      "attr \t| train (30338, 115) \t| valid (10420, 115) \t| test (12995, 115)\n",
      "obj \t| train (30338, 245) \t| valid (10420, 245) \t| test (12995, 245)\n",
      "image_dim:    360\n",
      "attr \t| train (30338, 115) \t| valid (10420, 115) \t| test (12995, 115)\n",
      "obj \t| train (30338, 245) \t| valid (10420, 245) \t| test (12995, 245)\n",
      "image_dim:    360\n",
      "attr \t| train (30338, 115) \t| valid (10420, 115) \t| test (12995, 115)\n",
      "obj \t| train (30338, 245) \t| valid (10420, 245) \t| test (12995, 245)\n",
      "image_dim:    360\n",
      "seen_indices: 2380 | unseen_indices: 10615\n",
      "Intervention features: torch.Size([12995, 360])\n",
      "features:  torch.Size([12995, 1962])\n",
      "topk: 1\n",
      "best_seen_acc: 0.5273\n",
      "best_unseen_acc: 0.6844\n",
      "best_harmonic_mean: 0.4923\n",
      "auc: 0.3214\n",
      "topk: 2\n",
      "best_seen_acc: 0.6693\n",
      "best_unseen_acc: 0.8167\n",
      "best_harmonic_mean: 0.6252\n",
      "auc: 0.5008\n",
      "topk: 3\n",
      "best_seen_acc: 0.7508\n",
      "best_unseen_acc: 0.8644\n",
      "best_harmonic_mean: 0.7019\n",
      "auc: 0.6080\n",
      "\n",
      "\n",
      "\n",
      "======================================================================\n",
      "| test | template compdl | template_id 5 |\n",
      "train pairs: 1262 | valid pairs: 600 | test pairs: 800\n",
      "train images: 30338 | valid images: 10420 | test images: 12995\n",
      "seen_indices: 1844 | unseen_indices: 8576\n",
      "seen_indices: 2380 | unseen_indices: 10615\n",
      "attr \t| train (30338, 115) \t| valid (10420, 115) \t| test (12995, 115)\n",
      "obj \t| train (30338, 245) \t| valid (10420, 245) \t| test (12995, 245)\n",
      "image_dim:    360\n",
      "attr \t| train (30338, 115) \t| valid (10420, 115) \t| test (12995, 115)\n",
      "obj \t| train (30338, 245) \t| valid (10420, 245) \t| test (12995, 245)\n",
      "image_dim:    360\n",
      "attr \t| train (30338, 115) \t| valid (10420, 115) \t| test (12995, 115)\n",
      "obj \t| train (30338, 245) \t| valid (10420, 245) \t| test (12995, 245)\n",
      "image_dim:    360\n",
      "seen_indices: 2380 | unseen_indices: 10615\n",
      "Intervention features: torch.Size([12995, 360])\n",
      "features:  torch.Size([12995, 1962])\n",
      "topk: 1\n",
      "best_seen_acc: 0.5483\n",
      "best_unseen_acc: 0.6643\n",
      "best_harmonic_mean: 0.5003\n",
      "auc: 0.3292\n",
      "topk: 2\n",
      "best_seen_acc: 0.6811\n",
      "best_unseen_acc: 0.7952\n",
      "best_harmonic_mean: 0.6434\n",
      "auc: 0.5061\n",
      "topk: 3\n",
      "best_seen_acc: 0.7542\n",
      "best_unseen_acc: 0.8672\n",
      "best_harmonic_mean: 0.7193\n",
      "auc: 0.6097\n",
      "\n",
      "\n",
      "\n",
      "======================================================================\n",
      "| test | template compdl | template_id 6 |\n",
      "train pairs: 1262 | valid pairs: 600 | test pairs: 800\n",
      "train images: 30338 | valid images: 10420 | test images: 12995\n",
      "seen_indices: 1844 | unseen_indices: 8576\n",
      "seen_indices: 2380 | unseen_indices: 10615\n",
      "attr \t| train (30338, 115) \t| valid (10420, 115) \t| test (12995, 115)\n",
      "obj \t| train (30338, 245) \t| valid (10420, 245) \t| test (12995, 245)\n",
      "image_dim:    360\n",
      "attr \t| train (30338, 115) \t| valid (10420, 115) \t| test (12995, 115)\n",
      "obj \t| train (30338, 245) \t| valid (10420, 245) \t| test (12995, 245)\n",
      "image_dim:    360\n",
      "attr \t| train (30338, 115) \t| valid (10420, 115) \t| test (12995, 115)\n",
      "obj \t| train (30338, 245) \t| valid (10420, 245) \t| test (12995, 245)\n",
      "image_dim:    360\n",
      "seen_indices: 2380 | unseen_indices: 10615\n",
      "Intervention features: torch.Size([12995, 360])\n",
      "features:  torch.Size([12995, 1962])\n",
      "topk: 1\n",
      "best_seen_acc: 0.5819\n",
      "best_unseen_acc: 0.7171\n",
      "best_harmonic_mean: 0.5326\n",
      "auc: 0.3749\n",
      "topk: 2\n",
      "best_seen_acc: 0.7160\n",
      "best_unseen_acc: 0.8338\n",
      "best_harmonic_mean: 0.6649\n",
      "auc: 0.5555\n",
      "topk: 3\n",
      "best_seen_acc: 0.7908\n",
      "best_unseen_acc: 0.8922\n",
      "best_harmonic_mean: 0.7433\n",
      "auc: 0.6707\n",
      "\n",
      "\n",
      "\n",
      "======================================================================\n",
      "| test | template compdl | template_id 7 |\n",
      "train pairs: 1262 | valid pairs: 600 | test pairs: 800\n",
      "train images: 30338 | valid images: 10420 | test images: 12995\n",
      "seen_indices: 1844 | unseen_indices: 8576\n",
      "seen_indices: 2380 | unseen_indices: 10615\n",
      "attr \t| train (30338, 115) \t| valid (10420, 115) \t| test (12995, 115)\n",
      "obj \t| train (30338, 245) \t| valid (10420, 245) \t| test (12995, 245)\n",
      "image_dim:    360\n",
      "attr \t| train (30338, 115) \t| valid (10420, 115) \t| test (12995, 115)\n",
      "obj \t| train (30338, 245) \t| valid (10420, 245) \t| test (12995, 245)\n",
      "image_dim:    360\n",
      "attr \t| train (30338, 115) \t| valid (10420, 115) \t| test (12995, 115)\n",
      "obj \t| train (30338, 245) \t| valid (10420, 245) \t| test (12995, 245)\n",
      "image_dim:    360\n",
      "seen_indices: 2380 | unseen_indices: 10615\n",
      "Intervention features: torch.Size([12995, 360])\n",
      "features:  torch.Size([12995, 1962])\n",
      "topk: 1\n",
      "best_seen_acc: 0.5525\n",
      "best_unseen_acc: 0.6531\n",
      "best_harmonic_mean: 0.4818\n",
      "auc: 0.3199\n",
      "topk: 2\n",
      "best_seen_acc: 0.6945\n",
      "best_unseen_acc: 0.7947\n",
      "best_harmonic_mean: 0.6337\n",
      "auc: 0.5121\n",
      "topk: 3\n",
      "best_seen_acc: 0.7735\n",
      "best_unseen_acc: 0.8576\n",
      "best_harmonic_mean: 0.7079\n",
      "auc: 0.6245\n",
      "\n",
      "\n",
      "\n",
      "======================================================================\n",
      "| test | template compdl | template_id 8 |\n",
      "train pairs: 1262 | valid pairs: 600 | test pairs: 800\n",
      "train images: 30338 | valid images: 10420 | test images: 12995\n",
      "seen_indices: 1844 | unseen_indices: 8576\n",
      "seen_indices: 2380 | unseen_indices: 10615\n",
      "attr \t| train (30338, 115) \t| valid (10420, 115) \t| test (12995, 115)\n",
      "obj \t| train (30338, 245) \t| valid (10420, 245) \t| test (12995, 245)\n",
      "image_dim:    360\n",
      "attr \t| train (30338, 115) \t| valid (10420, 115) \t| test (12995, 115)\n",
      "obj \t| train (30338, 245) \t| valid (10420, 245) \t| test (12995, 245)\n",
      "image_dim:    360\n",
      "attr \t| train (30338, 115) \t| valid (10420, 115) \t| test (12995, 115)\n",
      "obj \t| train (30338, 245) \t| valid (10420, 245) \t| test (12995, 245)\n",
      "image_dim:    360\n",
      "seen_indices: 2380 | unseen_indices: 10615\n",
      "Intervention features: torch.Size([12995, 360])\n",
      "features:  torch.Size([12995, 1962])\n",
      "topk: 1\n",
      "best_seen_acc: 0.4660\n",
      "best_unseen_acc: 0.6171\n",
      "best_harmonic_mean: 0.4293\n",
      "auc: 0.2457\n",
      "topk: 2\n",
      "best_seen_acc: 0.6517\n",
      "best_unseen_acc: 0.7918\n",
      "best_harmonic_mean: 0.5777\n",
      "auc: 0.4532\n",
      "topk: 3\n",
      "best_seen_acc: 0.7429\n",
      "best_unseen_acc: 0.8479\n",
      "best_harmonic_mean: 0.6729\n",
      "auc: 0.5647\n",
      "\n",
      "\n",
      "\n",
      "======================================================================\n",
      "| test | template compdl | template_id 9 |\n",
      "train pairs: 1262 | valid pairs: 600 | test pairs: 800\n",
      "train images: 30338 | valid images: 10420 | test images: 12995\n",
      "seen_indices: 1844 | unseen_indices: 8576\n",
      "seen_indices: 2380 | unseen_indices: 10615\n",
      "attr \t| train (30338, 115) \t| valid (10420, 115) \t| test (12995, 115)\n",
      "obj \t| train (30338, 245) \t| valid (10420, 245) \t| test (12995, 245)\n",
      "image_dim:    360\n",
      "attr \t| train (30338, 115) \t| valid (10420, 115) \t| test (12995, 115)\n",
      "obj \t| train (30338, 245) \t| valid (10420, 245) \t| test (12995, 245)\n",
      "image_dim:    360\n",
      "attr \t| train (30338, 115) \t| valid (10420, 115) \t| test (12995, 115)\n",
      "obj \t| train (30338, 245) \t| valid (10420, 245) \t| test (12995, 245)\n",
      "image_dim:    360\n",
      "seen_indices: 2380 | unseen_indices: 10615\n",
      "Intervention features: torch.Size([12995, 360])\n",
      "features:  torch.Size([12995, 1962])\n",
      "topk: 1\n",
      "best_seen_acc: 0.5361\n",
      "best_unseen_acc: 0.6737\n",
      "best_harmonic_mean: 0.4809\n",
      "auc: 0.3194\n",
      "topk: 2\n",
      "best_seen_acc: 0.6803\n",
      "best_unseen_acc: 0.7959\n",
      "best_harmonic_mean: 0.6349\n",
      "auc: 0.5005\n",
      "topk: 3\n",
      "best_seen_acc: 0.7513\n",
      "best_unseen_acc: 0.8476\n",
      "best_harmonic_mean: 0.7064\n",
      "auc: 0.6016\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Interpretability for Interv(GT)\n",
    "\n",
    "class parseArguments:\n",
    "    def __init__(self):\n",
    "        self.split = \"valid\"\n",
    "        self.is_open_world = False\n",
    "        \n",
    "        self.feature = \"primitive\"\n",
    "        self.model = \"clip\"\n",
    "        self.train_warmup = 0\n",
    "        self.is_image_projection = True\n",
    "        self.is_bias = False\n",
    "        self.is_limit = True\n",
    "        \n",
    "        self.data_root = \"../../data/mit_states\"\n",
    "        self.emb_root = \"../../data\"\n",
    "        self.precomputed_data_dir = f\"../../outputs/mit_states/clip_prompt_precompute_features\"\n",
    "        \n",
    "        self.emb_init = True\n",
    "        self.input_dim = 600\n",
    "        self.num_epochs = 400\n",
    "        self.batch_size = 128\n",
    "        self.learning_rate = 5e-5\n",
    "        self.weight_decay = 5e-5\n",
    "        self.logit_scale = 0.07\n",
    "\n",
    "args = parseArguments()\n",
    "\n",
    "# Start!!\n",
    "template_count_dict = {\"clip\": 7, \"compdl\": 10}\n",
    "\n",
    "for split in [\"valid\", \"test\"]:\n",
    "    args.split = split\n",
    "    for template_src, template_count in template_count_dict.items():\n",
    "        for template_id in range(template_count):\n",
    "            print(\"=\" * 70)\n",
    "            print(f\"| {args.split} | template {template_src} | template_id {template_id} |\")\n",
    "            args.precomputed_data_root = os.path.join(args.precomputed_data_dir, template_src, f\"template_{template_id}\")\n",
    "\n",
    "            # Load dataset\n",
    "            data = MITStatesDataset(root=args.data_root, split=\"train\")  # split can be ignored here\n",
    "            seen_ids_valid, unseen_ids_valid = get_seen_unseen_indices(\"valid\", data)\n",
    "            seen_ids_test, unseen_ids_test = get_seen_unseen_indices(\"test\", data)\n",
    "\n",
    "            train_dataset = Precomputed_MITStatesDataset(split=\"train\", feature=\"primitive\", data=data, args=args, is_limit=True)\n",
    "            valid_dataset = Precomputed_MITStatesDataset(split=\"valid\", feature=\"primitive\", data=data, args=args, is_limit=True)\n",
    "            test_dataset = Precomputed_MITStatesDataset(split=\"test\", feature=\"primitive\", data=data, args=args, is_limit=True)\n",
    "            seen_mask = train_dataset.seen_mask\n",
    "\n",
    "            # Load model\n",
    "            model = load_model(template_src, template_id)\n",
    "\n",
    "            # Prepare groundtruth labels\n",
    "            seen_ids, unseen_ids = get_seen_unseen_indices(args.split, data)\n",
    "\n",
    "            if args.split == \"valid\":\n",
    "                labels_attr = [sample[\"attr_id\"] for sample in data.valid_data]\n",
    "                labels_obj = [sample[\"obj_id\"] for sample in data.valid_data]\n",
    "                labels = [sample[\"pair_id\"] for sample in data.valid_data]\n",
    "                gt_features_attr = np.zeros((valid_dataset.image_embs.shape[0], 115))\n",
    "                gt_features_obj = np.zeros((valid_dataset.image_embs.shape[0], 245))\n",
    "            elif args.split == \"test\":\n",
    "                labels_attr = [sample[\"attr_id\"] for sample in data.test_data]\n",
    "                labels_obj = [sample[\"obj_id\"] for sample in data.test_data]\n",
    "                labels = [sample[\"pair_id\"] for sample in data.test_data]\n",
    "                gt_features_attr = np.zeros((test_dataset.image_embs.shape[0], 115))\n",
    "                gt_features_obj = np.zeros((test_dataset.image_embs.shape[0], 245))\n",
    "\n",
    "            gt_features_attr[np.arange(len(labels_attr)), labels_attr] = 1\n",
    "            gt_features_obj[np.arange(len(labels_obj)), labels_obj] = 1\n",
    "            gt_features_concat = np.concatenate([gt_features_attr, gt_features_obj], axis=-1)\n",
    "            gt_features_concat = torch.tensor(gt_features_concat).to(device).double()\n",
    "            print(f\"Intervention features: {gt_features_concat.shape}\")\n",
    "\n",
    "            # Intervene!!!\n",
    "            with torch.no_grad():\n",
    "                logits, loss = model(gt_features_concat, pair_labels=None, is_train=False, is_intervene=True)\n",
    "            features = logits.softmax(dim=-1).log()\n",
    "            print(\"features: \", features.shape)\n",
    "\n",
    "            # Compute metrics\n",
    "            overall_metrics = get_overall_metrics(features, labels, seen_ids, unseen_ids, seen_mask, data, topk_list=[1,2,3])\n",
    "            output_path = get_output_path(template_src, template_id, args)\n",
    "            with open(os.path.join(output_path, f\"{args.split}_metrics_interv_gt.json\"), \"w\") as f:\n",
    "                json.dump(overall_metrics, f)\n",
    "            print(\"\\n\\n\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "f5330762",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "template_src: clip\n",
      "split: valid\n",
      "| topk 1 | auc | mean: 39.267366 | std: 3.926191\n",
      "| topk 2 | auc | mean: 58.646386 | std: 3.537431\n",
      "| topk 3 | auc | mean: 68.550824 | std: 3.394782\n",
      "\n",
      "split: test\n",
      "| topk 1 | auc | mean: 39.861835 | std: 3.008578\n",
      "| topk 1 | best_seen_acc | mean: 59.237695 | std: 2.428830\n",
      "| topk 1 | best_unseen_acc | mean: 72.666712 | std: 2.387179\n",
      "| topk 1 | best_harmonic_mean | mean: 55.703362 | std: 2.639456\n",
      "| topk 2 | auc | mean: 58.146802 | std: 3.129845\n",
      "| topk 3 | auc | mean: 68.448382 | std: 2.582705\n",
      "\n",
      "template_src: compdl\n",
      "split: valid\n",
      "| topk 1 | auc | mean: 33.711541 | std: 3.976065\n",
      "| topk 2 | auc | mean: 52.148164 | std: 4.991058\n",
      "| topk 3 | auc | mean: 64.094234 | std: 5.566963\n",
      "\n",
      "split: test\n",
      "| topk 1 | auc | mean: 32.318739 | std: 3.805140\n",
      "| topk 1 | best_seen_acc | mean: 54.138655 | std: 3.506340\n",
      "| topk 1 | best_unseen_acc | mean: 67.075836 | std: 2.708888\n",
      "| topk 1 | best_harmonic_mean | mean: 49.055195 | std: 2.941528\n",
      "| topk 2 | auc | mean: 51.082362 | std: 3.604010\n",
      "| topk 3 | auc | mean: 61.434029 | std: 3.683906\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Aggregate interpretability results for Interv(GT)\n",
    "# Usefulness\n",
    "template_count_dict = {\"clip\": 7, \"compdl\": 10}\n",
    "\n",
    "for template_src, template_count in template_count_dict.items():\n",
    "    # For each possible template source, intialize a new statistics dict\n",
    "    all_statistics_dict = {\n",
    "        \"valid\": {\n",
    "            \"1\": {\"auc\": []},\n",
    "            \"2\": {\"auc\": []},\n",
    "            \"3\": {\"auc\": []},\n",
    "        },\n",
    "        \"test\": {\n",
    "            \"1\": {\n",
    "                \"auc\": [],\n",
    "                \"best_seen_acc\": [],\n",
    "                \"best_unseen_acc\": [],\n",
    "                \"best_harmonic_mean\": [],\n",
    "            },\n",
    "            \"2\": {\"auc\": []},\n",
    "            \"3\": {\"auc\": []},\n",
    "        },\n",
    "    }\n",
    "\n",
    "    # Sum all the required statistics\n",
    "    for template_id in range(template_count):\n",
    "        exp_name = \"clip_prompt_retrieval_model_TP{}_TID{}_open0_init1_imgproj1_bias0_{}_Lim1_N400_TW0_B128_LR5e-05_WD5e-05_L0.07\".format(\n",
    "            template_src,\n",
    "            template_id,\n",
    "            args.feature,\n",
    "        )\n",
    "        output_dir = get_output_path(template_src, template_id, args)\n",
    "        \n",
    "        # Valid metrics\n",
    "        with open(os.path.join(output_dir, \"valid_metrics_interv_gt.json\"), \"rb\") as f:\n",
    "            data = json.load(f)\n",
    "        all_statistics_dict[\"valid\"][\"1\"][\"auc\"].append(data[\"1\"][\"auc\"])\n",
    "        all_statistics_dict[\"valid\"][\"2\"][\"auc\"].append(data[\"2\"][\"auc\"])\n",
    "        all_statistics_dict[\"valid\"][\"3\"][\"auc\"].append(data[\"3\"][\"auc\"])\n",
    "        \n",
    "        # Test metrics\n",
    "        with open(os.path.join(output_dir, \"test_metrics_interv_gt.json\"), \"rb\") as f:\n",
    "            data = json.load(f)\n",
    "        all_statistics_dict[\"test\"][\"1\"][\"auc\"].append(data[\"1\"][\"auc\"])\n",
    "        all_statistics_dict[\"test\"][\"2\"][\"auc\"].append(data[\"2\"][\"auc\"])\n",
    "        all_statistics_dict[\"test\"][\"3\"][\"auc\"].append(data[\"3\"][\"auc\"])\n",
    "        all_statistics_dict[\"test\"][\"1\"][\"best_seen_acc\"].append(data[\"1\"][\"best_seen_acc\"])\n",
    "        all_statistics_dict[\"test\"][\"1\"][\"best_unseen_acc\"].append(data[\"1\"][\"best_unseen_acc\"])\n",
    "        all_statistics_dict[\"test\"][\"1\"][\"best_harmonic_mean\"].append(data[\"1\"][\"best_harmonic_mean\"])\n",
    "        \n",
    "    # Take the mean of the required statistics and output\n",
    "    print(f\"template_src: {template_src}\")\n",
    "    print(\"split: valid\")\n",
    "    for topk in all_statistics_dict[\"valid\"].keys():\n",
    "        for k, v in all_statistics_dict[\"valid\"][topk].items():\n",
    "            mean = 100 * np.mean(all_statistics_dict[\"valid\"][topk][k])\n",
    "            std = 100 * np.std(all_statistics_dict[\"valid\"][topk][k])\n",
    "            print(f\"| topk {topk} | {k} | mean: {mean:8.6f} | std: {std:8.6f}\")\n",
    "    print()\n",
    "    \n",
    "    print(\"split: test\")\n",
    "    for topk in all_statistics_dict[\"test\"].keys():\n",
    "        for k, v in all_statistics_dict[\"test\"][topk].items():\n",
    "            mean = 100 * np.mean(all_statistics_dict[\"test\"][topk][k])\n",
    "            std = 100 * np.std(all_statistics_dict[\"test\"][topk][k])\n",
    "            print(f\"| topk {topk} | {k} | mean: {mean:8.6f} | std: {std:8.6f}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61255688",
   "metadata": {},
   "source": [
    "## 4.3. Interpretability for Interv(GTX)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "c7d2f68c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "| valid | template clip | template_id 0 |\n",
      "train pairs: 1262 | valid pairs: 600 | test pairs: 800\n",
      "train images: 30338 | valid images: 10420 | test images: 12995\n",
      "seen_indices: 1844 | unseen_indices: 8576\n",
      "seen_indices: 2380 | unseen_indices: 10615\n",
      "attr \t| train (30338, 115) \t| valid (10420, 115) \t| test (12995, 115)\n",
      "obj \t| train (30338, 245) \t| valid (10420, 245) \t| test (12995, 245)\n",
      "image_dim:    360\n",
      "attr \t| train (30338, 115) \t| valid (10420, 115) \t| test (12995, 115)\n",
      "obj \t| train (30338, 245) \t| valid (10420, 245) \t| test (12995, 245)\n",
      "image_dim:    360\n",
      "attr \t| train (30338, 115) \t| valid (10420, 115) \t| test (12995, 115)\n",
      "obj \t| train (30338, 245) \t| valid (10420, 245) \t| test (12995, 245)\n",
      "image_dim:    360\n",
      "seen_indices: 1844 | unseen_indices: 8576\n",
      "Intervention features: torch.Size([10420, 360])\n",
      "seen_indices: 1844 | unseen_indices: 8576\n",
      "Intervention features: torch.Size([10420, 360])\n",
      "features:  torch.Size([10420, 1962])\n",
      "topk: 1\n",
      "best_seen_acc: 0.7240\n",
      "best_unseen_acc: 0.7240\n",
      "best_harmonic_mean: 0.6233\n",
      "auc: 0.4921\n",
      "topk: 2\n",
      "best_seen_acc: 0.8536\n",
      "best_unseen_acc: 0.8650\n",
      "best_harmonic_mean: 0.7818\n",
      "auc: 0.7050\n",
      "topk: 3\n",
      "best_seen_acc: 0.8948\n",
      "best_unseen_acc: 0.9029\n",
      "best_harmonic_mean: 0.8342\n",
      "auc: 0.7880\n",
      "\n",
      "\n",
      "\n",
      "======================================================================\n",
      "| valid | template clip | template_id 1 |\n",
      "train pairs: 1262 | valid pairs: 600 | test pairs: 800\n",
      "train images: 30338 | valid images: 10420 | test images: 12995\n",
      "seen_indices: 1844 | unseen_indices: 8576\n",
      "seen_indices: 2380 | unseen_indices: 10615\n",
      "attr \t| train (30338, 115) \t| valid (10420, 115) \t| test (12995, 115)\n",
      "obj \t| train (30338, 245) \t| valid (10420, 245) \t| test (12995, 245)\n",
      "image_dim:    360\n",
      "attr \t| train (30338, 115) \t| valid (10420, 115) \t| test (12995, 115)\n",
      "obj \t| train (30338, 245) \t| valid (10420, 245) \t| test (12995, 245)\n",
      "image_dim:    360\n",
      "attr \t| train (30338, 115) \t| valid (10420, 115) \t| test (12995, 115)\n",
      "obj \t| train (30338, 245) \t| valid (10420, 245) \t| test (12995, 245)\n",
      "image_dim:    360\n",
      "seen_indices: 1844 | unseen_indices: 8576\n",
      "Intervention features: torch.Size([10420, 360])\n",
      "seen_indices: 1844 | unseen_indices: 8576\n",
      "Intervention features: torch.Size([10420, 360])\n",
      "features:  torch.Size([10420, 1962])\n",
      "topk: 1\n",
      "best_seen_acc: 0.6649\n",
      "best_unseen_acc: 0.6947\n",
      "best_harmonic_mean: 0.5834\n",
      "auc: 0.4310\n",
      "topk: 2\n",
      "best_seen_acc: 0.7993\n",
      "best_unseen_acc: 0.8273\n",
      "best_harmonic_mean: 0.7174\n",
      "auc: 0.6197\n",
      "topk: 3\n",
      "best_seen_acc: 0.8601\n",
      "best_unseen_acc: 0.8745\n",
      "best_harmonic_mean: 0.7809\n",
      "auc: 0.7015\n",
      "\n",
      "\n",
      "\n",
      "======================================================================\n",
      "| valid | template clip | template_id 2 |\n",
      "train pairs: 1262 | valid pairs: 600 | test pairs: 800\n",
      "train images: 30338 | valid images: 10420 | test images: 12995\n",
      "seen_indices: 1844 | unseen_indices: 8576\n",
      "seen_indices: 2380 | unseen_indices: 10615\n",
      "attr \t| train (30338, 115) \t| valid (10420, 115) \t| test (12995, 115)\n",
      "obj \t| train (30338, 245) \t| valid (10420, 245) \t| test (12995, 245)\n",
      "image_dim:    360\n",
      "attr \t| train (30338, 115) \t| valid (10420, 115) \t| test (12995, 115)\n",
      "obj \t| train (30338, 245) \t| valid (10420, 245) \t| test (12995, 245)\n",
      "image_dim:    360\n",
      "attr \t| train (30338, 115) \t| valid (10420, 115) \t| test (12995, 115)\n",
      "obj \t| train (30338, 245) \t| valid (10420, 245) \t| test (12995, 245)\n",
      "image_dim:    360\n",
      "seen_indices: 1844 | unseen_indices: 8576\n",
      "Intervention features: torch.Size([10420, 360])\n",
      "seen_indices: 1844 | unseen_indices: 8576\n",
      "Intervention features: torch.Size([10420, 360])\n",
      "features:  torch.Size([10420, 1962])\n",
      "topk: 1\n",
      "best_seen_acc: 0.5884\n",
      "best_unseen_acc: 0.6677\n",
      "best_harmonic_mean: 0.5529\n",
      "auc: 0.3694\n",
      "topk: 2\n",
      "best_seen_acc: 0.7489\n",
      "best_unseen_acc: 0.8158\n",
      "best_harmonic_mean: 0.6996\n",
      "auc: 0.5782\n",
      "topk: 3\n",
      "best_seen_acc: 0.8004\n",
      "best_unseen_acc: 0.8647\n",
      "best_harmonic_mean: 0.7489\n",
      "auc: 0.6595\n",
      "\n",
      "\n",
      "\n",
      "======================================================================\n",
      "| valid | template clip | template_id 3 |\n",
      "train pairs: 1262 | valid pairs: 600 | test pairs: 800\n",
      "train images: 30338 | valid images: 10420 | test images: 12995\n",
      "seen_indices: 1844 | unseen_indices: 8576\n",
      "seen_indices: 2380 | unseen_indices: 10615\n",
      "attr \t| train (30338, 115) \t| valid (10420, 115) \t| test (12995, 115)\n",
      "obj \t| train (30338, 245) \t| valid (10420, 245) \t| test (12995, 245)\n",
      "image_dim:    360\n",
      "attr \t| train (30338, 115) \t| valid (10420, 115) \t| test (12995, 115)\n",
      "obj \t| train (30338, 245) \t| valid (10420, 245) \t| test (12995, 245)\n",
      "image_dim:    360\n",
      "attr \t| train (30338, 115) \t| valid (10420, 115) \t| test (12995, 115)\n",
      "obj \t| train (30338, 245) \t| valid (10420, 245) \t| test (12995, 245)\n",
      "image_dim:    360\n",
      "seen_indices: 1844 | unseen_indices: 8576\n",
      "Intervention features: torch.Size([10420, 360])\n",
      "seen_indices: 1844 | unseen_indices: 8576\n",
      "Intervention features: torch.Size([10420, 360])\n",
      "features:  torch.Size([10420, 1962])\n",
      "topk: 1\n",
      "best_seen_acc: 0.7007\n",
      "best_unseen_acc: 0.7262\n",
      "best_harmonic_mean: 0.6083\n",
      "auc: 0.4716\n",
      "topk: 2\n",
      "best_seen_acc: 0.8411\n",
      "best_unseen_acc: 0.8615\n",
      "best_harmonic_mean: 0.7424\n",
      "auc: 0.6837\n",
      "topk: 3\n",
      "best_seen_acc: 0.8818\n",
      "best_unseen_acc: 0.9124\n",
      "best_harmonic_mean: 0.8140\n",
      "auc: 0.7725\n",
      "\n",
      "\n",
      "\n",
      "======================================================================\n",
      "| valid | template clip | template_id 4 |\n",
      "train pairs: 1262 | valid pairs: 600 | test pairs: 800\n",
      "train images: 30338 | valid images: 10420 | test images: 12995\n",
      "seen_indices: 1844 | unseen_indices: 8576\n",
      "seen_indices: 2380 | unseen_indices: 10615\n",
      "attr \t| train (30338, 115) \t| valid (10420, 115) \t| test (12995, 115)\n",
      "obj \t| train (30338, 245) \t| valid (10420, 245) \t| test (12995, 245)\n",
      "image_dim:    360\n",
      "attr \t| train (30338, 115) \t| valid (10420, 115) \t| test (12995, 115)\n",
      "obj \t| train (30338, 245) \t| valid (10420, 245) \t| test (12995, 245)\n",
      "image_dim:    360\n",
      "attr \t| train (30338, 115) \t| valid (10420, 115) \t| test (12995, 115)\n",
      "obj \t| train (30338, 245) \t| valid (10420, 245) \t| test (12995, 245)\n",
      "image_dim:    360\n",
      "seen_indices: 1844 | unseen_indices: 8576\n",
      "Intervention features: torch.Size([10420, 360])\n",
      "seen_indices: 1844 | unseen_indices: 8576\n",
      "Intervention features: torch.Size([10420, 360])\n",
      "features:  torch.Size([10420, 1962])\n",
      "topk: 1\n",
      "best_seen_acc: 0.6193\n",
      "best_unseen_acc: 0.6848\n",
      "best_harmonic_mean: 0.5456\n",
      "auc: 0.3866\n",
      "topk: 2\n",
      "best_seen_acc: 0.7744\n",
      "best_unseen_acc: 0.8258\n",
      "best_harmonic_mean: 0.6826\n",
      "auc: 0.5911\n",
      "topk: 3\n",
      "best_seen_acc: 0.8623\n",
      "best_unseen_acc: 0.8801\n",
      "best_harmonic_mean: 0.7608\n",
      "auc: 0.7113\n",
      "\n",
      "\n",
      "\n",
      "======================================================================\n",
      "| valid | template clip | template_id 5 |\n",
      "train pairs: 1262 | valid pairs: 600 | test pairs: 800\n",
      "train images: 30338 | valid images: 10420 | test images: 12995\n",
      "seen_indices: 1844 | unseen_indices: 8576\n",
      "seen_indices: 2380 | unseen_indices: 10615\n",
      "attr \t| train (30338, 115) \t| valid (10420, 115) \t| test (12995, 115)\n",
      "obj \t| train (30338, 245) \t| valid (10420, 245) \t| test (12995, 245)\n",
      "image_dim:    360\n",
      "attr \t| train (30338, 115) \t| valid (10420, 115) \t| test (12995, 115)\n",
      "obj \t| train (30338, 245) \t| valid (10420, 245) \t| test (12995, 245)\n",
      "image_dim:    360\n",
      "attr \t| train (30338, 115) \t| valid (10420, 115) \t| test (12995, 115)\n",
      "obj \t| train (30338, 245) \t| valid (10420, 245) \t| test (12995, 245)\n",
      "image_dim:    360\n",
      "seen_indices: 1844 | unseen_indices: 8576\n",
      "Intervention features: torch.Size([10420, 360])\n",
      "seen_indices: 1844 | unseen_indices: 8576\n",
      "Intervention features: torch.Size([10420, 360])\n",
      "features:  torch.Size([10420, 1962])\n",
      "topk: 1\n",
      "best_seen_acc: 0.6931\n",
      "best_unseen_acc: 0.7308\n",
      "best_harmonic_mean: 0.6176\n",
      "auc: 0.4737\n",
      "topk: 2\n",
      "best_seen_acc: 0.8308\n",
      "best_unseen_acc: 0.8246\n",
      "best_harmonic_mean: 0.7408\n",
      "auc: 0.6545\n",
      "topk: 3\n",
      "best_seen_acc: 0.8894\n",
      "best_unseen_acc: 0.8755\n",
      "best_harmonic_mean: 0.8036\n",
      "auc: 0.7563\n",
      "\n",
      "\n",
      "\n",
      "======================================================================\n",
      "| valid | template clip | template_id 6 |\n",
      "train pairs: 1262 | valid pairs: 600 | test pairs: 800\n",
      "train images: 30338 | valid images: 10420 | test images: 12995\n",
      "seen_indices: 1844 | unseen_indices: 8576\n",
      "seen_indices: 2380 | unseen_indices: 10615\n",
      "attr \t| train (30338, 115) \t| valid (10420, 115) \t| test (12995, 115)\n",
      "obj \t| train (30338, 245) \t| valid (10420, 245) \t| test (12995, 245)\n",
      "image_dim:    360\n",
      "attr \t| train (30338, 115) \t| valid (10420, 115) \t| test (12995, 115)\n",
      "obj \t| train (30338, 245) \t| valid (10420, 245) \t| test (12995, 245)\n",
      "image_dim:    360\n",
      "attr \t| train (30338, 115) \t| valid (10420, 115) \t| test (12995, 115)\n",
      "obj \t| train (30338, 245) \t| valid (10420, 245) \t| test (12995, 245)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "image_dim:    360\n",
      "seen_indices: 1844 | unseen_indices: 8576\n",
      "Intervention features: torch.Size([10420, 360])\n",
      "seen_indices: 1844 | unseen_indices: 8576\n",
      "Intervention features: torch.Size([10420, 360])\n",
      "features:  torch.Size([10420, 1962])\n",
      "topk: 1\n",
      "best_seen_acc: 0.6535\n",
      "best_unseen_acc: 0.7062\n",
      "best_harmonic_mean: 0.5891\n",
      "auc: 0.4288\n",
      "topk: 2\n",
      "best_seen_acc: 0.8113\n",
      "best_unseen_acc: 0.8455\n",
      "best_harmonic_mean: 0.7176\n",
      "auc: 0.6464\n",
      "topk: 3\n",
      "best_seen_acc: 0.8633\n",
      "best_unseen_acc: 0.8904\n",
      "best_harmonic_mean: 0.7954\n",
      "auc: 0.7418\n",
      "\n",
      "\n",
      "\n",
      "======================================================================\n",
      "| valid | template compdl | template_id 0 |\n",
      "train pairs: 1262 | valid pairs: 600 | test pairs: 800\n",
      "train images: 30338 | valid images: 10420 | test images: 12995\n",
      "seen_indices: 1844 | unseen_indices: 8576\n",
      "seen_indices: 2380 | unseen_indices: 10615\n",
      "attr \t| train (30338, 115) \t| valid (10420, 115) \t| test (12995, 115)\n",
      "obj \t| train (30338, 245) \t| valid (10420, 245) \t| test (12995, 245)\n",
      "image_dim:    360\n",
      "attr \t| train (30338, 115) \t| valid (10420, 115) \t| test (12995, 115)\n",
      "obj \t| train (30338, 245) \t| valid (10420, 245) \t| test (12995, 245)\n",
      "image_dim:    360\n",
      "attr \t| train (30338, 115) \t| valid (10420, 115) \t| test (12995, 115)\n",
      "obj \t| train (30338, 245) \t| valid (10420, 245) \t| test (12995, 245)\n",
      "image_dim:    360\n",
      "seen_indices: 1844 | unseen_indices: 8576\n",
      "Intervention features: torch.Size([10420, 360])\n",
      "seen_indices: 1844 | unseen_indices: 8576\n",
      "Intervention features: torch.Size([10420, 360])\n",
      "features:  torch.Size([10420, 1962])\n",
      "topk: 1\n",
      "best_seen_acc: 0.5954\n",
      "best_unseen_acc: 0.6566\n",
      "best_harmonic_mean: 0.5211\n",
      "auc: 0.3581\n",
      "topk: 2\n",
      "best_seen_acc: 0.7191\n",
      "best_unseen_acc: 0.8010\n",
      "best_harmonic_mean: 0.6524\n",
      "auc: 0.5348\n",
      "topk: 3\n",
      "best_seen_acc: 0.7842\n",
      "best_unseen_acc: 0.8419\n",
      "best_harmonic_mean: 0.7193\n",
      "auc: 0.6102\n",
      "\n",
      "\n",
      "\n",
      "======================================================================\n",
      "| valid | template compdl | template_id 1 |\n",
      "train pairs: 1262 | valid pairs: 600 | test pairs: 800\n",
      "train images: 30338 | valid images: 10420 | test images: 12995\n",
      "seen_indices: 1844 | unseen_indices: 8576\n",
      "seen_indices: 2380 | unseen_indices: 10615\n",
      "attr \t| train (30338, 115) \t| valid (10420, 115) \t| test (12995, 115)\n",
      "obj \t| train (30338, 245) \t| valid (10420, 245) \t| test (12995, 245)\n",
      "image_dim:    360\n",
      "attr \t| train (30338, 115) \t| valid (10420, 115) \t| test (12995, 115)\n",
      "obj \t| train (30338, 245) \t| valid (10420, 245) \t| test (12995, 245)\n",
      "image_dim:    360\n",
      "attr \t| train (30338, 115) \t| valid (10420, 115) \t| test (12995, 115)\n",
      "obj \t| train (30338, 245) \t| valid (10420, 245) \t| test (12995, 245)\n",
      "image_dim:    360\n",
      "seen_indices: 1844 | unseen_indices: 8576\n",
      "Intervention features: torch.Size([10420, 360])\n",
      "seen_indices: 1844 | unseen_indices: 8576\n",
      "Intervention features: torch.Size([10420, 360])\n",
      "features:  torch.Size([10420, 1962])\n",
      "topk: 1\n",
      "best_seen_acc: 0.6513\n",
      "best_unseen_acc: 0.7366\n",
      "best_harmonic_mean: 0.5872\n",
      "auc: 0.4409\n",
      "topk: 2\n",
      "best_seen_acc: 0.7950\n",
      "best_unseen_acc: 0.8598\n",
      "best_harmonic_mean: 0.7377\n",
      "auc: 0.6493\n",
      "topk: 3\n",
      "best_seen_acc: 0.8818\n",
      "best_unseen_acc: 0.9228\n",
      "best_harmonic_mean: 0.8011\n",
      "auc: 0.7752\n",
      "\n",
      "\n",
      "\n",
      "======================================================================\n",
      "| valid | template compdl | template_id 2 |\n",
      "train pairs: 1262 | valid pairs: 600 | test pairs: 800\n",
      "train images: 30338 | valid images: 10420 | test images: 12995\n",
      "seen_indices: 1844 | unseen_indices: 8576\n",
      "seen_indices: 2380 | unseen_indices: 10615\n",
      "attr \t| train (30338, 115) \t| valid (10420, 115) \t| test (12995, 115)\n",
      "obj \t| train (30338, 245) \t| valid (10420, 245) \t| test (12995, 245)\n",
      "image_dim:    360\n",
      "attr \t| train (30338, 115) \t| valid (10420, 115) \t| test (12995, 115)\n",
      "obj \t| train (30338, 245) \t| valid (10420, 245) \t| test (12995, 245)\n",
      "image_dim:    360\n",
      "attr \t| train (30338, 115) \t| valid (10420, 115) \t| test (12995, 115)\n",
      "obj \t| train (30338, 245) \t| valid (10420, 245) \t| test (12995, 245)\n",
      "image_dim:    360\n",
      "seen_indices: 1844 | unseen_indices: 8576\n",
      "Intervention features: torch.Size([10420, 360])\n",
      "seen_indices: 1844 | unseen_indices: 8576\n",
      "Intervention features: torch.Size([10420, 360])\n",
      "features:  torch.Size([10420, 1962])\n",
      "topk: 1\n",
      "best_seen_acc: 0.6231\n",
      "best_unseen_acc: 0.6550\n",
      "best_harmonic_mean: 0.5431\n",
      "auc: 0.3779\n",
      "topk: 2\n",
      "best_seen_acc: 0.7538\n",
      "best_unseen_acc: 0.8228\n",
      "best_harmonic_mean: 0.6831\n",
      "auc: 0.5746\n",
      "topk: 3\n",
      "best_seen_acc: 0.8194\n",
      "best_unseen_acc: 0.8840\n",
      "best_harmonic_mean: 0.7732\n",
      "auc: 0.6895\n",
      "\n",
      "\n",
      "\n",
      "======================================================================\n",
      "| valid | template compdl | template_id 3 |\n",
      "train pairs: 1262 | valid pairs: 600 | test pairs: 800\n",
      "train images: 30338 | valid images: 10420 | test images: 12995\n",
      "seen_indices: 1844 | unseen_indices: 8576\n",
      "seen_indices: 2380 | unseen_indices: 10615\n",
      "attr \t| train (30338, 115) \t| valid (10420, 115) \t| test (12995, 115)\n",
      "obj \t| train (30338, 245) \t| valid (10420, 245) \t| test (12995, 245)\n",
      "image_dim:    360\n",
      "attr \t| train (30338, 115) \t| valid (10420, 115) \t| test (12995, 115)\n",
      "obj \t| train (30338, 245) \t| valid (10420, 245) \t| test (12995, 245)\n",
      "image_dim:    360\n",
      "attr \t| train (30338, 115) \t| valid (10420, 115) \t| test (12995, 115)\n",
      "obj \t| train (30338, 245) \t| valid (10420, 245) \t| test (12995, 245)\n",
      "image_dim:    360\n",
      "seen_indices: 1844 | unseen_indices: 8576\n",
      "Intervention features: torch.Size([10420, 360])\n",
      "seen_indices: 1844 | unseen_indices: 8576\n",
      "Intervention features: torch.Size([10420, 360])\n",
      "features:  torch.Size([10420, 1962])\n",
      "topk: 1\n",
      "best_seen_acc: 0.5743\n",
      "best_unseen_acc: 0.6431\n",
      "best_harmonic_mean: 0.5037\n",
      "auc: 0.3378\n",
      "topk: 2\n",
      "best_seen_acc: 0.7148\n",
      "best_unseen_acc: 0.7744\n",
      "best_harmonic_mean: 0.6516\n",
      "auc: 0.5132\n",
      "topk: 3\n",
      "best_seen_acc: 0.8156\n",
      "best_unseen_acc: 0.8405\n",
      "best_harmonic_mean: 0.7314\n",
      "auc: 0.6438\n",
      "\n",
      "\n",
      "\n",
      "======================================================================\n",
      "| valid | template compdl | template_id 4 |\n",
      "train pairs: 1262 | valid pairs: 600 | test pairs: 800\n",
      "train images: 30338 | valid images: 10420 | test images: 12995\n",
      "seen_indices: 1844 | unseen_indices: 8576\n",
      "seen_indices: 2380 | unseen_indices: 10615\n",
      "attr \t| train (30338, 115) \t| valid (10420, 115) \t| test (12995, 115)\n",
      "obj \t| train (30338, 245) \t| valid (10420, 245) \t| test (12995, 245)\n",
      "image_dim:    360\n",
      "attr \t| train (30338, 115) \t| valid (10420, 115) \t| test (12995, 115)\n",
      "obj \t| train (30338, 245) \t| valid (10420, 245) \t| test (12995, 245)\n",
      "image_dim:    360\n",
      "attr \t| train (30338, 115) \t| valid (10420, 115) \t| test (12995, 115)\n",
      "obj \t| train (30338, 245) \t| valid (10420, 245) \t| test (12995, 245)\n",
      "image_dim:    360\n",
      "seen_indices: 1844 | unseen_indices: 8576\n",
      "Intervention features: torch.Size([10420, 360])\n",
      "seen_indices: 1844 | unseen_indices: 8576\n",
      "Intervention features: torch.Size([10420, 360])\n",
      "features:  torch.Size([10420, 1962])\n",
      "topk: 1\n",
      "best_seen_acc: 0.5786\n",
      "best_unseen_acc: 0.6658\n",
      "best_harmonic_mean: 0.5207\n",
      "auc: 0.3490\n",
      "topk: 2\n",
      "best_seen_acc: 0.7489\n",
      "best_unseen_acc: 0.8317\n",
      "best_harmonic_mean: 0.6714\n",
      "auc: 0.5732\n",
      "topk: 3\n",
      "best_seen_acc: 0.8416\n",
      "best_unseen_acc: 0.8973\n",
      "best_harmonic_mean: 0.7593\n",
      "auc: 0.7043\n",
      "\n",
      "\n",
      "\n",
      "======================================================================\n",
      "| valid | template compdl | template_id 5 |\n",
      "train pairs: 1262 | valid pairs: 600 | test pairs: 800\n",
      "train images: 30338 | valid images: 10420 | test images: 12995\n",
      "seen_indices: 1844 | unseen_indices: 8576\n",
      "seen_indices: 2380 | unseen_indices: 10615\n",
      "attr \t| train (30338, 115) \t| valid (10420, 115) \t| test (12995, 115)\n",
      "obj \t| train (30338, 245) \t| valid (10420, 245) \t| test (12995, 245)\n",
      "image_dim:    360\n",
      "attr \t| train (30338, 115) \t| valid (10420, 115) \t| test (12995, 115)\n",
      "obj \t| train (30338, 245) \t| valid (10420, 245) \t| test (12995, 245)\n",
      "image_dim:    360\n",
      "attr \t| train (30338, 115) \t| valid (10420, 115) \t| test (12995, 115)\n",
      "obj \t| train (30338, 245) \t| valid (10420, 245) \t| test (12995, 245)\n",
      "image_dim:    360\n",
      "seen_indices: 1844 | unseen_indices: 8576\n",
      "Intervention features: torch.Size([10420, 360])\n",
      "seen_indices: 1844 | unseen_indices: 8576\n",
      "Intervention features: torch.Size([10420, 360])\n",
      "features:  torch.Size([10420, 1962])\n",
      "topk: 1\n",
      "best_seen_acc: 0.6361\n",
      "best_unseen_acc: 0.6592\n",
      "best_harmonic_mean: 0.5366\n",
      "auc: 0.3843\n",
      "topk: 2\n",
      "best_seen_acc: 0.7581\n",
      "best_unseen_acc: 0.7807\n",
      "best_harmonic_mean: 0.6899\n",
      "auc: 0.5591\n",
      "topk: 3\n",
      "best_seen_acc: 0.8395\n",
      "best_unseen_acc: 0.8551\n",
      "best_harmonic_mean: 0.7722\n",
      "auc: 0.6852\n",
      "\n",
      "\n",
      "\n",
      "======================================================================\n",
      "| valid | template compdl | template_id 6 |\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train pairs: 1262 | valid pairs: 600 | test pairs: 800\n",
      "train images: 30338 | valid images: 10420 | test images: 12995\n",
      "seen_indices: 1844 | unseen_indices: 8576\n",
      "seen_indices: 2380 | unseen_indices: 10615\n",
      "attr \t| train (30338, 115) \t| valid (10420, 115) \t| test (12995, 115)\n",
      "obj \t| train (30338, 245) \t| valid (10420, 245) \t| test (12995, 245)\n",
      "image_dim:    360\n",
      "attr \t| train (30338, 115) \t| valid (10420, 115) \t| test (12995, 115)\n",
      "obj \t| train (30338, 245) \t| valid (10420, 245) \t| test (12995, 245)\n",
      "image_dim:    360\n",
      "attr \t| train (30338, 115) \t| valid (10420, 115) \t| test (12995, 115)\n",
      "obj \t| train (30338, 245) \t| valid (10420, 245) \t| test (12995, 245)\n",
      "image_dim:    360\n",
      "seen_indices: 1844 | unseen_indices: 8576\n",
      "Intervention features: torch.Size([10420, 360])\n",
      "seen_indices: 1844 | unseen_indices: 8576\n",
      "Intervention features: torch.Size([10420, 360])\n",
      "features:  torch.Size([10420, 1962])\n",
      "topk: 1\n",
      "best_seen_acc: 0.6367\n",
      "best_unseen_acc: 0.7134\n",
      "best_harmonic_mean: 0.5683\n",
      "auc: 0.4174\n",
      "topk: 2\n",
      "best_seen_acc: 0.8113\n",
      "best_unseen_acc: 0.8715\n",
      "best_harmonic_mean: 0.7228\n",
      "auc: 0.6562\n",
      "topk: 3\n",
      "best_seen_acc: 0.8606\n",
      "best_unseen_acc: 0.9302\n",
      "best_harmonic_mean: 0.8055\n",
      "auc: 0.7642\n",
      "\n",
      "\n",
      "\n",
      "======================================================================\n",
      "| valid | template compdl | template_id 7 |\n",
      "train pairs: 1262 | valid pairs: 600 | test pairs: 800\n",
      "train images: 30338 | valid images: 10420 | test images: 12995\n",
      "seen_indices: 1844 | unseen_indices: 8576\n",
      "seen_indices: 2380 | unseen_indices: 10615\n",
      "attr \t| train (30338, 115) \t| valid (10420, 115) \t| test (12995, 115)\n",
      "obj \t| train (30338, 245) \t| valid (10420, 245) \t| test (12995, 245)\n",
      "image_dim:    360\n",
      "attr \t| train (30338, 115) \t| valid (10420, 115) \t| test (12995, 115)\n",
      "obj \t| train (30338, 245) \t| valid (10420, 245) \t| test (12995, 245)\n",
      "image_dim:    360\n",
      "attr \t| train (30338, 115) \t| valid (10420, 115) \t| test (12995, 115)\n",
      "obj \t| train (30338, 245) \t| valid (10420, 245) \t| test (12995, 245)\n",
      "image_dim:    360\n",
      "seen_indices: 1844 | unseen_indices: 8576\n",
      "Intervention features: torch.Size([10420, 360])\n",
      "seen_indices: 1844 | unseen_indices: 8576\n",
      "Intervention features: torch.Size([10420, 360])\n",
      "features:  torch.Size([10420, 1962])\n",
      "topk: 1\n",
      "best_seen_acc: 0.5781\n",
      "best_unseen_acc: 0.6559\n",
      "best_harmonic_mean: 0.5156\n",
      "auc: 0.3484\n",
      "topk: 2\n",
      "best_seen_acc: 0.7549\n",
      "best_unseen_acc: 0.8054\n",
      "best_harmonic_mean: 0.6801\n",
      "auc: 0.5632\n",
      "topk: 3\n",
      "best_seen_acc: 0.8167\n",
      "best_unseen_acc: 0.8932\n",
      "best_harmonic_mean: 0.7554\n",
      "auc: 0.6770\n",
      "\n",
      "\n",
      "\n",
      "======================================================================\n",
      "| valid | template compdl | template_id 8 |\n",
      "train pairs: 1262 | valid pairs: 600 | test pairs: 800\n",
      "train images: 30338 | valid images: 10420 | test images: 12995\n",
      "seen_indices: 1844 | unseen_indices: 8576\n",
      "seen_indices: 2380 | unseen_indices: 10615\n",
      "attr \t| train (30338, 115) \t| valid (10420, 115) \t| test (12995, 115)\n",
      "obj \t| train (30338, 245) \t| valid (10420, 245) \t| test (12995, 245)\n",
      "image_dim:    360\n",
      "attr \t| train (30338, 115) \t| valid (10420, 115) \t| test (12995, 115)\n",
      "obj \t| train (30338, 245) \t| valid (10420, 245) \t| test (12995, 245)\n",
      "image_dim:    360\n",
      "attr \t| train (30338, 115) \t| valid (10420, 115) \t| test (12995, 115)\n",
      "obj \t| train (30338, 245) \t| valid (10420, 245) \t| test (12995, 245)\n",
      "image_dim:    360\n",
      "seen_indices: 1844 | unseen_indices: 8576\n",
      "Intervention features: torch.Size([10420, 360])\n",
      "seen_indices: 1844 | unseen_indices: 8576\n",
      "Intervention features: torch.Size([10420, 360])\n",
      "features:  torch.Size([10420, 1962])\n",
      "topk: 1\n",
      "best_seen_acc: 0.5168\n",
      "best_unseen_acc: 0.5915\n",
      "best_harmonic_mean: 0.4604\n",
      "auc: 0.2733\n",
      "topk: 2\n",
      "best_seen_acc: 0.6697\n",
      "best_unseen_acc: 0.7990\n",
      "best_harmonic_mean: 0.5912\n",
      "auc: 0.4727\n",
      "topk: 3\n",
      "best_seen_acc: 0.7424\n",
      "best_unseen_acc: 0.8675\n",
      "best_harmonic_mean: 0.6797\n",
      "auc: 0.5829\n",
      "\n",
      "\n",
      "\n",
      "======================================================================\n",
      "| valid | template compdl | template_id 9 |\n",
      "train pairs: 1262 | valid pairs: 600 | test pairs: 800\n",
      "train images: 30338 | valid images: 10420 | test images: 12995\n",
      "seen_indices: 1844 | unseen_indices: 8576\n",
      "seen_indices: 2380 | unseen_indices: 10615\n",
      "attr \t| train (30338, 115) \t| valid (10420, 115) \t| test (12995, 115)\n",
      "obj \t| train (30338, 245) \t| valid (10420, 245) \t| test (12995, 245)\n",
      "image_dim:    360\n",
      "attr \t| train (30338, 115) \t| valid (10420, 115) \t| test (12995, 115)\n",
      "obj \t| train (30338, 245) \t| valid (10420, 245) \t| test (12995, 245)\n",
      "image_dim:    360\n",
      "attr \t| train (30338, 115) \t| valid (10420, 115) \t| test (12995, 115)\n",
      "obj \t| train (30338, 245) \t| valid (10420, 245) \t| test (12995, 245)\n",
      "image_dim:    360\n",
      "seen_indices: 1844 | unseen_indices: 8576\n",
      "Intervention features: torch.Size([10420, 360])\n",
      "seen_indices: 1844 | unseen_indices: 8576\n",
      "Intervention features: torch.Size([10420, 360])\n",
      "features:  torch.Size([10420, 1962])\n",
      "topk: 1\n",
      "best_seen_acc: 0.5835\n",
      "best_unseen_acc: 0.6845\n",
      "best_harmonic_mean: 0.5309\n",
      "auc: 0.3657\n",
      "topk: 2\n",
      "best_seen_acc: 0.7533\n",
      "best_unseen_acc: 0.8011\n",
      "best_harmonic_mean: 0.6736\n",
      "auc: 0.5613\n",
      "topk: 3\n",
      "best_seen_acc: 0.8178\n",
      "best_unseen_acc: 0.8749\n",
      "best_harmonic_mean: 0.7636\n",
      "auc: 0.6805\n",
      "\n",
      "\n",
      "\n",
      "======================================================================\n",
      "| test | template clip | template_id 0 |\n",
      "train pairs: 1262 | valid pairs: 600 | test pairs: 800\n",
      "train images: 30338 | valid images: 10420 | test images: 12995\n",
      "seen_indices: 1844 | unseen_indices: 8576\n",
      "seen_indices: 2380 | unseen_indices: 10615\n",
      "attr \t| train (30338, 115) \t| valid (10420, 115) \t| test (12995, 115)\n",
      "obj \t| train (30338, 245) \t| valid (10420, 245) \t| test (12995, 245)\n",
      "image_dim:    360\n",
      "attr \t| train (30338, 115) \t| valid (10420, 115) \t| test (12995, 115)\n",
      "obj \t| train (30338, 245) \t| valid (10420, 245) \t| test (12995, 245)\n",
      "image_dim:    360\n",
      "attr \t| train (30338, 115) \t| valid (10420, 115) \t| test (12995, 115)\n",
      "obj \t| train (30338, 245) \t| valid (10420, 245) \t| test (12995, 245)\n",
      "image_dim:    360\n",
      "seen_indices: 2380 | unseen_indices: 10615\n",
      "Intervention features: torch.Size([12995, 360])\n",
      "seen_indices: 2380 | unseen_indices: 10615\n",
      "Intervention features: torch.Size([12995, 360])\n",
      "features:  torch.Size([12995, 1962])\n",
      "topk: 1\n",
      "best_seen_acc: 0.6727\n",
      "best_unseen_acc: 0.7639\n",
      "best_harmonic_mean: 0.6186\n",
      "auc: 0.4814\n",
      "topk: 2\n",
      "best_seen_acc: 0.8231\n",
      "best_unseen_acc: 0.8718\n",
      "best_harmonic_mean: 0.7622\n",
      "auc: 0.6906\n",
      "topk: 3\n",
      "best_seen_acc: 0.8819\n",
      "best_unseen_acc: 0.9084\n",
      "best_harmonic_mean: 0.8261\n",
      "auc: 0.7783\n",
      "\n",
      "\n",
      "\n",
      "======================================================================\n",
      "| test | template clip | template_id 1 |\n",
      "train pairs: 1262 | valid pairs: 600 | test pairs: 800\n",
      "train images: 30338 | valid images: 10420 | test images: 12995\n",
      "seen_indices: 1844 | unseen_indices: 8576\n",
      "seen_indices: 2380 | unseen_indices: 10615\n",
      "attr \t| train (30338, 115) \t| valid (10420, 115) \t| test (12995, 115)\n",
      "obj \t| train (30338, 245) \t| valid (10420, 245) \t| test (12995, 245)\n",
      "image_dim:    360\n",
      "attr \t| train (30338, 115) \t| valid (10420, 115) \t| test (12995, 115)\n",
      "obj \t| train (30338, 245) \t| valid (10420, 245) \t| test (12995, 245)\n",
      "image_dim:    360\n",
      "attr \t| train (30338, 115) \t| valid (10420, 115) \t| test (12995, 115)\n",
      "obj \t| train (30338, 245) \t| valid (10420, 245) \t| test (12995, 245)\n",
      "image_dim:    360\n",
      "seen_indices: 2380 | unseen_indices: 10615\n",
      "Intervention features: torch.Size([12995, 360])\n",
      "seen_indices: 2380 | unseen_indices: 10615\n",
      "Intervention features: torch.Size([12995, 360])\n",
      "features:  torch.Size([12995, 1962])\n",
      "topk: 1\n",
      "best_seen_acc: 0.6332\n",
      "best_unseen_acc: 0.7649\n",
      "best_harmonic_mean: 0.6020\n",
      "auc: 0.4541\n",
      "topk: 2\n",
      "best_seen_acc: 0.7790\n",
      "best_unseen_acc: 0.8670\n",
      "best_harmonic_mean: 0.7387\n",
      "auc: 0.6458\n",
      "topk: 3\n",
      "best_seen_acc: 0.8357\n",
      "best_unseen_acc: 0.9157\n",
      "best_harmonic_mean: 0.7967\n",
      "auc: 0.7234\n",
      "\n",
      "\n",
      "\n",
      "======================================================================\n",
      "| test | template clip | template_id 2 |\n",
      "train pairs: 1262 | valid pairs: 600 | test pairs: 800\n",
      "train images: 30338 | valid images: 10420 | test images: 12995\n",
      "seen_indices: 1844 | unseen_indices: 8576\n",
      "seen_indices: 2380 | unseen_indices: 10615\n",
      "attr \t| train (30338, 115) \t| valid (10420, 115) \t| test (12995, 115)\n",
      "obj \t| train (30338, 245) \t| valid (10420, 245) \t| test (12995, 245)\n",
      "image_dim:    360\n",
      "attr \t| train (30338, 115) \t| valid (10420, 115) \t| test (12995, 115)\n",
      "obj \t| train (30338, 245) \t| valid (10420, 245) \t| test (12995, 245)\n",
      "image_dim:    360\n",
      "attr \t| train (30338, 115) \t| valid (10420, 115) \t| test (12995, 115)\n",
      "obj \t| train (30338, 245) \t| valid (10420, 245) \t| test (12995, 245)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "image_dim:    360\n",
      "seen_indices: 2380 | unseen_indices: 10615\n",
      "Intervention features: torch.Size([12995, 360])\n",
      "seen_indices: 2380 | unseen_indices: 10615\n",
      "Intervention features: torch.Size([12995, 360])\n",
      "features:  torch.Size([12995, 1962])\n",
      "topk: 1\n",
      "best_seen_acc: 0.5739\n",
      "best_unseen_acc: 0.7502\n",
      "best_harmonic_mean: 0.5640\n",
      "auc: 0.4016\n",
      "topk: 2\n",
      "best_seen_acc: 0.7336\n",
      "best_unseen_acc: 0.8643\n",
      "best_harmonic_mean: 0.6959\n",
      "auc: 0.5979\n",
      "topk: 3\n",
      "best_seen_acc: 0.8172\n",
      "best_unseen_acc: 0.9068\n",
      "best_harmonic_mean: 0.7753\n",
      "auc: 0.7121\n",
      "\n",
      "\n",
      "\n",
      "======================================================================\n",
      "| test | template clip | template_id 3 |\n",
      "train pairs: 1262 | valid pairs: 600 | test pairs: 800\n",
      "train images: 30338 | valid images: 10420 | test images: 12995\n",
      "seen_indices: 1844 | unseen_indices: 8576\n",
      "seen_indices: 2380 | unseen_indices: 10615\n",
      "attr \t| train (30338, 115) \t| valid (10420, 115) \t| test (12995, 115)\n",
      "obj \t| train (30338, 245) \t| valid (10420, 245) \t| test (12995, 245)\n",
      "image_dim:    360\n",
      "attr \t| train (30338, 115) \t| valid (10420, 115) \t| test (12995, 115)\n",
      "obj \t| train (30338, 245) \t| valid (10420, 245) \t| test (12995, 245)\n",
      "image_dim:    360\n",
      "attr \t| train (30338, 115) \t| valid (10420, 115) \t| test (12995, 115)\n",
      "obj \t| train (30338, 245) \t| valid (10420, 245) \t| test (12995, 245)\n",
      "image_dim:    360\n",
      "seen_indices: 2380 | unseen_indices: 10615\n",
      "Intervention features: torch.Size([12995, 360])\n",
      "seen_indices: 2380 | unseen_indices: 10615\n",
      "Intervention features: torch.Size([12995, 360])\n",
      "features:  torch.Size([12995, 1962])\n",
      "topk: 1\n",
      "best_seen_acc: 0.6340\n",
      "best_unseen_acc: 0.7685\n",
      "best_harmonic_mean: 0.6005\n",
      "auc: 0.4536\n",
      "topk: 2\n",
      "best_seen_acc: 0.7752\n",
      "best_unseen_acc: 0.8723\n",
      "best_harmonic_mean: 0.7447\n",
      "auc: 0.6443\n",
      "topk: 3\n",
      "best_seen_acc: 0.8454\n",
      "best_unseen_acc: 0.9233\n",
      "best_harmonic_mean: 0.8068\n",
      "auc: 0.7517\n",
      "\n",
      "\n",
      "\n",
      "======================================================================\n",
      "| test | template clip | template_id 4 |\n",
      "train pairs: 1262 | valid pairs: 600 | test pairs: 800\n",
      "train images: 30338 | valid images: 10420 | test images: 12995\n",
      "seen_indices: 1844 | unseen_indices: 8576\n",
      "seen_indices: 2380 | unseen_indices: 10615\n",
      "attr \t| train (30338, 115) \t| valid (10420, 115) \t| test (12995, 115)\n",
      "obj \t| train (30338, 245) \t| valid (10420, 245) \t| test (12995, 245)\n",
      "image_dim:    360\n",
      "attr \t| train (30338, 115) \t| valid (10420, 115) \t| test (12995, 115)\n",
      "obj \t| train (30338, 245) \t| valid (10420, 245) \t| test (12995, 245)\n",
      "image_dim:    360\n",
      "attr \t| train (30338, 115) \t| valid (10420, 115) \t| test (12995, 115)\n",
      "obj \t| train (30338, 245) \t| valid (10420, 245) \t| test (12995, 245)\n",
      "image_dim:    360\n",
      "seen_indices: 2380 | unseen_indices: 10615\n",
      "Intervention features: torch.Size([12995, 360])\n",
      "seen_indices: 2380 | unseen_indices: 10615\n",
      "Intervention features: torch.Size([12995, 360])\n",
      "features:  torch.Size([12995, 1962])\n",
      "topk: 1\n",
      "best_seen_acc: 0.6000\n",
      "best_unseen_acc: 0.7017\n",
      "best_harmonic_mean: 0.5363\n",
      "auc: 0.3805\n",
      "topk: 2\n",
      "best_seen_acc: 0.7492\n",
      "best_unseen_acc: 0.8268\n",
      "best_harmonic_mean: 0.6699\n",
      "auc: 0.5773\n",
      "topk: 3\n",
      "best_seen_acc: 0.8197\n",
      "best_unseen_acc: 0.8837\n",
      "best_harmonic_mean: 0.7538\n",
      "auc: 0.6754\n",
      "\n",
      "\n",
      "\n",
      "======================================================================\n",
      "| test | template clip | template_id 5 |\n",
      "train pairs: 1262 | valid pairs: 600 | test pairs: 800\n",
      "train images: 30338 | valid images: 10420 | test images: 12995\n",
      "seen_indices: 1844 | unseen_indices: 8576\n",
      "seen_indices: 2380 | unseen_indices: 10615\n",
      "attr \t| train (30338, 115) \t| valid (10420, 115) \t| test (12995, 115)\n",
      "obj \t| train (30338, 245) \t| valid (10420, 245) \t| test (12995, 245)\n",
      "image_dim:    360\n",
      "attr \t| train (30338, 115) \t| valid (10420, 115) \t| test (12995, 115)\n",
      "obj \t| train (30338, 245) \t| valid (10420, 245) \t| test (12995, 245)\n",
      "image_dim:    360\n",
      "attr \t| train (30338, 115) \t| valid (10420, 115) \t| test (12995, 115)\n",
      "obj \t| train (30338, 245) \t| valid (10420, 245) \t| test (12995, 245)\n",
      "image_dim:    360\n",
      "seen_indices: 2380 | unseen_indices: 10615\n",
      "Intervention features: torch.Size([12995, 360])\n",
      "seen_indices: 2380 | unseen_indices: 10615\n",
      "Intervention features: torch.Size([12995, 360])\n",
      "features:  torch.Size([12995, 1962])\n",
      "topk: 1\n",
      "best_seen_acc: 0.6630\n",
      "best_unseen_acc: 0.7541\n",
      "best_harmonic_mean: 0.6239\n",
      "auc: 0.4735\n",
      "topk: 2\n",
      "best_seen_acc: 0.7975\n",
      "best_unseen_acc: 0.8546\n",
      "best_harmonic_mean: 0.7469\n",
      "auc: 0.6563\n",
      "topk: 3\n",
      "best_seen_acc: 0.8597\n",
      "best_unseen_acc: 0.8929\n",
      "best_harmonic_mean: 0.8085\n",
      "auc: 0.7494\n",
      "\n",
      "\n",
      "\n",
      "======================================================================\n",
      "| test | template clip | template_id 6 |\n",
      "train pairs: 1262 | valid pairs: 600 | test pairs: 800\n",
      "train images: 30338 | valid images: 10420 | test images: 12995\n",
      "seen_indices: 1844 | unseen_indices: 8576\n",
      "seen_indices: 2380 | unseen_indices: 10615\n",
      "attr \t| train (30338, 115) \t| valid (10420, 115) \t| test (12995, 115)\n",
      "obj \t| train (30338, 245) \t| valid (10420, 245) \t| test (12995, 245)\n",
      "image_dim:    360\n",
      "attr \t| train (30338, 115) \t| valid (10420, 115) \t| test (12995, 115)\n",
      "obj \t| train (30338, 245) \t| valid (10420, 245) \t| test (12995, 245)\n",
      "image_dim:    360\n",
      "attr \t| train (30338, 115) \t| valid (10420, 115) \t| test (12995, 115)\n",
      "obj \t| train (30338, 245) \t| valid (10420, 245) \t| test (12995, 245)\n",
      "image_dim:    360\n",
      "seen_indices: 2380 | unseen_indices: 10615\n",
      "Intervention features: torch.Size([12995, 360])\n",
      "seen_indices: 2380 | unseen_indices: 10615\n",
      "Intervention features: torch.Size([12995, 360])\n",
      "features:  torch.Size([12995, 1962])\n",
      "topk: 1\n",
      "best_seen_acc: 0.6193\n",
      "best_unseen_acc: 0.7585\n",
      "best_harmonic_mean: 0.5866\n",
      "auc: 0.4380\n",
      "topk: 2\n",
      "best_seen_acc: 0.7878\n",
      "best_unseen_acc: 0.8675\n",
      "best_harmonic_mean: 0.7159\n",
      "auc: 0.6448\n",
      "topk: 3\n",
      "best_seen_acc: 0.8466\n",
      "best_unseen_acc: 0.9124\n",
      "best_harmonic_mean: 0.7944\n",
      "auc: 0.7423\n",
      "\n",
      "\n",
      "\n",
      "======================================================================\n",
      "| test | template compdl | template_id 0 |\n",
      "train pairs: 1262 | valid pairs: 600 | test pairs: 800\n",
      "train images: 30338 | valid images: 10420 | test images: 12995\n",
      "seen_indices: 1844 | unseen_indices: 8576\n",
      "seen_indices: 2380 | unseen_indices: 10615\n",
      "attr \t| train (30338, 115) \t| valid (10420, 115) \t| test (12995, 115)\n",
      "obj \t| train (30338, 245) \t| valid (10420, 245) \t| test (12995, 245)\n",
      "image_dim:    360\n",
      "attr \t| train (30338, 115) \t| valid (10420, 115) \t| test (12995, 115)\n",
      "obj \t| train (30338, 245) \t| valid (10420, 245) \t| test (12995, 245)\n",
      "image_dim:    360\n",
      "attr \t| train (30338, 115) \t| valid (10420, 115) \t| test (12995, 115)\n",
      "obj \t| train (30338, 245) \t| valid (10420, 245) \t| test (12995, 245)\n",
      "image_dim:    360\n",
      "seen_indices: 2380 | unseen_indices: 10615\n",
      "Intervention features: torch.Size([12995, 360])\n",
      "seen_indices: 2380 | unseen_indices: 10615\n",
      "Intervention features: torch.Size([12995, 360])\n",
      "features:  torch.Size([12995, 1962])\n",
      "topk: 1\n",
      "best_seen_acc: 0.5416\n",
      "best_unseen_acc: 0.6814\n",
      "best_harmonic_mean: 0.4934\n",
      "auc: 0.3261\n",
      "topk: 2\n",
      "best_seen_acc: 0.6941\n",
      "best_unseen_acc: 0.8368\n",
      "best_harmonic_mean: 0.6581\n",
      "auc: 0.5372\n",
      "topk: 3\n",
      "best_seen_acc: 0.7618\n",
      "best_unseen_acc: 0.8662\n",
      "best_harmonic_mean: 0.7291\n",
      "auc: 0.6315\n",
      "\n",
      "\n",
      "\n",
      "======================================================================\n",
      "| test | template compdl | template_id 1 |\n",
      "train pairs: 1262 | valid pairs: 600 | test pairs: 800\n",
      "train images: 30338 | valid images: 10420 | test images: 12995\n",
      "seen_indices: 1844 | unseen_indices: 8576\n",
      "seen_indices: 2380 | unseen_indices: 10615\n",
      "attr \t| train (30338, 115) \t| valid (10420, 115) \t| test (12995, 115)\n",
      "obj \t| train (30338, 245) \t| valid (10420, 245) \t| test (12995, 245)\n",
      "image_dim:    360\n",
      "attr \t| train (30338, 115) \t| valid (10420, 115) \t| test (12995, 115)\n",
      "obj \t| train (30338, 245) \t| valid (10420, 245) \t| test (12995, 245)\n",
      "image_dim:    360\n",
      "attr \t| train (30338, 115) \t| valid (10420, 115) \t| test (12995, 115)\n",
      "obj \t| train (30338, 245) \t| valid (10420, 245) \t| test (12995, 245)\n",
      "image_dim:    360\n",
      "seen_indices: 2380 | unseen_indices: 10615\n",
      "Intervention features: torch.Size([12995, 360])\n",
      "seen_indices: 2380 | unseen_indices: 10615\n",
      "Intervention features: torch.Size([12995, 360])\n",
      "features:  torch.Size([12995, 1962])\n",
      "topk: 1\n",
      "best_seen_acc: 0.6361\n",
      "best_unseen_acc: 0.7350\n",
      "best_harmonic_mean: 0.5661\n",
      "auc: 0.4272\n",
      "topk: 2\n",
      "best_seen_acc: 0.7718\n",
      "best_unseen_acc: 0.8543\n",
      "best_harmonic_mean: 0.7163\n",
      "auc: 0.6253\n",
      "topk: 3\n",
      "best_seen_acc: 0.8277\n",
      "best_unseen_acc: 0.8925\n",
      "best_harmonic_mean: 0.7852\n",
      "auc: 0.7151\n",
      "\n",
      "\n",
      "\n",
      "======================================================================\n",
      "| test | template compdl | template_id 2 |\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train pairs: 1262 | valid pairs: 600 | test pairs: 800\n",
      "train images: 30338 | valid images: 10420 | test images: 12995\n",
      "seen_indices: 1844 | unseen_indices: 8576\n",
      "seen_indices: 2380 | unseen_indices: 10615\n",
      "attr \t| train (30338, 115) \t| valid (10420, 115) \t| test (12995, 115)\n",
      "obj \t| train (30338, 245) \t| valid (10420, 245) \t| test (12995, 245)\n",
      "image_dim:    360\n",
      "attr \t| train (30338, 115) \t| valid (10420, 115) \t| test (12995, 115)\n",
      "obj \t| train (30338, 245) \t| valid (10420, 245) \t| test (12995, 245)\n",
      "image_dim:    360\n",
      "attr \t| train (30338, 115) \t| valid (10420, 115) \t| test (12995, 115)\n",
      "obj \t| train (30338, 245) \t| valid (10420, 245) \t| test (12995, 245)\n",
      "image_dim:    360\n",
      "seen_indices: 2380 | unseen_indices: 10615\n",
      "Intervention features: torch.Size([12995, 360])\n",
      "seen_indices: 2380 | unseen_indices: 10615\n",
      "Intervention features: torch.Size([12995, 360])\n",
      "features:  torch.Size([12995, 1962])\n",
      "topk: 1\n",
      "best_seen_acc: 0.5597\n",
      "best_unseen_acc: 0.6904\n",
      "best_harmonic_mean: 0.5162\n",
      "auc: 0.3499\n",
      "topk: 2\n",
      "best_seen_acc: 0.7063\n",
      "best_unseen_acc: 0.8259\n",
      "best_harmonic_mean: 0.6535\n",
      "auc: 0.5420\n",
      "topk: 3\n",
      "best_seen_acc: 0.7668\n",
      "best_unseen_acc: 0.8652\n",
      "best_harmonic_mean: 0.7320\n",
      "auc: 0.6322\n",
      "\n",
      "\n",
      "\n",
      "======================================================================\n",
      "| test | template compdl | template_id 3 |\n",
      "train pairs: 1262 | valid pairs: 600 | test pairs: 800\n",
      "train images: 30338 | valid images: 10420 | test images: 12995\n",
      "seen_indices: 1844 | unseen_indices: 8576\n",
      "seen_indices: 2380 | unseen_indices: 10615\n",
      "attr \t| train (30338, 115) \t| valid (10420, 115) \t| test (12995, 115)\n",
      "obj \t| train (30338, 245) \t| valid (10420, 245) \t| test (12995, 245)\n",
      "image_dim:    360\n",
      "attr \t| train (30338, 115) \t| valid (10420, 115) \t| test (12995, 115)\n",
      "obj \t| train (30338, 245) \t| valid (10420, 245) \t| test (12995, 245)\n",
      "image_dim:    360\n",
      "attr \t| train (30338, 115) \t| valid (10420, 115) \t| test (12995, 115)\n",
      "obj \t| train (30338, 245) \t| valid (10420, 245) \t| test (12995, 245)\n",
      "image_dim:    360\n",
      "seen_indices: 2380 | unseen_indices: 10615\n",
      "Intervention features: torch.Size([12995, 360])\n",
      "seen_indices: 2380 | unseen_indices: 10615\n",
      "Intervention features: torch.Size([12995, 360])\n",
      "features:  torch.Size([12995, 1962])\n",
      "topk: 1\n",
      "best_seen_acc: 0.5605\n",
      "best_unseen_acc: 0.6796\n",
      "best_harmonic_mean: 0.4981\n",
      "auc: 0.3335\n",
      "topk: 2\n",
      "best_seen_acc: 0.7029\n",
      "best_unseen_acc: 0.8184\n",
      "best_harmonic_mean: 0.6537\n",
      "auc: 0.5324\n",
      "topk: 3\n",
      "best_seen_acc: 0.7597\n",
      "best_unseen_acc: 0.8642\n",
      "best_harmonic_mean: 0.7228\n",
      "auc: 0.6244\n",
      "\n",
      "\n",
      "\n",
      "======================================================================\n",
      "| test | template compdl | template_id 4 |\n",
      "train pairs: 1262 | valid pairs: 600 | test pairs: 800\n",
      "train images: 30338 | valid images: 10420 | test images: 12995\n",
      "seen_indices: 1844 | unseen_indices: 8576\n",
      "seen_indices: 2380 | unseen_indices: 10615\n",
      "attr \t| train (30338, 115) \t| valid (10420, 115) \t| test (12995, 115)\n",
      "obj \t| train (30338, 245) \t| valid (10420, 245) \t| test (12995, 245)\n",
      "image_dim:    360\n",
      "attr \t| train (30338, 115) \t| valid (10420, 115) \t| test (12995, 115)\n",
      "obj \t| train (30338, 245) \t| valid (10420, 245) \t| test (12995, 245)\n",
      "image_dim:    360\n",
      "attr \t| train (30338, 115) \t| valid (10420, 115) \t| test (12995, 115)\n",
      "obj \t| train (30338, 245) \t| valid (10420, 245) \t| test (12995, 245)\n",
      "image_dim:    360\n",
      "seen_indices: 2380 | unseen_indices: 10615\n",
      "Intervention features: torch.Size([12995, 360])\n",
      "seen_indices: 2380 | unseen_indices: 10615\n",
      "Intervention features: torch.Size([12995, 360])\n",
      "features:  torch.Size([12995, 1962])\n",
      "topk: 1\n",
      "best_seen_acc: 0.5580\n",
      "best_unseen_acc: 0.6984\n",
      "best_harmonic_mean: 0.5059\n",
      "auc: 0.3478\n",
      "topk: 2\n",
      "best_seen_acc: 0.7139\n",
      "best_unseen_acc: 0.8229\n",
      "best_harmonic_mean: 0.6541\n",
      "auc: 0.5405\n",
      "topk: 3\n",
      "best_seen_acc: 0.8017\n",
      "best_unseen_acc: 0.8747\n",
      "best_harmonic_mean: 0.7356\n",
      "auc: 0.6589\n",
      "\n",
      "\n",
      "\n",
      "======================================================================\n",
      "| test | template compdl | template_id 5 |\n",
      "train pairs: 1262 | valid pairs: 600 | test pairs: 800\n",
      "train images: 30338 | valid images: 10420 | test images: 12995\n",
      "seen_indices: 1844 | unseen_indices: 8576\n",
      "seen_indices: 2380 | unseen_indices: 10615\n",
      "attr \t| train (30338, 115) \t| valid (10420, 115) \t| test (12995, 115)\n",
      "obj \t| train (30338, 245) \t| valid (10420, 245) \t| test (12995, 245)\n",
      "image_dim:    360\n",
      "attr \t| train (30338, 115) \t| valid (10420, 115) \t| test (12995, 115)\n",
      "obj \t| train (30338, 245) \t| valid (10420, 245) \t| test (12995, 245)\n",
      "image_dim:    360\n",
      "attr \t| train (30338, 115) \t| valid (10420, 115) \t| test (12995, 115)\n",
      "obj \t| train (30338, 245) \t| valid (10420, 245) \t| test (12995, 245)\n",
      "image_dim:    360\n",
      "seen_indices: 2380 | unseen_indices: 10615\n",
      "Intervention features: torch.Size([12995, 360])\n",
      "seen_indices: 2380 | unseen_indices: 10615\n",
      "Intervention features: torch.Size([12995, 360])\n",
      "features:  torch.Size([12995, 1962])\n",
      "topk: 1\n",
      "best_seen_acc: 0.5773\n",
      "best_unseen_acc: 0.6837\n",
      "best_harmonic_mean: 0.5179\n",
      "auc: 0.3567\n",
      "topk: 2\n",
      "best_seen_acc: 0.7050\n",
      "best_unseen_acc: 0.8238\n",
      "best_harmonic_mean: 0.6621\n",
      "auc: 0.5417\n",
      "topk: 3\n",
      "best_seen_acc: 0.7744\n",
      "best_unseen_acc: 0.8703\n",
      "best_harmonic_mean: 0.7345\n",
      "auc: 0.6369\n",
      "\n",
      "\n",
      "\n",
      "======================================================================\n",
      "| test | template compdl | template_id 6 |\n",
      "train pairs: 1262 | valid pairs: 600 | test pairs: 800\n",
      "train images: 30338 | valid images: 10420 | test images: 12995\n",
      "seen_indices: 1844 | unseen_indices: 8576\n",
      "seen_indices: 2380 | unseen_indices: 10615\n",
      "attr \t| train (30338, 115) \t| valid (10420, 115) \t| test (12995, 115)\n",
      "obj \t| train (30338, 245) \t| valid (10420, 245) \t| test (12995, 245)\n",
      "image_dim:    360\n",
      "attr \t| train (30338, 115) \t| valid (10420, 115) \t| test (12995, 115)\n",
      "obj \t| train (30338, 245) \t| valid (10420, 245) \t| test (12995, 245)\n",
      "image_dim:    360\n",
      "attr \t| train (30338, 115) \t| valid (10420, 115) \t| test (12995, 115)\n",
      "obj \t| train (30338, 245) \t| valid (10420, 245) \t| test (12995, 245)\n",
      "image_dim:    360\n",
      "seen_indices: 2380 | unseen_indices: 10615\n",
      "Intervention features: torch.Size([12995, 360])\n",
      "seen_indices: 2380 | unseen_indices: 10615\n",
      "Intervention features: torch.Size([12995, 360])\n",
      "features:  torch.Size([12995, 1962])\n",
      "topk: 1\n",
      "best_seen_acc: 0.5983\n",
      "best_unseen_acc: 0.7337\n",
      "best_harmonic_mean: 0.5500\n",
      "auc: 0.3973\n",
      "topk: 2\n",
      "best_seen_acc: 0.7634\n",
      "best_unseen_acc: 0.8549\n",
      "best_harmonic_mean: 0.6932\n",
      "auc: 0.6074\n",
      "topk: 3\n",
      "best_seen_acc: 0.8307\n",
      "best_unseen_acc: 0.9118\n",
      "best_harmonic_mean: 0.7764\n",
      "auc: 0.7228\n",
      "\n",
      "\n",
      "\n",
      "======================================================================\n",
      "| test | template compdl | template_id 7 |\n",
      "train pairs: 1262 | valid pairs: 600 | test pairs: 800\n",
      "train images: 30338 | valid images: 10420 | test images: 12995\n",
      "seen_indices: 1844 | unseen_indices: 8576\n",
      "seen_indices: 2380 | unseen_indices: 10615\n",
      "attr \t| train (30338, 115) \t| valid (10420, 115) \t| test (12995, 115)\n",
      "obj \t| train (30338, 245) \t| valid (10420, 245) \t| test (12995, 245)\n",
      "image_dim:    360\n",
      "attr \t| train (30338, 115) \t| valid (10420, 115) \t| test (12995, 115)\n",
      "obj \t| train (30338, 245) \t| valid (10420, 245) \t| test (12995, 245)\n",
      "image_dim:    360\n",
      "attr \t| train (30338, 115) \t| valid (10420, 115) \t| test (12995, 115)\n",
      "obj \t| train (30338, 245) \t| valid (10420, 245) \t| test (12995, 245)\n",
      "image_dim:    360\n",
      "seen_indices: 2380 | unseen_indices: 10615\n",
      "Intervention features: torch.Size([12995, 360])\n",
      "seen_indices: 2380 | unseen_indices: 10615\n",
      "Intervention features: torch.Size([12995, 360])\n",
      "features:  torch.Size([12995, 1962])\n",
      "topk: 1\n",
      "best_seen_acc: 0.5756\n",
      "best_unseen_acc: 0.6838\n",
      "best_harmonic_mean: 0.5088\n",
      "auc: 0.3526\n",
      "topk: 2\n",
      "best_seen_acc: 0.7252\n",
      "best_unseen_acc: 0.8198\n",
      "best_harmonic_mean: 0.6567\n",
      "auc: 0.5516\n",
      "topk: 3\n",
      "best_seen_acc: 0.7866\n",
      "best_unseen_acc: 0.8702\n",
      "best_harmonic_mean: 0.7315\n",
      "auc: 0.6486\n",
      "\n",
      "\n",
      "\n",
      "======================================================================\n",
      "| test | template compdl | template_id 8 |\n",
      "train pairs: 1262 | valid pairs: 600 | test pairs: 800\n",
      "train images: 30338 | valid images: 10420 | test images: 12995\n",
      "seen_indices: 1844 | unseen_indices: 8576\n",
      "seen_indices: 2380 | unseen_indices: 10615\n",
      "attr \t| train (30338, 115) \t| valid (10420, 115) \t| test (12995, 115)\n",
      "obj \t| train (30338, 245) \t| valid (10420, 245) \t| test (12995, 245)\n",
      "image_dim:    360\n",
      "attr \t| train (30338, 115) \t| valid (10420, 115) \t| test (12995, 115)\n",
      "obj \t| train (30338, 245) \t| valid (10420, 245) \t| test (12995, 245)\n",
      "image_dim:    360\n",
      "attr \t| train (30338, 115) \t| valid (10420, 115) \t| test (12995, 115)\n",
      "obj \t| train (30338, 245) \t| valid (10420, 245) \t| test (12995, 245)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "image_dim:    360\n",
      "seen_indices: 2380 | unseen_indices: 10615\n",
      "Intervention features: torch.Size([12995, 360])\n",
      "seen_indices: 2380 | unseen_indices: 10615\n",
      "Intervention features: torch.Size([12995, 360])\n",
      "features:  torch.Size([12995, 1962])\n",
      "topk: 1\n",
      "best_seen_acc: 0.4908\n",
      "best_unseen_acc: 0.6229\n",
      "best_harmonic_mean: 0.4404\n",
      "auc: 0.2644\n",
      "topk: 2\n",
      "best_seen_acc: 0.6836\n",
      "best_unseen_acc: 0.8100\n",
      "best_harmonic_mean: 0.6036\n",
      "auc: 0.4883\n",
      "topk: 3\n",
      "best_seen_acc: 0.7563\n",
      "best_unseen_acc: 0.8642\n",
      "best_harmonic_mean: 0.6938\n",
      "auc: 0.6081\n",
      "\n",
      "\n",
      "\n",
      "======================================================================\n",
      "| test | template compdl | template_id 9 |\n",
      "train pairs: 1262 | valid pairs: 600 | test pairs: 800\n",
      "train images: 30338 | valid images: 10420 | test images: 12995\n",
      "seen_indices: 1844 | unseen_indices: 8576\n",
      "seen_indices: 2380 | unseen_indices: 10615\n",
      "attr \t| train (30338, 115) \t| valid (10420, 115) \t| test (12995, 115)\n",
      "obj \t| train (30338, 245) \t| valid (10420, 245) \t| test (12995, 245)\n",
      "image_dim:    360\n",
      "attr \t| train (30338, 115) \t| valid (10420, 115) \t| test (12995, 115)\n",
      "obj \t| train (30338, 245) \t| valid (10420, 245) \t| test (12995, 245)\n",
      "image_dim:    360\n",
      "attr \t| train (30338, 115) \t| valid (10420, 115) \t| test (12995, 115)\n",
      "obj \t| train (30338, 245) \t| valid (10420, 245) \t| test (12995, 245)\n",
      "image_dim:    360\n",
      "seen_indices: 2380 | unseen_indices: 10615\n",
      "Intervention features: torch.Size([12995, 360])\n",
      "seen_indices: 2380 | unseen_indices: 10615\n",
      "Intervention features: torch.Size([12995, 360])\n",
      "features:  torch.Size([12995, 1962])\n",
      "topk: 1\n",
      "best_seen_acc: 0.5626\n",
      "best_unseen_acc: 0.6885\n",
      "best_harmonic_mean: 0.5027\n",
      "auc: 0.3426\n",
      "topk: 2\n",
      "best_seen_acc: 0.7210\n",
      "best_unseen_acc: 0.8114\n",
      "best_harmonic_mean: 0.6605\n",
      "auc: 0.5449\n",
      "topk: 3\n",
      "best_seen_acc: 0.7790\n",
      "best_unseen_acc: 0.8628\n",
      "best_harmonic_mean: 0.7308\n",
      "auc: 0.6376\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Interpretability for Interv(GTX)\n",
    "\n",
    "class parseArguments:\n",
    "    def __init__(self):\n",
    "        self.split = \"valid\"\n",
    "        self.is_open_world = False\n",
    "        \n",
    "        self.feature = \"primitive\"\n",
    "        self.model = \"clip\"\n",
    "        self.train_warmup = 0\n",
    "        self.is_image_projection = True\n",
    "        self.is_bias = False\n",
    "        self.is_limit = True\n",
    "        \n",
    "        self.data_root = \"../../data/mit_states\"\n",
    "        self.emb_root = \"../../data\"\n",
    "        self.precomputed_data_dir = f\"../../outputs/mit_states/clip_prompt_precompute_features\"\n",
    "        \n",
    "        self.emb_init = True\n",
    "        self.input_dim = 600\n",
    "        self.num_epochs = 400\n",
    "        self.batch_size = 128\n",
    "        self.learning_rate = 5e-5\n",
    "        self.weight_decay = 5e-5\n",
    "        self.logit_scale = 0.07\n",
    "\n",
    "args = parseArguments()\n",
    "\n",
    "# Start!!\n",
    "template_count_dict = {\"clip\": 7, \"compdl\": 10}\n",
    "\n",
    "for split in [\"valid\", \"test\"]:\n",
    "    args.split = split\n",
    "    for template_src, template_count in template_count_dict.items():\n",
    "        for template_id in range(template_count):\n",
    "            print(\"=\" * 70)\n",
    "            print(f\"| {args.split} | template {template_src} | template_id {template_id} |\")\n",
    "            args.precomputed_data_root = os.path.join(args.precomputed_data_dir, template_src, f\"template_{template_id}\")\n",
    "\n",
    "            # Load dataset\n",
    "            data = MITStatesDataset(root=args.data_root, split=\"train\")  # split can be ignored here\n",
    "            seen_ids_valid, unseen_ids_valid = get_seen_unseen_indices(\"valid\", data)\n",
    "            seen_ids_test, unseen_ids_test = get_seen_unseen_indices(\"test\", data)\n",
    "\n",
    "            train_dataset = Precomputed_MITStatesDataset(split=\"train\", feature=\"primitive\", data=data, args=args, is_limit=True)\n",
    "            valid_dataset = Precomputed_MITStatesDataset(split=\"valid\", feature=\"primitive\", data=data, args=args, is_limit=True)\n",
    "            test_dataset = Precomputed_MITStatesDataset(split=\"test\", feature=\"primitive\", data=data, args=args, is_limit=True)\n",
    "            seen_mask = train_dataset.seen_mask\n",
    "\n",
    "            # Load model\n",
    "            model = load_model(template_src, template_id)\n",
    "\n",
    "            # Prepare groundtruth labels\n",
    "            seen_ids, unseen_ids = get_seen_unseen_indices(args.split, data)\n",
    "\n",
    "            if args.split == \"valid\":\n",
    "                labels_attr = [sample[\"attr_id\"] for sample in data.valid_data]\n",
    "                labels_obj = [sample[\"obj_id\"] for sample in data.valid_data]\n",
    "                labels = [sample[\"pair_id\"] for sample in data.valid_data]\n",
    "                gt_features_attr = np.zeros((valid_dataset.image_embs.shape[0], 115))\n",
    "                gt_features_obj = np.zeros((valid_dataset.image_embs.shape[0], 245))\n",
    "            elif args.split == \"test\":\n",
    "                labels_attr = [sample[\"attr_id\"] for sample in data.test_data]\n",
    "                labels_obj = [sample[\"obj_id\"] for sample in data.test_data]\n",
    "                labels = [sample[\"pair_id\"] for sample in data.test_data]\n",
    "                gt_features_attr = np.zeros((test_dataset.image_embs.shape[0], 115))\n",
    "                gt_features_obj = np.zeros((test_dataset.image_embs.shape[0], 245))\n",
    "\n",
    "            gt_features_attr[np.arange(len(labels_attr)), labels_attr] = 1\n",
    "            gt_features_obj[np.arange(len(labels_obj)), labels_obj] = 1\n",
    "            gt_features_concat = np.concatenate([gt_features_attr, gt_features_obj], axis=-1)\n",
    "            gt_features_concat = torch.tensor(gt_features_concat).to(device).double()\n",
    "            print(f\"Intervention features: {gt_features_concat.shape}\")\n",
    "\n",
    "            # Prepare \"binary\" primitive concept activations\n",
    "            # Set GT concepts to 1\n",
    "            seen_ids, unseen_ids = get_seen_unseen_indices(args.split, data)\n",
    "\n",
    "            if args.split == \"valid\":\n",
    "                labels_attr = [sample[\"attr_id\"] for sample in data.valid_data]\n",
    "                labels_obj = [sample[\"obj_id\"] for sample in data.valid_data]\n",
    "                labels = [sample[\"pair_id\"] for sample in data.valid_data]\n",
    "                features_interv = valid_dataset.image_embs\n",
    "                features_interv = torch.tensor(features_interv).to(device).double()\n",
    "                features_interv = F.normalize(features_interv, dim=1)\n",
    "\n",
    "            elif args.split == \"test\":\n",
    "                labels_attr = [sample[\"attr_id\"] for sample in data.test_data]\n",
    "                labels_obj = [sample[\"obj_id\"] for sample in data.test_data]\n",
    "                labels = [sample[\"pair_id\"] for sample in data.test_data]\n",
    "                features_interv = test_dataset.image_embs\n",
    "                features_interv = torch.tensor(features_interv).to(device).double()\n",
    "                features_interv = F.normalize(features_interv, dim=1)\n",
    "\n",
    "            features_interv = torch.where(gt_features_concat==1, 1.0, features_interv)\n",
    "            print(f\"Intervention features: {features_interv.shape}\")\n",
    "\n",
    "            # Intervene!!!\n",
    "            with torch.no_grad():\n",
    "                logits, loss = model(features_interv, pair_labels=None, is_train=False, is_intervene=True)\n",
    "            features = logits.softmax(dim=-1).log()\n",
    "            print(\"features: \", features.shape)\n",
    "\n",
    "            # Compute metrics\n",
    "            overall_metrics = get_overall_metrics(features, labels, seen_ids, unseen_ids, seen_mask, data, topk_list=[1,2,3])\n",
    "            output_path = get_output_path(template_src, template_id, args)\n",
    "            with open(os.path.join(output_path, f\"{args.split}_metrics_interv_gtx.json\"), \"w\") as f:\n",
    "                json.dump(overall_metrics, f)\n",
    "            print(\"\\n\\n\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "37e8b278",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "template_src: clip\n",
      "split: valid\n",
      "| topk 1 | auc | mean: 43.616760 | std: 4.273825\n",
      "| topk 2 | auc | mean: 63.981882 | std: 4.311276\n",
      "| topk 3 | auc | mean: 73.299077 | std: 4.150668\n",
      "\n",
      "split: test\n",
      "| topk 1 | auc | mean: 44.037432 | std: 3.430984\n",
      "| topk 1 | best_seen_acc | mean: 62.803121 | std: 3.177578\n",
      "| topk 1 | best_unseen_acc | mean: 75.168562 | std: 2.123174\n",
      "| topk 1 | best_harmonic_mean | mean: 59.028622 | std: 2.875708\n",
      "| topk 2 | auc | mean: 63.670394 | std: 3.489931\n",
      "| topk 3 | auc | mean: 73.324113 | std: 3.069886\n",
      "\n",
      "template_src: compdl\n",
      "split: valid\n",
      "| topk 1 | auc | mean: 36.527693 | std: 4.332185\n",
      "| topk 2 | auc | mean: 56.577485 | std: 5.268111\n",
      "| topk 3 | auc | mean: 68.128082 | std: 5.697588\n",
      "\n",
      "split: test\n",
      "| topk 1 | auc | mean: 34.980297 | std: 4.061202\n",
      "| topk 1 | best_seen_acc | mean: 56.605042 | std: 3.547861\n",
      "| topk 1 | best_unseen_acc | mean: 68.974093 | std: 2.961350\n",
      "| topk 1 | best_harmonic_mean | mean: 50.995273 | std: 3.191393\n",
      "| topk 2 | auc | mean: 55.112662 | std: 3.670802\n",
      "| topk 3 | auc | mean: 65.159907 | std: 3.603692\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Aggregate interpretability results for Interv(GT)\n",
    "# \n",
    "template_count_dict = {\"clip\": 7, \"compdl\": 10}\n",
    "\n",
    "for template_src, template_count in template_count_dict.items():\n",
    "    # For each possible template source, intialize a new statistics dict\n",
    "    all_statistics_dict = {\n",
    "        \"valid\": {\n",
    "            \"1\": {\"auc\": []},\n",
    "            \"2\": {\"auc\": []},\n",
    "            \"3\": {\"auc\": []},\n",
    "        },\n",
    "        \"test\": {\n",
    "            \"1\": {\n",
    "                \"auc\": [],\n",
    "                \"best_seen_acc\": [],\n",
    "                \"best_unseen_acc\": [],\n",
    "                \"best_harmonic_mean\": [],\n",
    "            },\n",
    "            \"2\": {\"auc\": []},\n",
    "            \"3\": {\"auc\": []},\n",
    "        },\n",
    "    }\n",
    "\n",
    "    # Sum all the required statistics\n",
    "    for template_id in range(template_count):\n",
    "        exp_name = \"clip_prompt_retrieval_model_TP{}_TID{}_open0_init1_imgproj1_bias0_{}_Lim1_N400_TW0_B128_LR5e-05_WD5e-05_L0.07\".format(\n",
    "            template_src,\n",
    "            template_id,\n",
    "            args.feature,\n",
    "        )\n",
    "        output_dir = get_output_path(template_src, template_id, args)\n",
    "        \n",
    "        # Valid metrics\n",
    "        with open(os.path.join(output_dir, \"valid_metrics_interv_gtx.json\"), \"rb\") as f:\n",
    "            data = json.load(f)\n",
    "        all_statistics_dict[\"valid\"][\"1\"][\"auc\"].append(data[\"1\"][\"auc\"])\n",
    "        all_statistics_dict[\"valid\"][\"2\"][\"auc\"].append(data[\"2\"][\"auc\"])\n",
    "        all_statistics_dict[\"valid\"][\"3\"][\"auc\"].append(data[\"3\"][\"auc\"])\n",
    "        \n",
    "        # Test metrics\n",
    "        with open(os.path.join(output_dir, \"test_metrics_interv_gtx.json\"), \"rb\") as f:\n",
    "            data = json.load(f)\n",
    "        all_statistics_dict[\"test\"][\"1\"][\"auc\"].append(data[\"1\"][\"auc\"])\n",
    "        all_statistics_dict[\"test\"][\"2\"][\"auc\"].append(data[\"2\"][\"auc\"])\n",
    "        all_statistics_dict[\"test\"][\"3\"][\"auc\"].append(data[\"3\"][\"auc\"])\n",
    "        all_statistics_dict[\"test\"][\"1\"][\"best_seen_acc\"].append(data[\"1\"][\"best_seen_acc\"])\n",
    "        all_statistics_dict[\"test\"][\"1\"][\"best_unseen_acc\"].append(data[\"1\"][\"best_unseen_acc\"])\n",
    "        all_statistics_dict[\"test\"][\"1\"][\"best_harmonic_mean\"].append(data[\"1\"][\"best_harmonic_mean\"])\n",
    "        \n",
    "    # Take the mean of the required statistics and output\n",
    "    print(f\"template_src: {template_src}\")\n",
    "    print(\"split: valid\")\n",
    "    for topk in all_statistics_dict[\"valid\"].keys():\n",
    "        for k, v in all_statistics_dict[\"valid\"][topk].items():\n",
    "            mean = 100 * np.mean(all_statistics_dict[\"valid\"][topk][k])\n",
    "            std = 100 * np.std(all_statistics_dict[\"valid\"][topk][k])\n",
    "            print(f\"| topk {topk} | {k} | mean: {mean:8.6f} | std: {std:8.6f}\")\n",
    "    print()\n",
    "    \n",
    "    print(\"split: test\")\n",
    "    for topk in all_statistics_dict[\"test\"].keys():\n",
    "        for k, v in all_statistics_dict[\"test\"][topk].items():\n",
    "            mean = 100 * np.mean(all_statistics_dict[\"test\"][topk][k])\n",
    "            std = 100 * np.std(all_statistics_dict[\"test\"][topk][k])\n",
    "            print(f\"| topk {topk} | {k} | mean: {mean:8.6f} | std: {std:8.6f}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f923df79",
   "metadata": {},
   "source": [
    "## 4.4. Usefulness - Open World"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "3ca5d93f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "template_src: clip\n",
      "split: valid\n",
      "| topk 1 | auc | mean: 2.630426 | std: 0.240937\n",
      "| topk 2 | auc | mean: 6.052778 | std: 0.412947\n",
      "| topk 3 | auc | mean: 9.103339 | std: 0.452032\n",
      "\n",
      "split: test\n",
      "| topk 1 | auc | mean: 2.153398 | std: 0.120531\n",
      "| topk 1 | best_seen_acc | mean: 32.803121 | std: 0.906104\n",
      "| topk 1 | best_unseen_acc | mean: 10.055851 | std: 0.545023\n",
      "| topk 1 | best_harmonic_mean | mean: 11.030260 | std: 0.324100\n",
      "| topk 2 | auc | mean: 5.241396 | std: 0.255591\n",
      "| topk 3 | auc | mean: 7.897845 | std: 0.411861\n",
      "\n",
      "template_src: compdl\n",
      "split: valid\n",
      "| topk 1 | auc | mean: 2.516759 | std: 0.132692\n",
      "| topk 2 | auc | mean: 5.833674 | std: 0.258408\n",
      "| topk 3 | auc | mean: 8.817559 | std: 0.357837\n",
      "\n",
      "split: test\n",
      "| topk 1 | auc | mean: 2.057855 | std: 0.183082\n",
      "| topk 1 | best_seen_acc | mean: 32.147059 | std: 1.081629\n",
      "| topk 1 | best_unseen_acc | mean: 9.848328 | std: 0.582881\n",
      "| topk 1 | best_harmonic_mean | mean: 10.809411 | std: 0.533516\n",
      "| topk 2 | auc | mean: 4.984341 | std: 0.391096\n",
      "| topk 3 | auc | mean: 7.640945 | std: 0.526965\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Aggregate open-world usefulness\n",
    "template_count_dict = {\"clip\": 7, \"compdl\": 10}\n",
    "args.feature = \"pair\"\n",
    "\n",
    "for template_src, template_count in template_count_dict.items():\n",
    "    # For each possible template source, intialize a new statistics dict\n",
    "    all_statistics_dict = {\n",
    "        \"valid\": {\n",
    "            \"1\": {\"auc\": []},\n",
    "            \"2\": {\"auc\": []},\n",
    "            \"3\": {\"auc\": []},\n",
    "        },\n",
    "        \"test\": {\n",
    "            \"1\": {\n",
    "                \"auc\": [],\n",
    "                \"best_seen_acc\": [],\n",
    "                \"best_unseen_acc\": [],\n",
    "                \"best_harmonic_mean\": [],\n",
    "            },\n",
    "            \"2\": {\"auc\": []},\n",
    "            \"3\": {\"auc\": []},\n",
    "        },\n",
    "    }\n",
    "\n",
    "    # Sum all the required statistics\n",
    "    for template_id in range(template_count):\n",
    "        exp_name = \"clip_prompt_retrieval_model_TP{}_TID{}_open1_init1_imgproj1_bias0_{}_Lim1_N400_TW0_B128_LR5e-05_WD5e-05_L0.07\".format(\n",
    "            template_src,\n",
    "            template_id,\n",
    "            args.feature,\n",
    "        )\n",
    "        output_dir = os.path.join(\"../../outputs/mit_states/clip_prompt_open_world_eval\", exp_name)\n",
    "        \n",
    "        # Valid metrics\n",
    "        with open(os.path.join(output_dir, \"valid_metrics.json\"), \"rb\") as f:\n",
    "            data = json.load(f)\n",
    "        all_statistics_dict[\"valid\"][\"1\"][\"auc\"].append(data[\"1\"][\"auc\"])\n",
    "        all_statistics_dict[\"valid\"][\"2\"][\"auc\"].append(data[\"2\"][\"auc\"])\n",
    "        all_statistics_dict[\"valid\"][\"3\"][\"auc\"].append(data[\"3\"][\"auc\"])\n",
    "        \n",
    "        # Test metrics\n",
    "        with open(os.path.join(output_dir, \"test_metrics.json\"), \"rb\") as f:\n",
    "            data = json.load(f)\n",
    "        all_statistics_dict[\"test\"][\"1\"][\"auc\"].append(data[\"1\"][\"auc\"])\n",
    "        all_statistics_dict[\"test\"][\"2\"][\"auc\"].append(data[\"2\"][\"auc\"])\n",
    "        all_statistics_dict[\"test\"][\"3\"][\"auc\"].append(data[\"3\"][\"auc\"])\n",
    "        all_statistics_dict[\"test\"][\"1\"][\"best_seen_acc\"].append(data[\"1\"][\"best_seen_acc\"])\n",
    "        all_statistics_dict[\"test\"][\"1\"][\"best_unseen_acc\"].append(data[\"1\"][\"best_unseen_acc\"])\n",
    "        all_statistics_dict[\"test\"][\"1\"][\"best_harmonic_mean\"].append(data[\"1\"][\"best_harmonic_mean\"])\n",
    "        \n",
    "    # Take the mean of the required statistics and output\n",
    "    print(f\"template_src: {template_src}\")\n",
    "    print(\"split: valid\")\n",
    "    for topk in all_statistics_dict[\"valid\"].keys():\n",
    "        for k, v in all_statistics_dict[\"valid\"][topk].items():\n",
    "            mean = 100 * np.mean(all_statistics_dict[\"valid\"][topk][k])\n",
    "            std = 100 * np.std(all_statistics_dict[\"valid\"][topk][k])\n",
    "            print(f\"| topk {topk} | {k} | mean: {mean:8.6f} | std: {std:8.6f}\")\n",
    "    print()\n",
    "    \n",
    "    print(\"split: test\")\n",
    "    for topk in all_statistics_dict[\"test\"].keys():\n",
    "        for k, v in all_statistics_dict[\"test\"][topk].items():\n",
    "            mean = 100 * np.mean(all_statistics_dict[\"test\"][topk][k])\n",
    "            std = 100 * np.std(all_statistics_dict[\"test\"][topk][k])\n",
    "            print(f\"| topk {topk} | {k} | mean: {mean:8.6f} | std: {std:8.6f}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "3db17017",
   "metadata": {},
   "outputs": [],
   "source": [
    "templates = {\n",
    "    \"clip\": [\n",
    "        \"itap of a {}\",\n",
    "        \"a bad photo of the {}\",\n",
    "        \"a origami {}\",\n",
    "        \"a photo of the large {}\",\n",
    "        \"a {} in a video game\",\n",
    "        \"art of the {}\",\n",
    "        \"a photo of the small {}\",\n",
    "    ],\n",
    "    \"compdl\": [\n",
    "        \"this is {}\",\n",
    "        \"the object is {}\",\n",
    "        \"the item is {}\",\n",
    "        \"the item in the given picture is {}\",\n",
    "        \"the thing in this bad photo is {}\",\n",
    "        \"the item in the photo is {}\",\n",
    "        \"the item in this cool photo is {}\",\n",
    "        \"the main object in the photo is {}\",\n",
    "        \"the item in the low resolution image is {}\",\n",
    "        \"the object in the photo is {}\",\n",
    "    ],\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed0ea787",
   "metadata": {},
   "source": [
    "# 5. Evaluation for (Random) Multiple Prompts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "35ac6e37",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load trained model\n",
    "def load_model(template_src, num_prompts):\n",
    "    ckpt_dir = f\"../../outputs/mit_states/clip_prompt_retrieval_model_random\"\n",
    "    ckpt_name = \"clip_prompt_retrieval_model_TP{}_NP{}_open{}_init{}_imgproj{}_bias{}_{}_Lim{}_N{}_TW{}_B{}_LR{}_WD{}_L{}\".format(\n",
    "        template_src,\n",
    "        num_prompts,\n",
    "        1 if args.is_open_world else 0,\n",
    "        1 if args.emb_init else 0,\n",
    "        1 if args.is_image_projection else 0,\n",
    "        1 if args.is_bias else 0,\n",
    "        args.feature,\n",
    "        1 if args.is_limit else 0, \n",
    "        args.num_epochs,\n",
    "        args.train_warmup,\n",
    "        args.batch_size,\n",
    "        args.learning_rate,\n",
    "        args.weight_decay,\n",
    "        args.logit_scale,\n",
    "    )\n",
    "    ckpt_path = os.path.join(ckpt_dir, ckpt_name, \"retrieval_model.ckpt\")\n",
    "\n",
    "    limit_pairs = list(train_dataset.limit_pair2idx.keys())\n",
    "    open_world_pairs = list(valid_dataset.open_world_pair2idx.keys())\n",
    "    model = RetrievalModel(data, train_dataset.image_dim, limit_pairs, open_world_pairs, args)\n",
    "    model.load_state_dict(torch.load(ckpt_path))\n",
    "    model.eval()\n",
    "    model.to(device)\n",
    "    return model\n",
    "\n",
    "def get_output_path(template_src, num_prompts, args):\n",
    "    ckpt_dir = f\"../../outputs/mit_states/clip_prompt_retrieval_model_random\"\n",
    "    ckpt_name = \"clip_prompt_retrieval_model_TP{}_NP{}_open{}_init{}_imgproj{}_bias{}_{}_Lim{}_N{}_TW{}_B{}_LR{}_WD{}_L{}\".format(\n",
    "        template_src,\n",
    "        num_prompts,\n",
    "        1 if args.is_open_world else 0,\n",
    "        1 if args.emb_init else 0,\n",
    "        1 if args.is_image_projection else 0,\n",
    "        1 if args.is_bias else 0,\n",
    "        args.feature,\n",
    "        1 if args.is_limit else 0, \n",
    "        args.num_epochs,\n",
    "        args.train_warmup,\n",
    "        args.batch_size,\n",
    "        args.learning_rate,\n",
    "        args.weight_decay,\n",
    "        args.logit_scale,\n",
    "    )\n",
    "    return os.path.join(ckpt_dir, ckpt_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "6ef9971b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "clip_prompt_retrieval_model_TPclip_NP4_open0_init1_imgproj1_bias0_pair_Lim1_N400_TW0_B128_LR5e-05_WD5e-05_L0.07\r\n",
      "clip_prompt_retrieval_model_TPclip_NP4_open0_init1_imgproj1_bias0_primitive_Lim1_N400_TW0_B128_LR5e-05_WD5e-05_L0.07\r\n"
     ]
    }
   ],
   "source": [
    "!ls ../../outputs/mit_states/clip_prompt_retrieval_model_random"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1afaa584",
   "metadata": {},
   "source": [
    "## 5.1. Interpretabilty for Interv(GT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "593e927d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "clip_prompt_retrieval_model_TPclip_NP4_open0_init1_imgproj1_bias0_pair_Lim1_N400_TW0_B128_LR5e-05_WD5e-05_L0.07\r\n",
      "clip_prompt_retrieval_model_TPclip_NP4_open0_init1_imgproj1_bias0_primitive_Lim1_N400_TW0_B128_LR5e-05_WD5e-05_L0.07\r\n"
     ]
    }
   ],
   "source": [
    "!ls ../../outputs/mit_states/clip_prompt_retrieval_model_random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "e67625af",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "| valid | template clip | num_prompts 4 |\n",
      "train pairs: 1262 | valid pairs: 600 | test pairs: 800\n",
      "train images: 30338 | valid images: 10420 | test images: 12995\n",
      "seen_indices: 1844 | unseen_indices: 8576\n",
      "seen_indices: 2380 | unseen_indices: 10615\n",
      "attr \t| train (30338, 115) \t| valid (10420, 115) \t| test (12995, 115)\n",
      "obj \t| train (30338, 245) \t| valid (10420, 245) \t| test (12995, 245)\n",
      "image_dim:    360\n",
      "attr \t| train (30338, 115) \t| valid (10420, 115) \t| test (12995, 115)\n",
      "obj \t| train (30338, 245) \t| valid (10420, 245) \t| test (12995, 245)\n",
      "image_dim:    360\n",
      "attr \t| train (30338, 115) \t| valid (10420, 115) \t| test (12995, 115)\n",
      "obj \t| train (30338, 245) \t| valid (10420, 245) \t| test (12995, 245)\n",
      "image_dim:    360\n",
      "seen_indices: 1844 | unseen_indices: 8576\n",
      "Intervention features: torch.Size([10420, 360])\n",
      "features:  torch.Size([10420, 1962])\n",
      "topk: 1\n",
      "best_seen_acc: 0.6855\n",
      "best_unseen_acc: 0.7164\n",
      "best_harmonic_mean: 0.6094\n",
      "auc: 0.4588\n",
      "topk: 2\n",
      "best_seen_acc: 0.8048\n",
      "best_unseen_acc: 0.8443\n",
      "best_harmonic_mean: 0.7393\n",
      "auc: 0.6465\n",
      "topk: 3\n",
      "best_seen_acc: 0.8850\n",
      "best_unseen_acc: 0.8819\n",
      "best_harmonic_mean: 0.7916\n",
      "auc: 0.7519\n",
      "\n",
      "\n",
      "\n",
      "======================================================================\n",
      "| test | template clip | num_prompts 4 |\n",
      "train pairs: 1262 | valid pairs: 600 | test pairs: 800\n",
      "train images: 30338 | valid images: 10420 | test images: 12995\n",
      "seen_indices: 1844 | unseen_indices: 8576\n",
      "seen_indices: 2380 | unseen_indices: 10615\n",
      "attr \t| train (30338, 115) \t| valid (10420, 115) \t| test (12995, 115)\n",
      "obj \t| train (30338, 245) \t| valid (10420, 245) \t| test (12995, 245)\n",
      "image_dim:    360\n",
      "attr \t| train (30338, 115) \t| valid (10420, 115) \t| test (12995, 115)\n",
      "obj \t| train (30338, 245) \t| valid (10420, 245) \t| test (12995, 245)\n",
      "image_dim:    360\n",
      "attr \t| train (30338, 115) \t| valid (10420, 115) \t| test (12995, 115)\n",
      "obj \t| train (30338, 245) \t| valid (10420, 245) \t| test (12995, 245)\n",
      "image_dim:    360\n",
      "seen_indices: 2380 | unseen_indices: 10615\n",
      "Intervention features: torch.Size([12995, 360])\n",
      "features:  torch.Size([12995, 1962])\n",
      "topk: 1\n",
      "best_seen_acc: 0.6265\n",
      "best_unseen_acc: 0.7773\n",
      "best_harmonic_mean: 0.6214\n",
      "auc: 0.4631\n",
      "topk: 2\n",
      "best_seen_acc: 0.7828\n",
      "best_unseen_acc: 0.8672\n",
      "best_harmonic_mean: 0.7482\n",
      "auc: 0.6554\n",
      "topk: 3\n",
      "best_seen_acc: 0.8454\n",
      "best_unseen_acc: 0.9132\n",
      "best_harmonic_mean: 0.8091\n",
      "auc: 0.7488\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Interpretability for Interv(GT)\n",
    "\n",
    "class parseArguments:\n",
    "    def __init__(self):\n",
    "        self.split = \"valid\"\n",
    "        self.is_open_world = False\n",
    "        \n",
    "        self.feature = \"primitive\"\n",
    "        self.model = \"clip\"\n",
    "        self.num_prompts = 4\n",
    "        self.train_warmup = 0\n",
    "        self.is_image_projection = True\n",
    "        self.is_bias = False\n",
    "        self.is_limit = True\n",
    "        \n",
    "        self.data_root = \"../../data/mit_states\"\n",
    "        self.emb_root = \"../../data\"\n",
    "        self.precomputed_data_root = f\"../../outputs/mit_states/clip_prompt_precompute_features_random/{self.model}/num_prompts_{self.num_prompts}\"\n",
    "        \n",
    "        self.emb_init = True\n",
    "        self.input_dim = 600\n",
    "        self.num_epochs = 400\n",
    "        self.batch_size = 128\n",
    "        self.learning_rate = 5e-5\n",
    "        self.weight_decay = 5e-5\n",
    "        self.logit_scale = 0.07\n",
    "\n",
    "args = parseArguments()\n",
    "\n",
    "# Start!!\n",
    "template_count_dict = {\"clip\": 7, \"compdl\": 10}\n",
    "\n",
    "for split in [\"valid\", \"test\"]:\n",
    "    args.split = split\n",
    "    for template_src in [\"clip\"]:\n",
    "        print(\"=\" * 70)\n",
    "        print(f\"| {args.split} | template {template_src} | num_prompts {args.num_prompts} |\")\n",
    "\n",
    "        # Load dataset\n",
    "        data = MITStatesDataset(root=args.data_root, split=\"train\")  # split can be ignored here\n",
    "        seen_ids_valid, unseen_ids_valid = get_seen_unseen_indices(\"valid\", data)\n",
    "        seen_ids_test, unseen_ids_test = get_seen_unseen_indices(\"test\", data)\n",
    "\n",
    "        train_dataset = Precomputed_MITStatesDataset(split=\"train\", feature=\"primitive\", data=data, args=args, is_limit=True)\n",
    "        valid_dataset = Precomputed_MITStatesDataset(split=\"valid\", feature=\"primitive\", data=data, args=args, is_limit=True)\n",
    "        test_dataset = Precomputed_MITStatesDataset(split=\"test\", feature=\"primitive\", data=data, args=args, is_limit=True)\n",
    "        seen_mask = train_dataset.seen_mask\n",
    "\n",
    "        # Load model\n",
    "        model = load_model(template_src, args.num_prompts)\n",
    "\n",
    "        # Prepare groundtruth labels\n",
    "        seen_ids, unseen_ids = get_seen_unseen_indices(args.split, data)\n",
    "\n",
    "        if args.split == \"valid\":\n",
    "            labels_attr = [sample[\"attr_id\"] for sample in data.valid_data]\n",
    "            labels_obj = [sample[\"obj_id\"] for sample in data.valid_data]\n",
    "            labels = [sample[\"pair_id\"] for sample in data.valid_data]\n",
    "            gt_features_attr = np.zeros((valid_dataset.image_embs.shape[0], 115))\n",
    "            gt_features_obj = np.zeros((valid_dataset.image_embs.shape[0], 245))\n",
    "        elif args.split == \"test\":\n",
    "            labels_attr = [sample[\"attr_id\"] for sample in data.test_data]\n",
    "            labels_obj = [sample[\"obj_id\"] for sample in data.test_data]\n",
    "            labels = [sample[\"pair_id\"] for sample in data.test_data]\n",
    "            gt_features_attr = np.zeros((test_dataset.image_embs.shape[0], 115))\n",
    "            gt_features_obj = np.zeros((test_dataset.image_embs.shape[0], 245))\n",
    "\n",
    "        gt_features_attr[np.arange(len(labels_attr)), labels_attr] = 1\n",
    "        gt_features_obj[np.arange(len(labels_obj)), labels_obj] = 1\n",
    "        gt_features_concat = np.concatenate([gt_features_attr, gt_features_obj], axis=-1)\n",
    "        gt_features_concat = torch.tensor(gt_features_concat).to(device).double()\n",
    "        print(f\"Intervention features: {gt_features_concat.shape}\")\n",
    "\n",
    "        # Intervene!!!\n",
    "        with torch.no_grad():\n",
    "            logits, loss = model(gt_features_concat, pair_labels=None, is_train=False, is_intervene=True)\n",
    "        features = logits.softmax(dim=-1).log()\n",
    "        print(\"features: \", features.shape)\n",
    "\n",
    "        # Compute metrics\n",
    "        overall_metrics = get_overall_metrics(features, labels, seen_ids, unseen_ids, seen_mask, data, topk_list=[1,2,3])\n",
    "        output_path = get_output_path(template_src, args.num_prompts, args)\n",
    "        with open(os.path.join(output_path, f\"{args.split}_metrics_interv_gt.json\"), \"w\") as f:\n",
    "            json.dump(overall_metrics, f)\n",
    "        print(\"\\n\\n\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b6055e7",
   "metadata": {},
   "source": [
    "## 5.2. Interpretability for Interv(GTX)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "3ba4d3bd",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "| valid | template clip | num_prompts 4 |\n",
      "train pairs: 1262 | valid pairs: 600 | test pairs: 800\n",
      "train images: 30338 | valid images: 10420 | test images: 12995\n",
      "seen_indices: 1844 | unseen_indices: 8576\n",
      "seen_indices: 2380 | unseen_indices: 10615\n",
      "attr \t| train (30338, 115) \t| valid (10420, 115) \t| test (12995, 115)\n",
      "obj \t| train (30338, 245) \t| valid (10420, 245) \t| test (12995, 245)\n",
      "image_dim:    360\n",
      "attr \t| train (30338, 115) \t| valid (10420, 115) \t| test (12995, 115)\n",
      "obj \t| train (30338, 245) \t| valid (10420, 245) \t| test (12995, 245)\n",
      "image_dim:    360\n",
      "attr \t| train (30338, 115) \t| valid (10420, 115) \t| test (12995, 115)\n",
      "obj \t| train (30338, 245) \t| valid (10420, 245) \t| test (12995, 245)\n",
      "image_dim:    360\n",
      "seen_indices: 1844 | unseen_indices: 8576\n",
      "Intervention features: torch.Size([10420, 360])\n",
      "seen_indices: 1844 | unseen_indices: 8576\n",
      "Intervention features: torch.Size([10420, 360])\n",
      "features:  torch.Size([10420, 1962])\n",
      "topk: 1\n",
      "best_seen_acc: 0.7164\n",
      "best_unseen_acc: 0.7463\n",
      "best_harmonic_mean: 0.6307\n",
      "auc: 0.4969\n",
      "topk: 2\n",
      "best_seen_acc: 0.8449\n",
      "best_unseen_acc: 0.8707\n",
      "best_harmonic_mean: 0.7700\n",
      "auc: 0.7041\n",
      "topk: 3\n",
      "best_seen_acc: 0.9121\n",
      "best_unseen_acc: 0.9025\n",
      "best_harmonic_mean: 0.8208\n",
      "auc: 0.7738\n",
      "\n",
      "\n",
      "\n",
      "======================================================================\n",
      "| test | template clip | num_prompts 4 |\n",
      "train pairs: 1262 | valid pairs: 600 | test pairs: 800\n",
      "train images: 30338 | valid images: 10420 | test images: 12995\n",
      "seen_indices: 1844 | unseen_indices: 8576\n",
      "seen_indices: 2380 | unseen_indices: 10615\n",
      "attr \t| train (30338, 115) \t| valid (10420, 115) \t| test (12995, 115)\n",
      "obj \t| train (30338, 245) \t| valid (10420, 245) \t| test (12995, 245)\n",
      "image_dim:    360\n",
      "attr \t| train (30338, 115) \t| valid (10420, 115) \t| test (12995, 115)\n",
      "obj \t| train (30338, 245) \t| valid (10420, 245) \t| test (12995, 245)\n",
      "image_dim:    360\n",
      "attr \t| train (30338, 115) \t| valid (10420, 115) \t| test (12995, 115)\n",
      "obj \t| train (30338, 245) \t| valid (10420, 245) \t| test (12995, 245)\n",
      "image_dim:    360\n",
      "seen_indices: 2380 | unseen_indices: 10615\n",
      "Intervention features: torch.Size([12995, 360])\n",
      "seen_indices: 2380 | unseen_indices: 10615\n",
      "Intervention features: torch.Size([12995, 360])\n",
      "features:  torch.Size([12995, 1962])\n",
      "topk: 1\n",
      "best_seen_acc: 0.6618\n",
      "best_unseen_acc: 0.7952\n",
      "best_harmonic_mean: 0.6524\n",
      "auc: 0.5014\n",
      "topk: 2\n",
      "best_seen_acc: 0.8134\n",
      "best_unseen_acc: 0.8870\n",
      "best_harmonic_mean: 0.7810\n",
      "auc: 0.6982\n",
      "topk: 3\n",
      "best_seen_acc: 0.8639\n",
      "best_unseen_acc: 0.9292\n",
      "best_harmonic_mean: 0.8309\n",
      "auc: 0.7826\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Interpretability for Interv(GT)\n",
    "\n",
    "class parseArguments:\n",
    "    def __init__(self):\n",
    "        self.split = \"valid\"\n",
    "        self.is_open_world = False\n",
    "        \n",
    "        self.feature = \"primitive\"\n",
    "        self.model = \"clip\"\n",
    "        self.num_prompts = 4\n",
    "        self.train_warmup = 0\n",
    "        self.is_image_projection = True\n",
    "        self.is_bias = False\n",
    "        self.is_limit = True\n",
    "        \n",
    "        self.data_root = \"../../data/mit_states\"\n",
    "        self.emb_root = \"../../data\"\n",
    "        self.precomputed_data_root = f\"../../outputs/mit_states/clip_prompt_precompute_features_random/{self.model}/num_prompts_{self.num_prompts}\"\n",
    "        \n",
    "        self.emb_init = True\n",
    "        self.input_dim = 600\n",
    "        self.num_epochs = 400\n",
    "        self.batch_size = 128\n",
    "        self.learning_rate = 5e-5\n",
    "        self.weight_decay = 5e-5\n",
    "        self.logit_scale = 0.07\n",
    "\n",
    "args = parseArguments()\n",
    "\n",
    "# Start!!\n",
    "template_count_dict = {\"clip\": 7, \"compdl\": 10}\n",
    "\n",
    "for split in [\"valid\", \"test\"]:\n",
    "    args.split = split\n",
    "    for template_src in [\"clip\"]:\n",
    "        print(\"=\" * 70)\n",
    "        print(f\"| {args.split} | template {template_src} | num_prompts {args.num_prompts} |\")\n",
    "\n",
    "        # Load dataset\n",
    "        data = MITStatesDataset(root=args.data_root, split=\"train\")  # split can be ignored here\n",
    "        seen_ids_valid, unseen_ids_valid = get_seen_unseen_indices(\"valid\", data)\n",
    "        seen_ids_test, unseen_ids_test = get_seen_unseen_indices(\"test\", data)\n",
    "\n",
    "        train_dataset = Precomputed_MITStatesDataset(split=\"train\", feature=\"primitive\", data=data, args=args, is_limit=True)\n",
    "        valid_dataset = Precomputed_MITStatesDataset(split=\"valid\", feature=\"primitive\", data=data, args=args, is_limit=True)\n",
    "        test_dataset = Precomputed_MITStatesDataset(split=\"test\", feature=\"primitive\", data=data, args=args, is_limit=True)\n",
    "        seen_mask = train_dataset.seen_mask\n",
    "\n",
    "        # Load model\n",
    "        model = load_model(template_src, args.num_prompts)\n",
    "\n",
    "        # Prepare groundtruth labels\n",
    "        seen_ids, unseen_ids = get_seen_unseen_indices(args.split, data)\n",
    "\n",
    "        if args.split == \"valid\":\n",
    "            labels_attr = [sample[\"attr_id\"] for sample in data.valid_data]\n",
    "            labels_obj = [sample[\"obj_id\"] for sample in data.valid_data]\n",
    "            labels = [sample[\"pair_id\"] for sample in data.valid_data]\n",
    "            gt_features_attr = np.zeros((valid_dataset.image_embs.shape[0], 115))\n",
    "            gt_features_obj = np.zeros((valid_dataset.image_embs.shape[0], 245))\n",
    "        elif args.split == \"test\":\n",
    "            labels_attr = [sample[\"attr_id\"] for sample in data.test_data]\n",
    "            labels_obj = [sample[\"obj_id\"] for sample in data.test_data]\n",
    "            labels = [sample[\"pair_id\"] for sample in data.test_data]\n",
    "            gt_features_attr = np.zeros((test_dataset.image_embs.shape[0], 115))\n",
    "            gt_features_obj = np.zeros((test_dataset.image_embs.shape[0], 245))\n",
    "\n",
    "        gt_features_attr[np.arange(len(labels_attr)), labels_attr] = 1\n",
    "        gt_features_obj[np.arange(len(labels_obj)), labels_obj] = 1\n",
    "        gt_features_concat = np.concatenate([gt_features_attr, gt_features_obj], axis=-1)\n",
    "        gt_features_concat = torch.tensor(gt_features_concat).to(device).double()\n",
    "        print(f\"Intervention features: {gt_features_concat.shape}\")\n",
    "        \n",
    "        # Prepare \"binary\" primitive concept activations\n",
    "        # Set GT concepts to 1\n",
    "        seen_ids, unseen_ids = get_seen_unseen_indices(args.split, data)\n",
    "\n",
    "        if args.split == \"valid\":\n",
    "            labels_attr = [sample[\"attr_id\"] for sample in data.valid_data]\n",
    "            labels_obj = [sample[\"obj_id\"] for sample in data.valid_data]\n",
    "            labels = [sample[\"pair_id\"] for sample in data.valid_data]\n",
    "            features_interv = valid_dataset.image_embs\n",
    "            features_interv = torch.tensor(features_interv).to(device).double()\n",
    "            features_interv = F.normalize(features_interv, dim=1)\n",
    "\n",
    "        elif args.split == \"test\":\n",
    "            labels_attr = [sample[\"attr_id\"] for sample in data.test_data]\n",
    "            labels_obj = [sample[\"obj_id\"] for sample in data.test_data]\n",
    "            labels = [sample[\"pair_id\"] for sample in data.test_data]\n",
    "            features_interv = test_dataset.image_embs\n",
    "            features_interv = torch.tensor(features_interv).to(device).double()\n",
    "            features_interv = F.normalize(features_interv, dim=1)\n",
    "\n",
    "        features_interv = torch.where(gt_features_concat==1, 1.0, features_interv)\n",
    "        print(f\"Intervention features: {features_interv.shape}\")\n",
    "\n",
    "        # Intervene!!!\n",
    "        with torch.no_grad():\n",
    "            logits, loss = model(features_interv, pair_labels=None, is_train=False, is_intervene=True)\n",
    "        features = logits.softmax(dim=-1).log()\n",
    "        print(\"features: \", features.shape)\n",
    "\n",
    "        # Compute metrics\n",
    "        overall_metrics = get_overall_metrics(features, labels, seen_ids, unseen_ids, seen_mask, data, topk_list=[1,2,3])\n",
    "        output_path = get_output_path(template_src, args.num_prompts, args)\n",
    "        with open(os.path.join(output_path, f\"{args.split}_metrics_interv_gt.json\"), \"w\") as f:\n",
    "            json.dump(overall_metrics, f)\n",
    "        print(\"\\n\\n\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22552721",
   "metadata": {},
   "source": [
    "# 6. Evaluation for All Prompts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "b7ee913d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load trained model\n",
    "def load_model(template_src, template_id):\n",
    "    ckpt_dir = f\"../../outputs/mit_states/clip_prompt_retrieval_model\"\n",
    "    ckpt_name = \"clip_prompt_retrieval_model_TP{}_TID{}_open{}_init{}_imgproj{}_bias{}_{}_Lim{}_N{}_TW{}_B{}_LR{}_WD{}_L{}\".format(\n",
    "        template_src,\n",
    "        template_id,\n",
    "        1 if args.is_open_world else 0,\n",
    "        1 if args.emb_init else 0,\n",
    "        1 if args.is_image_projection else 0,\n",
    "        1 if args.is_bias else 0,\n",
    "        args.feature,\n",
    "        1 if args.is_limit else 0, \n",
    "        args.num_epochs,\n",
    "        args.train_warmup,\n",
    "        args.batch_size,\n",
    "        args.learning_rate,\n",
    "        args.weight_decay,\n",
    "        args.logit_scale,\n",
    "    )\n",
    "    ckpt_path = os.path.join(ckpt_dir, ckpt_name, \"retrieval_model.ckpt\")\n",
    "    print(ckpt_path)\n",
    "\n",
    "    limit_pairs = list(train_dataset.limit_pair2idx.keys())\n",
    "    open_world_pairs = list(valid_dataset.open_world_pair2idx.keys())\n",
    "    model = RetrievalModel(data, train_dataset.image_dim, limit_pairs, open_world_pairs, args)\n",
    "    model.load_state_dict(torch.load(ckpt_path))\n",
    "    model.eval()\n",
    "    model.to(device)\n",
    "    return model\n",
    "\n",
    "def get_output_path(template_src, template_id, args):\n",
    "    ckpt_dir = f\"../../outputs/mit_states/clip_prompt_retrieval_model\"\n",
    "    ckpt_name = \"clip_prompt_retrieval_model_TP{}_TID{}_open{}_init{}_imgproj{}_bias{}_{}_Lim{}_N{}_TW{}_B{}_LR{}_WD{}_L{}\".format(\n",
    "        template_src,\n",
    "        template_id,\n",
    "        1 if args.is_open_world else 0,\n",
    "        1 if args.emb_init else 0,\n",
    "        1 if args.is_image_projection else 0,\n",
    "        1 if args.is_bias else 0,\n",
    "        args.feature,\n",
    "        1 if args.is_limit else 0, \n",
    "        args.num_epochs,\n",
    "        args.train_warmup,\n",
    "        args.batch_size,\n",
    "        args.learning_rate,\n",
    "        args.weight_decay,\n",
    "        args.logit_scale,\n",
    "    )\n",
    "    return os.path.join(ckpt_dir, ckpt_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f54198f",
   "metadata": {},
   "source": [
    "## 6.1. Interpretabilty for Interv(GT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "2bc5d309",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "| valid | template clip | template_id all |\n",
      "train pairs: 1262 | valid pairs: 600 | test pairs: 800\n",
      "train images: 30338 | valid images: 10420 | test images: 12995\n",
      "seen_indices: 1844 | unseen_indices: 8576\n",
      "seen_indices: 2380 | unseen_indices: 10615\n",
      "attr \t| train (30338, 115) \t| valid (10420, 115) \t| test (12995, 115)\n",
      "obj \t| train (30338, 245) \t| valid (10420, 245) \t| test (12995, 245)\n",
      "image_dim:    360\n",
      "attr \t| train (30338, 115) \t| valid (10420, 115) \t| test (12995, 115)\n",
      "obj \t| train (30338, 245) \t| valid (10420, 245) \t| test (12995, 245)\n",
      "image_dim:    360\n",
      "attr \t| train (30338, 115) \t| valid (10420, 115) \t| test (12995, 115)\n",
      "obj \t| train (30338, 245) \t| valid (10420, 245) \t| test (12995, 245)\n",
      "image_dim:    360\n",
      "../../outputs/mit_states/clip_prompt_retrieval_model/clip_prompt_retrieval_model_TPclip_TIDall_open0_init1_imgproj1_bias0_primitive_Lim1_N400_TW0_B128_LR5e-05_WD5e-05_L0.07/retrieval_model.ckpt\n",
      "seen_indices: 1844 | unseen_indices: 8576\n",
      "Intervention features: torch.Size([10420, 360])\n",
      "features:  torch.Size([10420, 1962])\n",
      "topk: 1\n",
      "best_seen_acc: 0.6838\n",
      "best_unseen_acc: 0.7206\n",
      "best_harmonic_mean: 0.6106\n",
      "auc: 0.4576\n",
      "topk: 2\n",
      "best_seen_acc: 0.8069\n",
      "best_unseen_acc: 0.8527\n",
      "best_harmonic_mean: 0.7433\n",
      "auc: 0.6569\n",
      "topk: 3\n",
      "best_seen_acc: 0.8937\n",
      "best_unseen_acc: 0.8952\n",
      "best_harmonic_mean: 0.7992\n",
      "auc: 0.7672\n",
      "../../outputs/mit_states/clip_prompt_retrieval_model/clip_prompt_retrieval_model_TPclip_TIDall_open0_init1_imgproj1_bias0_primitive_Lim1_N400_TW0_B128_LR5e-05_WD5e-05_L0.07\n",
      "\n",
      "\n",
      "\n",
      "======================================================================\n",
      "| test | template clip | template_id all |\n",
      "train pairs: 1262 | valid pairs: 600 | test pairs: 800\n",
      "train images: 30338 | valid images: 10420 | test images: 12995\n",
      "seen_indices: 1844 | unseen_indices: 8576\n",
      "seen_indices: 2380 | unseen_indices: 10615\n",
      "attr \t| train (30338, 115) \t| valid (10420, 115) \t| test (12995, 115)\n",
      "obj \t| train (30338, 245) \t| valid (10420, 245) \t| test (12995, 245)\n",
      "image_dim:    360\n",
      "attr \t| train (30338, 115) \t| valid (10420, 115) \t| test (12995, 115)\n",
      "obj \t| train (30338, 245) \t| valid (10420, 245) \t| test (12995, 245)\n",
      "image_dim:    360\n",
      "attr \t| train (30338, 115) \t| valid (10420, 115) \t| test (12995, 115)\n",
      "obj \t| train (30338, 245) \t| valid (10420, 245) \t| test (12995, 245)\n",
      "image_dim:    360\n",
      "../../outputs/mit_states/clip_prompt_retrieval_model/clip_prompt_retrieval_model_TPclip_TIDall_open0_init1_imgproj1_bias0_primitive_Lim1_N400_TW0_B128_LR5e-05_WD5e-05_L0.07/retrieval_model.ckpt\n",
      "seen_indices: 2380 | unseen_indices: 10615\n",
      "Intervention features: torch.Size([12995, 360])\n",
      "features:  torch.Size([12995, 1962])\n",
      "topk: 1\n",
      "best_seen_acc: 0.6500\n",
      "best_unseen_acc: 0.7790\n",
      "best_harmonic_mean: 0.6243\n",
      "auc: 0.4769\n",
      "topk: 2\n",
      "best_seen_acc: 0.7811\n",
      "best_unseen_acc: 0.8742\n",
      "best_harmonic_mean: 0.7542\n",
      "auc: 0.6502\n",
      "topk: 3\n",
      "best_seen_acc: 0.8563\n",
      "best_unseen_acc: 0.9181\n",
      "best_harmonic_mean: 0.8073\n",
      "auc: 0.7606\n",
      "../../outputs/mit_states/clip_prompt_retrieval_model/clip_prompt_retrieval_model_TPclip_TIDall_open0_init1_imgproj1_bias0_primitive_Lim1_N400_TW0_B128_LR5e-05_WD5e-05_L0.07\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Interpretability for Interv(GT)\n",
    "\n",
    "class parseArguments:\n",
    "    def __init__(self):\n",
    "        self.split = \"valid\"\n",
    "        self.is_open_world = False\n",
    "        \n",
    "        self.feature = \"primitive\"\n",
    "        self.model = \"clip\"\n",
    "        self.train_warmup = 0\n",
    "        self.is_image_projection = True\n",
    "        self.is_bias = False\n",
    "        self.is_limit = True\n",
    "        \n",
    "        self.data_root = \"../../data/mit_states\"\n",
    "        self.emb_root = \"../../data\"\n",
    "        self.precomputed_data_dir = f\"../../outputs/mit_states/clip_prompt_precompute_features\"\n",
    "        \n",
    "        self.emb_init = True\n",
    "        self.input_dim = 600\n",
    "        self.num_epochs = 400\n",
    "        self.batch_size = 128\n",
    "        self.learning_rate = 5e-5\n",
    "        self.weight_decay = 5e-5\n",
    "        self.logit_scale = 0.07\n",
    "\n",
    "args = parseArguments()\n",
    "\n",
    "# Start!!\n",
    "template_count_dict = {\"clip\": 7, \"compdl\": 10}\n",
    "\n",
    "for split in [\"valid\", \"test\"]:\n",
    "    args.split = split\n",
    "    template_src = \"clip\"\n",
    "    template_id = \"all\"  # 3 or all\n",
    "    \n",
    "    print(\"=\" * 70)\n",
    "    print(f\"| {args.split} | template {template_src} | template_id {template_id} |\")\n",
    "    if template_id == \"all\":\n",
    "        args.precomputed_data_root = os.path.join(args.precomputed_data_dir, template_src, \"combined\")\n",
    "    else:\n",
    "        args.precomputed_data_root = os.path.join(args.precomputed_data_dir, template_src, f\"template_{template_id}\")\n",
    "\n",
    "    # Load dataset\n",
    "    data = MITStatesDataset(root=args.data_root, split=\"train\")  # split can be ignored here\n",
    "    seen_ids_valid, unseen_ids_valid = get_seen_unseen_indices(\"valid\", data)\n",
    "    seen_ids_test, unseen_ids_test = get_seen_unseen_indices(\"test\", data)\n",
    "\n",
    "    train_dataset = Precomputed_MITStatesDataset(split=\"train\", feature=\"primitive\", data=data, args=args, is_limit=True)\n",
    "    valid_dataset = Precomputed_MITStatesDataset(split=\"valid\", feature=\"primitive\", data=data, args=args, is_limit=True)\n",
    "    test_dataset = Precomputed_MITStatesDataset(split=\"test\", feature=\"primitive\", data=data, args=args, is_limit=True)\n",
    "    seen_mask = train_dataset.seen_mask\n",
    "\n",
    "    # Load model\n",
    "    model = load_model(template_src, template_id)\n",
    "\n",
    "    # Prepare groundtruth labels\n",
    "    seen_ids, unseen_ids = get_seen_unseen_indices(args.split, data)\n",
    "\n",
    "    if args.split == \"valid\":\n",
    "        labels_attr = [sample[\"attr_id\"] for sample in data.valid_data]\n",
    "        labels_obj = [sample[\"obj_id\"] for sample in data.valid_data]\n",
    "        labels = [sample[\"pair_id\"] for sample in data.valid_data]\n",
    "        gt_features_attr = np.zeros((valid_dataset.image_embs.shape[0], 115))\n",
    "        gt_features_obj = np.zeros((valid_dataset.image_embs.shape[0], 245))\n",
    "    elif args.split == \"test\":\n",
    "        labels_attr = [sample[\"attr_id\"] for sample in data.test_data]\n",
    "        labels_obj = [sample[\"obj_id\"] for sample in data.test_data]\n",
    "        labels = [sample[\"pair_id\"] for sample in data.test_data]\n",
    "        gt_features_attr = np.zeros((test_dataset.image_embs.shape[0], 115))\n",
    "        gt_features_obj = np.zeros((test_dataset.image_embs.shape[0], 245))\n",
    "\n",
    "    gt_features_attr[np.arange(len(labels_attr)), labels_attr] = 1\n",
    "    gt_features_obj[np.arange(len(labels_obj)), labels_obj] = 1\n",
    "    gt_features_concat = np.concatenate([gt_features_attr, gt_features_obj], axis=-1)\n",
    "    gt_features_concat = torch.tensor(gt_features_concat).to(device).double()\n",
    "    print(f\"Intervention features: {gt_features_concat.shape}\")\n",
    "\n",
    "    # Intervene!!!\n",
    "    with torch.no_grad():\n",
    "        logits, loss = model(gt_features_concat, pair_labels=None, is_train=False, is_intervene=True)\n",
    "    features = logits.softmax(dim=-1).log()\n",
    "    print(\"features: \", features.shape)\n",
    "\n",
    "    # Compute metrics\n",
    "    overall_metrics = get_overall_metrics(features, labels, seen_ids, unseen_ids, seen_mask, data, topk_list=[1,2,3])\n",
    "    output_path = get_output_path(template_src, template_id, args)\n",
    "    print(output_path)\n",
    "    with open(os.path.join(output_path, f\"{args.split}_metrics_interv_gt.json\"), \"w\") as f:\n",
    "        json.dump(overall_metrics, f)\n",
    "    print(\"\\n\\n\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b47a2099",
   "metadata": {},
   "source": [
    "## 6.2. Interpretability for Interv(GTX)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "284e112f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "| valid | template clip | template_id all |\n",
      "train pairs: 1262 | valid pairs: 600 | test pairs: 800\n",
      "train images: 30338 | valid images: 10420 | test images: 12995\n",
      "seen_indices: 1844 | unseen_indices: 8576\n",
      "seen_indices: 2380 | unseen_indices: 10615\n",
      "attr \t| train (30338, 115) \t| valid (10420, 115) \t| test (12995, 115)\n",
      "obj \t| train (30338, 245) \t| valid (10420, 245) \t| test (12995, 245)\n",
      "image_dim:    360\n",
      "attr \t| train (30338, 115) \t| valid (10420, 115) \t| test (12995, 115)\n",
      "obj \t| train (30338, 245) \t| valid (10420, 245) \t| test (12995, 245)\n",
      "image_dim:    360\n",
      "attr \t| train (30338, 115) \t| valid (10420, 115) \t| test (12995, 115)\n",
      "obj \t| train (30338, 245) \t| valid (10420, 245) \t| test (12995, 245)\n",
      "image_dim:    360\n",
      "../../outputs/mit_states/clip_prompt_retrieval_model/clip_prompt_retrieval_model_TPclip_TIDall_open0_init1_imgproj1_bias0_primitive_Lim1_N400_TW0_B128_LR5e-05_WD5e-05_L0.07/retrieval_model.ckpt\n",
      "seen_indices: 1844 | unseen_indices: 8576\n",
      "Intervention features: torch.Size([10420, 360])\n",
      "seen_indices: 1844 | unseen_indices: 8576\n",
      "Intervention features: torch.Size([10420, 360])\n",
      "features:  torch.Size([10420, 1962])\n",
      "topk: 1\n",
      "best_seen_acc: 0.7240\n",
      "best_unseen_acc: 0.7618\n",
      "best_harmonic_mean: 0.6381\n",
      "auc: 0.5150\n",
      "topk: 2\n",
      "best_seen_acc: 0.8541\n",
      "best_unseen_acc: 0.8853\n",
      "best_harmonic_mean: 0.7732\n",
      "auc: 0.7212\n",
      "topk: 3\n",
      "best_seen_acc: 0.9111\n",
      "best_unseen_acc: 0.9144\n",
      "best_harmonic_mean: 0.8325\n",
      "auc: 0.8068\n",
      "\n",
      "\n",
      "\n",
      "======================================================================\n",
      "| test | template clip | template_id all |\n",
      "train pairs: 1262 | valid pairs: 600 | test pairs: 800\n",
      "train images: 30338 | valid images: 10420 | test images: 12995\n",
      "seen_indices: 1844 | unseen_indices: 8576\n",
      "seen_indices: 2380 | unseen_indices: 10615\n",
      "attr \t| train (30338, 115) \t| valid (10420, 115) \t| test (12995, 115)\n",
      "obj \t| train (30338, 245) \t| valid (10420, 245) \t| test (12995, 245)\n",
      "image_dim:    360\n",
      "attr \t| train (30338, 115) \t| valid (10420, 115) \t| test (12995, 115)\n",
      "obj \t| train (30338, 245) \t| valid (10420, 245) \t| test (12995, 245)\n",
      "image_dim:    360\n",
      "attr \t| train (30338, 115) \t| valid (10420, 115) \t| test (12995, 115)\n",
      "obj \t| train (30338, 245) \t| valid (10420, 245) \t| test (12995, 245)\n",
      "image_dim:    360\n",
      "../../outputs/mit_states/clip_prompt_retrieval_model/clip_prompt_retrieval_model_TPclip_TIDall_open0_init1_imgproj1_bias0_primitive_Lim1_N400_TW0_B128_LR5e-05_WD5e-05_L0.07/retrieval_model.ckpt\n",
      "seen_indices: 2380 | unseen_indices: 10615\n",
      "Intervention features: torch.Size([12995, 360])\n",
      "seen_indices: 2380 | unseen_indices: 10615\n",
      "Intervention features: torch.Size([12995, 360])\n",
      "features:  torch.Size([12995, 1962])\n",
      "topk: 1\n",
      "best_seen_acc: 0.6731\n",
      "best_unseen_acc: 0.7979\n",
      "best_harmonic_mean: 0.6535\n",
      "auc: 0.5113\n",
      "topk: 2\n",
      "best_seen_acc: 0.8109\n",
      "best_unseen_acc: 0.8958\n",
      "best_harmonic_mean: 0.7791\n",
      "auc: 0.7005\n",
      "topk: 3\n",
      "best_seen_acc: 0.8836\n",
      "best_unseen_acc: 0.9340\n",
      "best_harmonic_mean: 0.8361\n",
      "auc: 0.8024\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Interpretability for Interv(GTX)\n",
    "\n",
    "class parseArguments:\n",
    "    def __init__(self):\n",
    "        self.split = \"valid\"\n",
    "        self.is_open_world = False\n",
    "        \n",
    "        self.feature = \"primitive\"\n",
    "        self.model = \"clip\"\n",
    "        self.train_warmup = 0\n",
    "        self.is_image_projection = True\n",
    "        self.is_bias = False\n",
    "        self.is_limit = True\n",
    "        \n",
    "        self.data_root = \"../../data/mit_states\"\n",
    "        self.emb_root = \"../../data\"\n",
    "        self.precomputed_data_dir = f\"../../outputs/mit_states/clip_prompt_precompute_features\"\n",
    "        \n",
    "        self.emb_init = True\n",
    "        self.input_dim = 600\n",
    "        self.num_epochs = 400\n",
    "        self.batch_size = 128\n",
    "        self.learning_rate = 5e-5\n",
    "        self.weight_decay = 5e-5\n",
    "        self.logit_scale = 0.07\n",
    "\n",
    "args = parseArguments()\n",
    "\n",
    "# Start!!\n",
    "template_count_dict = {\"clip\": 7, \"compdl\": 10}\n",
    "\n",
    "for split in [\"valid\", \"test\"]:\n",
    "    args.split = split\n",
    "    template_src = \"clip\"\n",
    "    template_id = \"all\"  # 3 or all\n",
    "    \n",
    "    print(\"=\" * 70)\n",
    "    print(f\"| {args.split} | template {template_src} | template_id {template_id} |\")\n",
    "    if template_id == \"all\":\n",
    "        args.precomputed_data_root = os.path.join(args.precomputed_data_dir, template_src, \"combined\")\n",
    "    else:\n",
    "        args.precomputed_data_root = os.path.join(args.precomputed_data_dir, template_src, f\"template_{template_id}\")\n",
    "\n",
    "    # Load dataset\n",
    "    data = MITStatesDataset(root=args.data_root, split=\"train\")  # split can be ignored here\n",
    "    seen_ids_valid, unseen_ids_valid = get_seen_unseen_indices(\"valid\", data)\n",
    "    seen_ids_test, unseen_ids_test = get_seen_unseen_indices(\"test\", data)\n",
    "\n",
    "    train_dataset = Precomputed_MITStatesDataset(split=\"train\", feature=\"primitive\", data=data, args=args, is_limit=True)\n",
    "    valid_dataset = Precomputed_MITStatesDataset(split=\"valid\", feature=\"primitive\", data=data, args=args, is_limit=True)\n",
    "    test_dataset = Precomputed_MITStatesDataset(split=\"test\", feature=\"primitive\", data=data, args=args, is_limit=True)\n",
    "    seen_mask = train_dataset.seen_mask\n",
    "\n",
    "    # Load model\n",
    "    model = load_model(template_src, template_id)\n",
    "\n",
    "    # Prepare groundtruth labels\n",
    "    seen_ids, unseen_ids = get_seen_unseen_indices(args.split, data)\n",
    "\n",
    "    if args.split == \"valid\":\n",
    "        labels_attr = [sample[\"attr_id\"] for sample in data.valid_data]\n",
    "        labels_obj = [sample[\"obj_id\"] for sample in data.valid_data]\n",
    "        labels = [sample[\"pair_id\"] for sample in data.valid_data]\n",
    "        gt_features_attr = np.zeros((valid_dataset.image_embs.shape[0], 115))\n",
    "        gt_features_obj = np.zeros((valid_dataset.image_embs.shape[0], 245))\n",
    "    elif args.split == \"test\":\n",
    "        labels_attr = [sample[\"attr_id\"] for sample in data.test_data]\n",
    "        labels_obj = [sample[\"obj_id\"] for sample in data.test_data]\n",
    "        labels = [sample[\"pair_id\"] for sample in data.test_data]\n",
    "        gt_features_attr = np.zeros((test_dataset.image_embs.shape[0], 115))\n",
    "        gt_features_obj = np.zeros((test_dataset.image_embs.shape[0], 245))\n",
    "\n",
    "    gt_features_attr[np.arange(len(labels_attr)), labels_attr] = 1\n",
    "    gt_features_obj[np.arange(len(labels_obj)), labels_obj] = 1\n",
    "    gt_features_concat = np.concatenate([gt_features_attr, gt_features_obj], axis=-1)\n",
    "    gt_features_concat = torch.tensor(gt_features_concat).to(device).double()\n",
    "    print(f\"Intervention features: {gt_features_concat.shape}\")\n",
    "\n",
    "    # Prepare \"binary\" primitive concept activations\n",
    "    # Set GT concepts to 1\n",
    "    seen_ids, unseen_ids = get_seen_unseen_indices(args.split, data)\n",
    "\n",
    "    if args.split == \"valid\":\n",
    "        labels_attr = [sample[\"attr_id\"] for sample in data.valid_data]\n",
    "        labels_obj = [sample[\"obj_id\"] for sample in data.valid_data]\n",
    "        labels = [sample[\"pair_id\"] for sample in data.valid_data]\n",
    "        features_interv = valid_dataset.image_embs\n",
    "        features_interv = torch.tensor(features_interv).to(device).double()\n",
    "        features_interv = F.normalize(features_interv, dim=1)\n",
    "\n",
    "    elif args.split == \"test\":\n",
    "        labels_attr = [sample[\"attr_id\"] for sample in data.test_data]\n",
    "        labels_obj = [sample[\"obj_id\"] for sample in data.test_data]\n",
    "        labels = [sample[\"pair_id\"] for sample in data.test_data]\n",
    "        features_interv = test_dataset.image_embs\n",
    "        features_interv = torch.tensor(features_interv).to(device).double()\n",
    "        features_interv = F.normalize(features_interv, dim=1)\n",
    "\n",
    "    features_interv = torch.where(gt_features_concat==1, 1.0, features_interv)\n",
    "    print(f\"Intervention features: {features_interv.shape}\")\n",
    "\n",
    "    # Intervene!!!\n",
    "    with torch.no_grad():\n",
    "        logits, loss = model(features_interv, pair_labels=None, is_train=False, is_intervene=True)\n",
    "    features = logits.softmax(dim=-1).log()\n",
    "    print(\"features: \", features.shape)\n",
    "\n",
    "    # Compute metrics\n",
    "    overall_metrics = get_overall_metrics(features, labels, seen_ids, unseen_ids, seen_mask, data, topk_list=[1,2,3])\n",
    "    output_path = get_output_path(template_src, template_id, args)\n",
    "    with open(os.path.join(output_path, f\"{args.split}_metrics_interv_gt.json\"), \"w\") as f:\n",
    "        json.dump(overall_metrics, f)\n",
    "    print(\"\\n\\n\")\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "vlm_concept",
   "language": "python",
   "name": "vlm_concept"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
